[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Course introduction\nWelcome to PHCM9795 Foundations of Biostatistics.\nThis introductory course in biostatistics aims to provide students with core biostatistical skills to analyse and present quantitative data from different study types. These are essential skills required in your degree and throughout your career.\nWe hope you enjoy the course and will value your feedback and comment throughout the course."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course information",
    "text": "Course information\nBiostatistics is a foundational discipline needed for the analysis and interpretation of quantitative information and its application to population health policy and practice.\nThis course is central to becoming a population health practitioner as the concepts and techniques developed in the course are fundamental to your studies and practice in population health. In this course you will develop an understanding of, and skills in, the core concepts of biostatistics that are necessary for analysis and interpretation of population health data and health literature.\nIn designing this course, we provide a learning sequence that will allow you to obtain the required graduate capabilities identified for your program. This course is taught with an emphasis on formulating a hypothesis and quantifying the evidence in relation to a specific research question. You will have the opportunity to analyse data from different study types commonly seen in population health research.\nThe course will allow those of you who have covered some of this material in your undergraduate and other professional education to consolidate your knowledge and skills. Students exposed to biostatistics for the first time may find the course challenging at times. Based on student feedback, the key to success in this course is to devote time to it every week. We recommend that you spend an average of 10-15 hours per week on the course, including the time spent reading the course notes and readings, listening to lectures, and working through learning activities and completing your assessments. Please use the resources provided to assist you, including online support."
  },
  {
    "objectID": "index.html#units-of-credit",
    "href": "index.html#units-of-credit",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Units of credit",
    "text": "Units of credit\nThis course is a core course of the Master of Public Health, Master of Global Health and Master of Infectious Diseases Intelligence programs and associated dual degrees, comprising 6 units of credit towards the total required for completion of the study program. A value of 6 UOC requires a minimum of 150 hours work for the average student across the term."
  },
  {
    "objectID": "index.html#course-aim",
    "href": "index.html#course-aim",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course aim",
    "text": "Course aim\nThis course aims to provide students with the core biostatistical skills to apply appropriate statistical techniques to analyse and present population health data."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this course, you will be able to:\n\nSummarise and visualise data using statistical software.\nDemonstrate an understanding of statistical inference by interpreting p-values and confidence intervals.\nApply appropriate statistical tests for different types of variables given a research question, and interpret computer output of these tests appropriately.\nDetermine the appropriate sample size when planning a research study.\nPresent and interpret statistical findings appropriate for a population health audience."
  },
  {
    "objectID": "07-testing-proportions.html#learning-objectives",
    "href": "07-testing-proportions.html#learning-objectives",
    "title": "1  Hypothesis testing for categorical data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nUse and interpret the appropriate test for testing associations between categorical data;\nConduct and interpret an appropriate test for independent proportions;\nConduct and interpret a test for paired proportions;"
  },
  {
    "objectID": "07-testing-proportions.html#readings",
    "href": "07-testing-proportions.html#readings",
    "title": "1  Hypothesis testing for categorical data",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 17. [UNSW Library Link]\nBland (2015); Chapter 13. [UNSW Library Link]\nAcock (2010); Section 7.6."
  },
  {
    "objectID": "07-testing-proportions.html#introduction",
    "href": "07-testing-proportions.html#introduction",
    "title": "1  Hypothesis testing for categorical data",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIn Module 6, we estimated the 95% confidence intervals of proportions and measures of association for categorical data and conducted a significance test comparing a sample proportion to a known value.\nWhen both the outcome variable and the exposure variable are categorical, a chi-squared test can be used as a formal statistical test to assess whether the exposure and outcome are related. The P-value obtained from a chi-squared test gives the probability of obtaining the observed association (or more extreme) if there is in fact no association between the exposure and outcome.\nIn this Module, we also include tests for a difference in proportion for paired data.\n\n1.1.1 Worked Example\nWe are using the randomised controlled trial as given in Worked Example 6.4 on the nauseating side effect of a drug.\nThe research question is whether the active drug resulted in a different rate of nausea than the placebo drug. This is equivalent to testing whether there is an association between nausea and type of drug received (active or placebo). Thus, we will test the null hypothesis that the experience of nausea and the treatment are not related to one another. The null hypothesis is:\n\nH0: The proportion with nausea in the active drug group is the same as the proportion with nausea in the placebo drug group.\n\nThe alternative hypothesis can be stated as:\n\nHa: The proportion with nausea in the active drug group is different to the proportion with nausea in the placebo drug group."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "href": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "title": "1  Hypothesis testing for categorical data",
    "section": "1.2 Chi-squared test for independent proportions",
    "text": "1.2 Chi-squared test for independent proportions\nA chi-squared test is used to test the null hypothesis that of no association between two categorical variables. First a contingency table is drawn up and then we estimate the counts of each cell (i.e. a, b, c and d) that would be expected if the null hypothesis was true. The row and column totals are used to calculate expected counts in each cell of the contingency table as follows:\nExpected count = (Row count × Column count) / Total count\nStatistical software will do this for us, as described in the Stata or R sections in this Module.\nA chi-squared value is then calculated to compare the expected counts (E) in each cell with the observed (actual) cell counts (O). The calculation is as follows:\n\\(\\chi ^ 2 = \\sum \\frac{(O - E)}{E} ^2\\)\nwith [Number of rows \\(-\\) 1] \\(\\times\\) [Number of columns \\(-\\) 1] degrees of freedom.\nAs for many statistics, the deviations between the observed and expected values are squared to prevent the negative and positive values balancing one another out.\nIf the expected counts are close to the observed counts, the chi-squared statistic will be close to zero, and the P-value will be close to 1. The larger the difference between the observed and expected counts, the larger the chi-squared statistic becomes (and the smaller the P-value). A large chi-squared statistic provides more evidence of an association between the exposure and outcome.\nA chi-squared test to examine whether two independent proportions are associated can be obtained in Stata using the Tables for Epidemiologists: cc and cs commands. The test can also be conducted using the tab2 command, with the advantage that the tab2 command shows the expected counts. We will show why this is important shortly.\nThe Stata output for Worked Example 7.1 for the study on nausea is shown in Output 7.1a. It is always important to request the percentages using the options in ‘Cells’ to show the proportion of patients who have the outcome in each exposure group, i.e. the row percents.\n\n\nTable 1.1: Nausea side-effect data\n\n\n\n\n(a) Observed counts\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n35 (70%)\n15 (30%)\n50 (100%)\n\n\nPlacebo\n46 (92%)\n4 (8%)\n50 (100%)\n\n\nTotal\n81 (81%)\n19 (19%)\n100 (100%)\n\n\n\n\n\n\n(b) Expected counts\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n40.5\n9.5\n50\n\n\nPlacebo\n40.5\n9.5\n50\n\n\nTotal\n81\n19\n100\n\n\n\n\n\n\nWe can see from the row percentages that 8% of patients in the placebo group experienced nausea compared to 30% of patients in the active group. If no association existed, we would expect to find approximately the same percent of patients with nausea in each group. The ‘Expected’ counts are higher for the groups with ‘No nausea’ because ‘No nausea’ is more prevalent in the sample than ‘Nausea’.\nWhile the tab2 command will perform the chi-square test, the measure of effect is best obtained using the cs or cc commands, as discussed in Module 6. Using the relative risk obtained from Module 6, a conclusion from the above test can be written as:\nThe proportion with nausea in those who received the active drug is 30%, compared to 8% in those who received the placebo drug. Nausea was more frequent in those who received the active drug (Relative Risk = 3.75, 95% CI: 1.34 to 10.51). There is strong evidence that the proportion with nausea differs between the two groups (\\(\\chi ^2\\) = 7.86 with 1 df, P=0.005).\n\n1.2.1 Assumptions for using a Pearson’s chi-squared test\nThe assumptions that must be met when using Pearson’s chi-squared test are that:\n\neach observation must be independent;\neach participant is represented in the table once only;\nat least 80% of the expected cell counts should exceed a value of five;\nall expected cell counts should exceed a value of one.\n\nThe first two assumptions are dictated by the study design. The last two assumptions relate to the numbers in the cells and can be explored when running the test. There should not be too many cells with low expected counts. Sometimes, the only way to avoid small cell counts is to recruit a much larger sample size. If small cell counts cannot be avoided, Fisher’s exact test can be used instead. More information on Fisher’s exact test can be found in Chapter 13 of An Introduction to Medical Statistics, Bland (2015).\n\n\n1.2.2 Interpreting chi-squared tests\nFor the data being considered from Worked Example 7.1 all cells have an expected count greater than 5 and that the minimum cell count is 9.5. Therefore, it is appropriate to use the Pearson’s Chi-Squared test. If one or more cells have an expected cell count less than 5, then the Fisher’s exact test should be used. The P value associated with the Pearson’s chi-squared test is 0.005 indicating that we can reject the null hypothesis at a 5% level of significance. Thus, we can conclude that there is strong evidence that more patients who were randomised to receive the active treatment experienced nausea than patients randomised to the control (placebo) group."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "title": "1  Hypothesis testing for categorical data",
    "section": "1.3 Chi-squared tests for tables larger than 2-by-2",
    "text": "1.3 Chi-squared tests for tables larger than 2-by-2\nChi-squared tests can also be used for tables larger than a 2-by-2 dimension. When a contingency table larger than 2-by-2 is used, say a 4-by-2 table if there were 4 exposure groups, the Pearson’s chi-squared can still be used.\n\n1.3.1 Worked Example\nThe files mod07_allergy.dta and mod07_allergy.rds contain information about the severity of allergic reaction, coded as absent, slight, moderate or severe. We can test the hypothesis that the severity of allergy is not different between males and females. To do this we can use a two-way tabulation to obtain Table 1.2 which shows the counts, expected counts and the percent of females and males who fall into each severity group for allergy. The table shows that the percentage of males is higher in each of the categories of severity (slight, moderate, severe) than the percentage of females.\n\n\nTable 1.2: Allergy data\n\n\n\n\n(a) Observed counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n150 (62.0%)\n50 (20.7%)\n27 (11.2%)\n15 (6.2%)\n242 (100%)\n\n\nMale\n137 (53.1%)\n70 (27.1%)\n32 (12.4%)\n19 (7.4%)\n258 (100%)\n\n\nTotal\n287 (57.4%)\n120 (24.0%)\n59 (11.8%)\n34 (6.8%)\n500 (100.0%)\n\n\n\n\n\n\n\n\n(b) Expected counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n138.9\n58.1\n28.6\n16.5\n242.0\n\n\nMale\n148.1\n61.9\n30.4\n17.5\n258.0\n\n\nTotal\n287.0\n120.0\n59.0\n34.0\n500.0\n\n\n\n\n\n\nThe Pearson chi-squared statistic is calculated as 4.31, with 3 degrees of freedom, providing a P-value of 0.23. Therefore, there is little evidence of an association between gender and the severity of allergy."
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "href": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "title": "1  Hypothesis testing for categorical data",
    "section": "1.4 McNemar’s test for categorical paired data",
    "text": "1.4 McNemar’s test for categorical paired data\nIf a binary categorical outcome is measured in a paired study design, McNemar’s statistic is used. This statistic is a form of chi-square applied to a paired situation. A Pearson’s chi-squared test cannot be used because the measurements are not independent. However, McNemar’s test can be used to assess whether there is a significant change in proportions between two time points or between two conditions, or whether there is a significant difference in proportions between matched cases and controls.\nFor McNemar’s test, the data are displayed as shown in Table 1.3. Cells ‘a’ and ‘d’ called concordant cells because the response was the same at both baseline and follow-up or between matched cases and controls. Cells ‘b’ and ‘c’ are called discordant cells because the responses between the pairs were different. For a follow-up study, the participants in cell ‘c’ had a positive response at baseline and a negative response at follow-up. Conversely, the participants in cell ‘b’ had a negative response at baseline and a positive response at follow-up.\nFor other types of paired data such as twins or matched cases and controls, the data are similarly displayed with the responses of one of the pairs in the columns and the responses for the other of the pairs in the rows. For paired data, the grand total ‘N’ is always the number of pairs and not the total number of participants.\n\n\n\n\nTable 1.3: Table layout for testing matched proportions\n\n\n\nNegative at follow-up\nPositive at follow-up\nTotal\n\n\nNegative at baseline\na\nb\na + b\n\n\nPositive at baseline\nc\nd\nc + d\n\n\nTotal\na + c\nb + d\nN\n\n\n\n\n\n\n\n\n\n1.4.1 Worked Example 7.3\nTwo drugs labelled A and B have been administered to patients in random order so that each patient acts as their own control. The datasets mod07_drug_response.dta and mod07_drug_response.rds are available on Moodle. The null hypothesis is as follows:\n\nH0: The proportion of patients who do better on drug A is the same as the proportion of patients who do better on drug B\n\nCounts and overall percentages are presented in . From the “Total” row in the table, we can see that the number of patients who respond to drug A is 41 (68%) and from the “Total” column the number who respond to drug B is less at 35 (58%), that is there is a difference of 10%.\n\nPaired data\n\n\n\n\n\n\n\n\n\nResponse to Drug B\nNo response to Drug B\nTotal\n\n\n\n\nResponse to Drug A\n21 (35%)\n20 (33%)\n41 (68%)\n\n\nNo response to Drug A\n14 (23%)\n5 (8%)\n19 (32%)\n\n\nTotal\n35 (58%)\n25 (42%)\n60 (100%)\n\n\n\nThe difference in the paired proportions is calculated using the simple equation:\n\\[ p_{A} - p_{B} = \\frac{(b - c)}{N} \\]\nHere, \\(p_{A} - p_{B} = \\frac{(20 - 14)}{60} = 0.1\\)\nThe cell counts show that 20 patients responded to Drug A but not to drug B, and 14 patients responded to Drug B but not to drug A. McNemar’s statistic is computed from these two discordant pairs (labelled as ‘b’ and ‘c’) as follows:\n\\[ X^2 = \\frac{(b-c)^2}{b+c} \\]\nwith 1 degree of freedom. Using our worked example, the McNemar’s chi-squared statistic is calculated as 1.06 with 1 degree of freedom, giving a P-value of 0.3.\nAs described above, the difference in proportions can be calculated. A 95% confidence interval for this difference can be obtained using statistical software.\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-square=1.06 with 1 degree of freedom, P=0.3)."
  },
  {
    "objectID": "07-testing-proportions.html#summary",
    "href": "07-testing-proportions.html#summary",
    "title": "1  Hypothesis testing for categorical data",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn Module 6, we estimated proportions and measures of association for categorical data and conducted a one-sample test of proportions. In this module, we conduct significance tests for two or more independent proportions using the chi-squared test. The chi-squared test can also be used to conduct a significance test when there are more than two categories in both variables. The McNemar’s test is used when we have paired data.\n\n\n\n\nAcock, Alan C. 2010. A Gentle Introduction to Stata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "09-non-parametrics.html",
    "href": "09-non-parametrics.html",
    "title": "2  Analysing non-normal data",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "09-non-parametrics.html#learning-objectives",
    "href": "09-non-parametrics.html#learning-objectives",
    "title": "2  Analysing non-normal data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nTransform non-normally distributed variables;\nExplain the purpose of non-parametric statistics and key principles for their use;\nCalculate ranks for variables;\nConduct and interpret a non-parametric independent samples significance test;\nConduct and interpret a non-parametric paired samples significance test;\nCalculate and interpret the Spearman rank correlation coefficient."
  },
  {
    "objectID": "09-non-parametrics.html#readings",
    "href": "09-non-parametrics.html#readings",
    "title": "2  Analysing non-normal data",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 13. [UNSW Library Link]\nBland (2015); Chapter 12. [UNSW Library Link]\nAcock (2010); Section 7.11."
  },
  {
    "objectID": "09-non-parametrics.html#introduction",
    "href": "09-non-parametrics.html#introduction",
    "title": "2  Analysing non-normal data",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn general, parametric statistics are preferred for reporting data because the summary statistics (mean, standard deviation, standard error of the mean etc) and the tests used (t-tests, correlation, regression etc) are familiar and the results are easy to communicate. However, non-parametric tests can be used if data are not normally distributed. Non-parametric tests make fewer assumptions about the distribution of the data."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "title": "2  Analysing non-normal data",
    "section": "2.2 Transforming non-normally distributed variables",
    "text": "2.2 Transforming non-normally distributed variables\nWhen a variable has a skewed distribution, one possibility is to transform the data to a new variable to try and obtain a normal or near normal distribution. Methods to transform non-normally distributed data include logarithmic transformation of each data point, or using the square root or the square or the inverse (i.e. 1/x) etc.\n\n2.2.1 Worked Example\nWe have data from 132 patients who had a hospital stay following admission to ICU available on Moodle (mod09_infection.dta and mod09_infection.rds). The distribution of the length of stay for these patients is shown in the histogram in Figure 2.1. As is common with variables that record time, the data are skewed with many patients having relatively short stays and a few patients having very long hospital stays. Clearly, it would be inappropriate to use parametric statistical methods for these data.\n\n\n\n\n\nFigure 2.1: Length of hospital stay for 132 patients\n\n\n\n\nWhen data are positively skewed, as shown in Figure 2.1, a logarithmic transformation can often make the data closer to being normally distributed. This is the most common transformation used. You should note, however, that the logarithmic function cannot handle 0 or negative values. One way to deal with zeros in a set of data is to add 1 to each value before taking the logarithm.\nWe would generate a new variable, as shown in the Stata or R notes. As the minimum length of stay in these sample data was 0, we have added 1 to each length of stay before taking the logarithm. The distribution of the logarithm of (length of stay + 1) is shown in Figure 2.2.\n\n\n\n\n\nFigure 2.2: istribution of log transformed (length of stay + 1)\n\n\n\n\nThe distribution now appears much more bell shaped. Table 2.1 shows the descriptive statistics for length of stay before and after logarithmic transformation. Before transformation, the SD is almost as large as the mean value which indicates that the data are skewed and that these statistics are not an accurate description of the centre and spread of the data.\n\n\nTable 2.1: Summary statistics for untransformed and transformed length of stay\n\n\n\n\n\n\n\n\nLength of stay\nlog(Length of stay + 1)\n\n\n\n\nMean (Standard deviation)\n38.1 (35.78)\n3.41 (0.715)\n\n\nMean: 95% confidence interval\n31.9 to 44.2\n3.29 to 3.53\n\n\nMedian [Interquartile range]\n27 [21 to 42]\n3.3 [3.1 to 3.8]\n\n\nRange\n0 to 244\n0 to 5.5\n\n\n\n\nThe mean and standard deviation of the transformed length of stay are in log base e (i.e. ln) units. If we raise the mean of the log of length of stay to the power of \\(e\\), it returns a value of 30.2 days (\\(e^{3.41}=30.2\\)).\nTechnically, this is called the geometric mean of the data, and it has a different interpretation to the usual mean, the arithmetic mean. This is a much better estimate in this case of the “average” length of stay than the mean of 38.1 days (95% CI 31.9, 44.2 days) obtained from the non-transformed positively skewed data. Note that, if you have added 1 to your data to deal with 0 values, the back-transformed estimate is approximately equal to the geometric mean.\nThis set of data also includes a variable summarising whether a patient acquired a nosocomial infection (also known as healthcare-associated infections), which are infections that develop while undergoing medical treatment but were absent at the time of admission.\nIf we were testing the hypothesis that there was a difference in length of stay between groups (status of nosocomial infection), t-tests should not be used with length of stay, but could be used for the log transformed variable, which is approximately normally distributed. The output from the t-test of the log-transformed length of stay is shown in Table 2.2. This is done using the t-test shown in Module 5.\n\n\nTable 2.2: Summary statistics for transformed length of stay\n\n\nNosocomial infection\nn\nMean (SE)\n95% Confidence interval\n\n\n\n\nNo\n106\n3.33 (0.068)\n3.19 to 3.46\n\n\nYes\n26\n3.73 (0.136)\n3.45 to 4.01\n\n\nDifference (Yes - No)\n\n0.39 (0.153)\n0.09 to 0.70\n\n\n\n\nHere, a two-sample t-test gives a test statistic of 2.59 with 130 degrees of freedom, and a P-value of 0.01.\nAs explained above, the estimated statistics would need to be converted back to the units in which the variable was measured. From Table 2.2, we can take the exponential of the corresponding log-transformed values:\n\nthe geometric mean of the infected group is approximately 41.5 days with a 95% confidence interval from 31.4 to 55.0 days.\nthe geometric mean of the uninfected group is approximately 27.9 days with a 95% confidence interval from 24.4 to 31.9 days."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-significance-tests",
    "href": "09-non-parametrics.html#non-parametric-significance-tests",
    "title": "2  Analysing non-normal data",
    "section": "2.3 Non-parametric significance tests",
    "text": "2.3 Non-parametric significance tests\nIt is often not possible or sensible to transform a non-normal distribution, for example if there are too many zero values or when we simply want to compare groups using the unit in which the measurement was taken (e.g. length of stay). For this, non-parametric significance tests can be used but the general idea behind these tests is that the data values are replaced by ranks. This also protects against outliers having too much influence.\n\n2.3.1 Ranking variables\nTable 9.1 shows how ranks are calculated for the first 21 patients in the length-of-stay data. First the data are sorted in order of their magnitude (from the lowest value to the highest) ignoring the group variable. Each data point is then assigned a rank. Data points that are equal are assigned the mean of their ranks. Thus, the two lengths of stay of 11 days share the ranks 4 and 5, and have a mean rank of 4.5. Similarly, there are 5 people with a length of stay of 14 days and these share the ranks 9 to 13, the mean of which is 11. Once ranks are computed they are assigned to each of the two groups and summed within each group.\n\n\n\nTransforming data to ranks: first 21 participants\n\n\nID\nInfection\nLength of stay\nRank\nInfection=no\nInfection=Yes\n\n\n32\nNo\n0\n1\n1\n\n\n\n33\nNo\n1\n2\n2\n\n\n\n12\nNo\n9\n3\n3\n\n\n\n22\nNo\n11\n4.5\n4.5\n\n\n\n16\nNo\n11\n4.5\n4.5\n\n\n\n28\nYes\n12\n6\n\n6\n\n\n27\nNo\n13\n7.5\n7.5\n\n\n\n20\nNo\n13\n7.5\n7.5\n\n\n\n24\nNo\n14\n11\n11\n\n\n\n11\nNo\n14\n11\n11\n\n\n\n130\nNo\n14\n11\n11\n\n\n\n10\nNo\n14\n11\n11\n\n\n\n25\nNo\n14\n11\n11\n\n\n\n19\nNo\n15\n15.5\n15.5\n\n\n\n30\nNo\n15\n15.5\n15.5\n\n\n\n23\nNo\n15\n15.5\n15.5\n\n\n\n14\nNo\n15\n15.5\n15.5\n\n\n\n15\nNo\n17\n20.5\n20.5\n\n\n\n13\nNo\n17\n20.5\n20.5\n\n\n\n21\nYes\n17\n20.5\n\n20.5\n\n\n17\nNo\n17\n20.5\n20.5\n\n\n\n\n\n\n\n\nBy assigning ranks to individuals, we lose information about their actual values and this makes it more difficult to detect a difference. However, outliers and extreme values in the data are brought back closer to the data so that they are less influential. For this reason, non-parametric tests have less power than parametric tests and they require much larger differences in the data to show statistical significance between groups."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "title": "2  Analysing non-normal data",
    "section": "2.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)",
    "text": "2.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)\nThe non-parametric equivalent to an independent samples t-test (Module 5) is the Wilcoxon ranked sum test, also known as the Mann-Whitney U test. This can be obtained using the ranksum command in Stata, and the wilcox.test in R.\nThe assumption for this test is that the distributions of the two populations have the same general shape. If this assumption is met, then this test evaluates the null hypothesis that the medians of the two populations are equal. This test does not assume that the populations are normally distributed, nor that their variances are equal.\nConducting the Wilcoxon ranked sum test for our length of stay data yields a P-value of 0.014, providing strong evidence of a difference in the median length of stay between the groups.\nThis P-value should be provided alongside non-parametric summary statistics such as medians and inter-quartile ranges. In our example, we can obtain the median length of stay values of 24 (Interquartile Range: 19 to 40 days) in the group with no infection and 37 (Interquartile Range: 24 to 50 days) in the group with infection."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "title": "2  Analysing non-normal data",
    "section": "2.5 Non-parametric test for paired data (Wilcoxon signed-rank test)",
    "text": "2.5 Non-parametric test for paired data (Wilcoxon signed-rank test)\nThere are two types of non-parametric tests for paired data, called the Sign test and the Wilcoxon signed rank test. In practice, the Sign test is rarely used and will not be discussed in this course.\nIf the differences between two paired measurements are not normally distributed, a non-parametric equivalent of a paired t-test (Module 5) should be used. The equivalent test is the Wilcoxon matched-pairs signed rank test, also simply called the Wilcoxon matched-pairs test. This test is resistant to outliers in the data, however the proportion of outliers in the sample should be small. This test evaluates the null hypothesis that the median of the paired differences is equal to zero.\nIn this test, the absolute differences between the paired scores are ranked and the difference scores that are equal to zero (i.e. scores where there is no difference between the pairs) are excluded. Thus, the test is not suitable when a large proportion of the differences are zero because the effective sample size is reduced considerably.\n\n2.5.1 Worked Example\nA crossover trial is done to compare symptom scores for two drugs in 11 people with arthritis (higher scores indicate more severe symptoms). The data are contained in Stata datafile file mod09_arthritis.csv. The data are shown in Table 9.2. The descriptive statistics indicate that the differences are not normally distributed. You can use the Explore function in Stata to determine this.\n\n\n\nArthritis symptom scores for 11 patients after administering two drugs\n\n\nPatient ID\nScore: Drug 1\nScore: Drug 2\nDifference (Drug 2 – Drug 1)\n\n\n1\n3\n4\n1\n\n\n2\n2\n7\n5\n\n\n3\n3\n4\n1\n\n\n4\n8\n10\n2\n\n\n5\n6\n8\n2\n\n\n6\n6\n1\n-5\n\n\n7\n2\n6\n4\n\n\n8\n3\n7\n4\n\n\n9\n5\n8\n3\n\n\n10\n9\n10\n1\n\n\n11\n7\n8\n1\n\n\n\n\n\n\n\nBefore doing the analysis let us examine the distribution of the difference of symptom scores between the two drugs. As in Module 5, we first need to compute the difference between the symptom scores. To examine the distribution, we plot a histogram as shown in Figure @ref(fig:mod09-diff-hist).\n\n\n\n\n\nDistribution of difference in symptom scores between Drug 1 and Drug 2\n\n\n\n\nThe histogram shows that the differences are not normally distributed. The data looks negatively skewed with a gap in the histogram between the values of -5 and 0. Therefore, it would not be appropriate to conduct a paired t-test. Hence, we conduct a non-parametric paired test (Wilcoxon matched-pairs signed-rank test).\nA non-parametric paired test can be obtained in Stata using the signrank command and the results of the test are shown in Output 9.4.\n\nStata Output 9.4: Wilcoxon matched-pairs signed-rank test\n\nWilcoxon signed-rank test\n\n        sign |      obs   sum ranks    expected\n-------------+---------------------------------\n    positive |        1        10.5          33\n    negative |       10        55.5          33\n        zero |        0           0           0\n-------------+---------------------------------\n         all |       11          66          66\n\nunadjusted variance      126.50\nadjustment for ties       -1.63\nadjustment for zeros       0.00\n                     ----------\nadjusted variance        124.88\n\nHo: drug_1 = drug_2\n             z =  -2.013\n    Prob &gt; |z| =   0.0441\n    Exact Prob =   0.0459\n\nThe table in Output 9.4 shows that there is 1 person who has a positive difference, where the symptom score on drug 2 that is smaller than that for drug 1 (i.e., drug 2 is better than drug 1); and 10 people who have a negative difference. No one has the same score for both drugs. The difference scores are ranked and the observed and expected sum of the ranks are shown in the output. This provides no intuitive summary statistics except to indicate which drug has higher ranks.\nThe test statistics are also shown under the table in Output 9.4. From the output, the exact P value of 0.046 indicates that there is evidence of a difference in symptom score between the two drugs."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "href": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "title": "2  Analysing non-normal data",
    "section": "2.6 Non-parametric estimates of correlation",
    "text": "2.6 Non-parametric estimates of correlation\nEstimating correlation using Pearson’s correlation coefficient can be problematic when bivariate Normality cannot be assumed, or in the presence of outliers or skewness. There are two commonly used non-parametric alternatives to Pearson’s correlation coefficient: Spearman’s rank correlation (\\(\\rho\\) or rho), and Kendall’s rank correlation (\\(\\tau\\) or tau).\nWhen estimating the correlation between x and y, Spearman’s rank correlation essentially replaces the observations x and y by their ranks, and calculates the correlation between the ranks. Kendall’s rank correlation compares the ranks between every possible combination of pairs of data to measure concordance: whether high values for x tend to be associated with high values for y (positively correlated) or low values of y (negatively correlated).\nIn terms of which is the more appropriate measure to use, the following passage from An Introduction to Medical Statistics (Bland (2015)) provides some guidance:\n\n“Why have two different rank correlation coefficients? Spearman’s \\(\\rho\\) is older than Kendall’s \\(\\tau\\), and can be thought of as a simple analogue of the product moment correlation coefficient, Pearson’s r. Kendall’s \\(\\tau\\) is a part of a more general and consistent system of ranking methods, and has a direct interpretation, as the difference between the proportions of concordant and discordant pairs. In general, the numerical value of \\(\\rho\\) is greater than that of \\(\\tau\\). It is not possible to calculate \\(\\tau\\) from \\(\\rho\\) or \\(\\rho\\) from \\(\\tau\\), they measure different sorts of correlation. \\(\\rho\\) gives more weight to reversals of order when data are far apart in rank than when there is a reversal close together in rank, \\(\\tau\\) does not. However, in terms of tests of significance, both have the same power to reject a false null hypothesis, so for this purpose it does not matter which is used.”\n\nWe will illustrate estimating rank correlation using the data mod08_lung_function.dta or mod08_lung_function.rds, which has information about height and lung function collected from a sample of 120 adults.\nThe Spearman rank correlation coefficient is estimated as 0.75, demonstrating a positive association between height and FVC. The Kendall rank correlation coefficient is estimated as 0.56, again demonstrating a positive association between height and FVC."
  },
  {
    "objectID": "09-non-parametrics.html#summary",
    "href": "09-non-parametrics.html#summary",
    "title": "2  Analysing non-normal data",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nIn this module, we have presented methods to conduct a hypothesis test with data that are not normally distributed. Non-parametric methods do not assume any distribution for the data and use significance tests based on ranks or sign (or both). A non-parametric test is always less powerful than its equivalent parametric test if the data are normally distributed and so whenever possible parametric significance tests should be used. In some cases when data are not normally distributed with a reasonably large sample size, the data can be transformed (most commonly by log transformation) to make the distribution normal. A parametric significance test should then be used with the transformed data to test the hypothesis."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "title": "2  Analysing non-normal data",
    "section": "2.8 Transforming non-normally distributed variables",
    "text": "2.8 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram (for example, Figure 9.1).\nYou can calculate a new, transformed, variable using the generate command in Stata from the menu Data &gt; Create or change data &gt; Create new variable. Below are the instructions for creating a log to the base e, referred to as “ln” of the length of stay data for mod09_infection.dta.\nGo to Data &gt; Create or change data &gt; Create new variable. In the generate dialog box, type a name for your new variable into the Variable name: box: for example, ln_los. To include the ln function, you can either:\n\nsimply type ln(los + 1) directly into the Specify a value or an expression text box, or;\nclick the Create button to bring up the Expression Builder dialog box. Double click on Functions to expand the list in the Categories box, then click on Mathematical to display the list of mathematical functions in the box on the right. Scroll down to the ln() function and double click on it to bring it to the main text box. Next scroll down the Categories box to Variables and click on it to check the variables you have in the dataset. In the text box, replace x with los + 1 as shown below.\n\n\n\n\n\n\nClick the OK button when you are done to transfer the expression to the Specify a value or an expression box in the generate dialog box as shown below.\n\n\n\n\n\nClick OK and the new variable will appear in your dataset. You can check in your Variables window or your Data Editor window.\n[Command: gen ln_los=ln(los + 1)]\nYou can now check whether this variable is normally distributed as described in Module 2, for example with the histogram command as shown in Figure 9.2.\nTo obtain the back-transformed mean shown in Output 9.1, go to Data &gt; Other Utilities &gt; Hand calculator. In the display dialog box, expand the Functions list in the Categories box and select Mathematical. On the right-hand-side box, double-click on exp(). Replace x with the mean, 3.407232 as shown below.\nClick OK when you are done, then OK or Submit in the display dialog box.\n\n\n\n\n\n[Command: di exp(3.407232)]\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results."
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "title": "2  Analysing non-normal data",
    "section": "2.9 Wilcoxon ranked-sum test",
    "text": "2.9 Wilcoxon ranked-sum test\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.dta. To perform the Wilcoxon ranked-sum test go to: Statistics &gt; Summaries, tables, and tests &gt; Nonparametric tests of hypotheses &gt; Wilcoxon ranked-sum test.\nIn the ranksum dialog box, select los as the Variable and select infect as the Grouping variable as shown below.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\nTwo-sample Wilcoxon rank-sum (Mann-Whitney) test\n\n      infect |      obs    rank sum    expected\n-------------+---------------------------------\n          No |      106        6620        7049\n         Yes |       26        2158        1729\n-------------+---------------------------------\n    combined |      132        8778        8778\n\nunadjusted variance    30545.67\nadjustment for ties      -53.87\n                     ----------\nadjusted variance      30491.80\n\nHo: los(infect==No) = los(infect==Yes)\n             z =  -2.457\n    Prob &gt; |z| =   0.0140\n    Exact Prob =   0.0135\n\nFor the length of stay data in the Worked Example 9.1, we first get a ranks table as shown in Output 9.3. The rank sum table gives us a direction of effect that the ranks are higher than expected in patients who had nosocomial infection. While the positive infection group has a lower sum of ranks because there were fewer people who contracted an infection, it is higher than expected, i.e. they have a longer length of stay compared with the negative infection group. This ranks table does not provide any summary statistics of direction of effect, central tendency or spread that describe the data.\nThe test statistics are shown under the rank sum table in Output 9.3. The variance shown immediately under the table are used to conduct the test, and are not reported on.\nFrom Output 9.3, there are two P-values shown: one assuming normality of the ranks (not the underlying data), and an “Exact” P-value. The exact P-value is calculated when the sample size is not too large (less than 200), and is preferred. The rounded exact P value is 0.014 which indicates that the there is evidence of a difference in length of stay between the groups.\n[Command: ranksum los, by(infect)]"
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "title": "2  Analysing non-normal data",
    "section": "2.10 Wilcoxon matched-pairs signed-rank test",
    "text": "2.10 Wilcoxon matched-pairs signed-rank test\nThe Wilcoxon matched-pairs signed-rank test in Stata will be demonstrated using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.\nTo do the analysis, go to: Statistics &gt; Summaries, tables, and tests &gt; Nonparametric tests of hypotheses &gt; Wilcoxon matched-pairs sign-rank test. In the signrank dialog box, select drug_1 in the Variable box and type drug_2 in the Expression box. The dialog box will look like:\n\n\n\n\n\nClick OK or Submit to obtain Output 9.4. [Command: signrank drug_1 = drug_2]"
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "title": "2  Analysing non-normal data",
    "section": "2.11 Estimating rank correlation coefficients",
    "text": "2.11 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways.\nStatistics &gt; Nonparametric analysis &gt; Tests of hypotheses &gt; Spearman’s rank correlation\n\n\n\n\n\nGiving the following output:\n\n. spearman Height FVC\n\n Number of obs =     120\nSpearman's rho =       0.7476\n\nTest of Ho: Height and FVC are independent\n    Prob &gt; |t| =       0.0000\n\nStatistics &gt; Nonparametric analysis &gt; Tests of hypotheses &gt; Kendall’s rank correlation\n\n\n\n\n\n\n. ktau Height FVC\n\n  Number of obs =     120\nKendall's tau-a =       0.5431\nKendall's tau-b =       0.5609\nKendall's score =    3878\n    SE of score =     439.463   (corrected for ties)\n\nTest of Ho: Height and FVC are independent\n     Prob &gt; |z| =       0.0000  (continuity corrected)\n\nStata provides two versions of the Kendall rank correlation coefficient: we would use tau-b (\\(\\tau_b\\)) as it allows for tied observations."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "title": "2  Analysing non-normal data",
    "section": "2.12 Transforming non-normally distributed variables",
    "text": "2.12 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram.\nYou can calculate a new, transformed, variable using standard commands. For example, to create a new column of data based on the log of length of stay:\n\nlibrary(jmv)\n\nhospital &lt;- readRDS(\"data/examples/mod09_infection.rds\")\n\nhospital$ln_los &lt;- log(hospital$los+1)\ndescriptives(data=hospital, vars=c(los, ln_los))\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         los         ln_los      \n ─────────────────────────────────────────────── \n   N                          132          132   \n   Missing                      0            0   \n   Mean                  38.05303     3.407232   \n   Median                27.00000     3.332205   \n   Standard deviation    35.78057    0.7149892   \n   Minimum               0.000000     0.000000   \n   Maximum               244.0000     5.501258   \n ─────────────────────────────────────────────── \n\n\nYou can now check whether this logged variable is normally distributed as described in Module 2, for example by plotting a histogram as shown in Figure 9.2.\nTo obtain the back-transformed mean, we can use the exp command to anti-log the mean:\n\nexp(3.407232)\n\n[1] 30.18159\n\n\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results."
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "title": "2  Analysing non-normal data",
    "section": "2.13 Wilcoxon ranked-sum test",
    "text": "2.13 Wilcoxon ranked-sum test\nWe use the wilcox.test function to perform the Wilcoxon ranked-sum test:\nwilcox.test(continuous_variable ~ group_variable, data=df)\nNote that the implementation of R’s Wilcoxon rank-sum test uses a “continuity correction” for calculating the P-value from the ranks. This differs from Stata which does not use the continuity correction. While the use of the continuity correction is preferable, in most cases the difference in P-values between the methods will be minimal.\nTo obtain results that are consistent with Stata, the correct=FALSE option can be used:\nwilcox.test(continuous_variable ~ group_variable, data=df, correct=FALSE)\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.rds. Here, out continuous variable is los and the grouping variable is infect.\n\nwilcox.test(los ~ infect, data=hospital)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  los by infect\nW = 949, p-value = 0.01413\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "title": "2  Analysing non-normal data",
    "section": "2.14 Wilcoxon matched-pairs signed-rank test",
    "text": "2.14 Wilcoxon matched-pairs signed-rank test\nThe wilcox.test function can also be used to conduct the Wilcoxon matched-pairs signed-rank test. The specification of the variables is a little different, in that each variable is specified as dataframe$variable:\nwilcox.test(df$continuous_variable_1, df$continuous_variable_1, paired=TRUE)\nWe will demonstrate using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.\n\narthritis &lt;- read.csv(\"data/examples/mod09_arthritis.csv\")\n\narthritis$difference = arthritis$drug_1 - arthritis$drug_2\n\nhist(arthritis$difference, xlab=\"Difference\", main=\"Histogram of differences in pain scores\")\n\n\n\nwilcox.test(arthritis$drug_1, arthritis$drug_2, \n            paired=TRUE)\n\nWarning in wilcox.test.default(arthritis$drug_1, arthritis$drug_2, paired =\nTRUE): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  arthritis$drug_1 and arthritis$drug_2\nV = 10.5, p-value = 0.04898\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "title": "2  Analysing non-normal data",
    "section": "2.15 Estimating rank correlation coefficients",
    "text": "2.15 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways:\n\nlung &lt;- readRDS(\"data/examples/mod08_lung_function.rds\")\n\ncor.test(lung$Height, lung$FVC, method=\"spearman\")\n\nWarning in cor.test.default(lung$Height, lung$FVC, method = \"spearman\"): Cannot\ncompute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lung$Height and lung$FVC\nS = 72699, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7475566 \n\ncor.test(lung$Height, lung$FVC, method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  lung$Height and lung$FVC\nz = 8.8244, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5609431"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acock, Alan C. 2010. A Gentle Introduction to\nStata. 3rd ed. College Station, Tex:\nStata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical\nStatistics. 4th ed. Oxford, New York:\nOxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of\nMedical Statistics. 2nd ed. Malden, Mass:\nWiley-Blackwell."
  }
]