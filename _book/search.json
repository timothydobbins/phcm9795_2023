[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Course introduction\nWelcome to PHCM9795 Foundations of Biostatistics.\nThis introductory course in biostatistics aims to provide students with core biostatistical skills to analyse and present quantitative data from different study types. These are essential skills required in your degree and throughout your career.\nWe hope you enjoy the course and will value your feedback and comment throughout the course."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course information",
    "text": "Course information\nBiostatistics is a foundational discipline needed for the analysis and interpretation of quantitative information and its application to population health policy and practice.\nThis course is central to becoming a population health practitioner as the concepts and techniques developed in the course are fundamental to your studies and practice in population health. In this course you will develop an understanding of, and skills in, the core concepts of biostatistics that are necessary for analysis and interpretation of population health data and health literature.\nIn designing this course, we provide a learning sequence that will allow you to obtain the required graduate capabilities identified for your program. This course is taught with an emphasis on formulating a hypothesis and quantifying the evidence in relation to a specific research question. You will have the opportunity to analyse data from different study types commonly seen in population health research.\nThe course will allow those of you who have covered some of this material in your undergraduate and other professional education to consolidate your knowledge and skills. Students exposed to biostatistics for the first time may find the course challenging at times. Based on student feedback, the key to success in this course is to devote time to it every week. We recommend that you spend an average of 10-15 hours per week on the course, including the time spent reading the course notes and readings, listening to lectures, and working through learning activities and completing your assessments. Please use the resources provided to assist you, including online support."
  },
  {
    "objectID": "index.html#units-of-credit",
    "href": "index.html#units-of-credit",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Units of credit",
    "text": "Units of credit\nThis course is a core course of the Master of Public Health, Master of Global Health and Master of Infectious Diseases Intelligence programs and associated dual degrees, comprising 6 units of credit towards the total required for completion of the study program. A value of 6 UOC requires a minimum of 150 hours work for the average student across the term."
  },
  {
    "objectID": "index.html#course-aim",
    "href": "index.html#course-aim",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course aim",
    "text": "Course aim\nThis course aims to provide students with the core biostatistical skills to apply appropriate statistical techniques to analyse and present population health data."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this course, you will be able to:\n\nSummarise and visualise data using statistical software.\nDemonstrate an understanding of statistical inference by interpreting p-values and confidence intervals.\nApply appropriate statistical tests for different types of variables given a research question, and interpret computer output of these tests appropriately.\nDetermine the appropriate sample size when planning a research study.\nPresent and interpret statistical findings appropriate for a population health audience."
  },
  {
    "objectID": "06-proportions.html",
    "href": "06-proportions.html",
    "title": "1  Summary statistics for binary data",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "06-proportions.html#learning-objectives",
    "href": "06-proportions.html#learning-objectives",
    "title": "1  Summary statistics for binary data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nCompute and interpret 95% confidence intervals for proportions;\nConduct and interpret a significance test for a one-sample proportion;\nUse Stata to compute 95% confidence intervals for a difference in proportions, a relative risk and an odds ratio."
  },
  {
    "objectID": "06-proportions.html#readings",
    "href": "06-proportions.html#readings",
    "title": "1  Summary statistics for binary data",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 16 [UNSW Library Link]\nBland (2015); Section 8.6, Section 13.7 [UNSW Library Link]\nAcock (2010); Section 7.5."
  },
  {
    "objectID": "06-proportions.html#introduction",
    "href": "06-proportions.html#introduction",
    "title": "1  Summary statistics for binary data",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIn Modules 4 and 5, we discussed methods used to test hypotheses when the data are continuous. In Modules 6 and 7, we will focus on hypothesis testing for binary categorical data.\nIn health research, we often collect information that can be put into two categories, e.g. male and female, disease present or disease absent etc. Binary categorical variables such as these are summarised using proportions."
  },
  {
    "objectID": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "href": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "title": "1  Summary statistics for binary data",
    "section": "1.2 Calculating proportions and 95% confidence intervals",
    "text": "1.2 Calculating proportions and 95% confidence intervals\n\n1.2.1 Calculating a proportion\nWe need two pieces of information to calculate a proportion: \\(n\\), the number of trials, and \\(k\\), the number of ‘successes’. Note that we use the term ‘success’ to describe the outcome of interest, recognising that a success may be a adverse outcome such as death or disease.\nThe following formula is used to calculate the proportion, \\(p\\):\n\\[ p = k / n \\]\nThe proportion, \\(p\\), is a number that lies between 0 and 1. Proportions and their confidence intervals can easily be converted to percentages by multiplying by 100 once computed.\nAs for all summary statistics, it is useful to compute the precision of the estimate as a 95% confidence interval (CI) to indicate the range of values in which are 95% confident that the true population value lies. In this module, we present two methods for computing a 95% confidence interval around a proportion.\n\n\n1.2.2 Calculating the 95% confidence interval of a proportion (Wald method)\nThe Wald method for calculating the 95% confidence interval is based on assuming that the proportion, \\(p\\), is Normally distributed. This assumption is reasonable if the sample is sufficiently large (for example, if \\(n>30\\)) and if \\(n \\times (1-p)\\) and \\(n \\times p\\) are both larger than 5.\nThe Wald method for calculating a 95% confidence interval is given by:\n\\[\\text{95\\% CI} = p \\pm (1.96 \\times \\text{SE}(p))\\]\nwhere the standard error of a proportion is computed as:\n\\[\\text{SE}(p) = \\sqrt{\\frac{p \\times (1 - p)}{n}}\\]\n\n\n1.2.3 Worked Example\nIn a cross-sectional study of children living in a rural village, 47 children from a random sample of 215 children were found to have scabies. Here \\(n=215\\) and \\(k=47\\), so the proportion of children with scabies is estimated as:\n\\[ p = \\frac{47}{215} = 0.2186 \\]\nGiven the large sample size and the number of children with the rarer outcome is larger than 5, the Wald method is used to calculate the standard error of the proportion as:\n\\[{\\text{SE}\\left( p \\right) = \\sqrt{\\frac{0.2186 \\times (1 - 0.2186)}{215}}\n}{= 0.02819}\\]\nThen, the 95% confidence interval is estimated as:\n\\[\\text{95\\% CI} = 0.2816 \\pm 1.96 \\times 0.02819\\]\n\\[= 0.1634 \\text{ to } 0.2739\\]\nThe prevalence of scabies among children in the village is 21.9% (95% CI 16.3%, 27.4%). These values tell us that we are 95% confident that the true prevalence of scabies among children in the village is between 16.3% and 27.4%.\n\n\n1.2.4 Calculating the 95% confidence interval of a proportion (Wilson method)\nAnother method to calculate the confidence interval of a proportion is the Wilson (sometimes also called the ‘score’) method. We can use it in situations where it is not appropriate to use the normal approximation to the binomial distribution as described above i.e. if the sample size is small (\\(n < 30\\)) or the number of subjects with the rarer outcome is 5 or fewer. This method much more difficult to implement by hand than the standard confidence interval, and so we will not discuss the hand calculation using the mathematical equation in this course. Instead, we use statistical software to do this.\nWhen using software, our worked example provides a 95% confidence interval of the prevalence of scabies of 16.9% to 27.9%.\n\n\n1.2.5 Wald vs Wilson methods\nWe have presented two methods for calculating the 95% confidence interval for a proportion. The Wald method, which assumes that the underlying proportion follows a Normal distribution, is easy to calculate and follows the form of other confidence intervals. The Wilson method, which is difficult to calculate by hand, has nicer mathematical properties. There are also a number of other methods for calculating confidence intervals for proportions, but we do not discuss these in this course.\nA paper by Brown, Cai and DasGupta (Brown, Cai, and DasGupta (2001)) has compared the properties of the Wald and Wilson methods (among others) and concluded that the Wilson method is preferred over the Wald method."
  },
  {
    "objectID": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "href": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "title": "1  Summary statistics for binary data",
    "section": "1.3 Hypothesis testing for one sample proportion",
    "text": "1.3 Hypothesis testing for one sample proportion\nWe can carry out a hypothesis test to compare a sample proportion to a hypothesised proportion. In much the same way as a one sample t-test was used in Module 5 to test a sample mean against a hypothesised mean, we can perform a one-sample test to test a sample proportion against a hypothesised proportion. The significance test will provide a P value to assess the evidence against the null hypothesis, while the 95% confidence interval will provide the range in which we are 95% confident that the true proportion lies.\nFor example, we can test the following null hypothesis:\nH0: sample proportion is not different from the hypothesised proportion\nMuch like constructing a 95% confidence interval, there are two main options when performing a hypothesis test on a single proportion: the first assumes that the proportion follows a Normal distribution, while the second relaxes this assumption.\n\n1.3.1 z-test for testing one sample proportion\nThe first step in the z-test is to calculate a z-statistic, which is then used to calculate a P-value. The z-statistic is calculated as the difference between the population proportion and the sample proportion divided by the standard error of the population proportion, i.e.\n\\[\nz = \\frac{(p_{population} - p_{sample})}{\\text{SE}(p_{population})}\n\\]\nThis z-statistic is then compared to the standard Normal distribution to calculate the P-value.\n\n\n1.3.2 Worked Example\nA national census in a country shows that 20% of the population are smokers. A survey of a community within the country that has received a public health anti-smoking intervention shows that 54 of 300 people sampled are smokers (18%). We can calculate a 95% confidence interval around this proportion using the Wilson method, which is calculated as 14.1% to 22.7%.\nThe researchers are interested in whether the proportion of smoking in this community is the same as the population prevalence of smoking of 20%. The null hypothesis can be written as: H0: the proportion of smokers in the community is 20% (the same as in the national census).\nWe can test this by calculating a z-statistic:\n\\[\n\\begin{aligned}\nz &= \\frac{(0.18 - 0.20)}{\\sqrt{\\frac{0.20 × (1 - 0.20)}{300}}} \\\\\n&= -0.87\n\\end{aligned}\n\\]\nThis z-statistic does not meet or exceed the critical value of 1.96 for a two tailed test. This indicates that there is insufficient evidence to conclude that there is a difference between the proportion of smokers in the community and the country. This is consistent with our 95%% confidence interval which crosses the null value of 20%.\nThe P-value for the test above can be obtained from a Normal distribution table as \\(P = 2 × 0.192 = 0.38\\) (using Table A2.1 in the Appendix), or using the hand-calculator in Stata.\n\n\n\n\n\n\n\n1.3.3 Binomial test for testing one sample proportion\nWe can use the binomial distribution to obtain an exact P-value for testing a single proportion. Historically, this was a time consuming process with much hand calculation. These days, Stata and other statistical software performs the calculations quickly and efficiently, and is the preferred method.\n\n\n1.3.4 Worked example\nThe file mod06_smoking_status.csv contains the data for this example. In the data file, smokers are coded as 1 and non-smokers are coded as 0.\nIn Stata, we can use the prtest command to perform a z-test, or the bitest command to perform the exact binomial test. In R, we can use the prop.test function to perform a z-test, or the binom.test function to perform the exact binomial test.\nThe z-test provides a two-sided P-value of 0.39, while the binomial test gives a two-sided P-value of 0.43. Both tests provide little evidence against the hypothesis that the prevalence of smoking in the community is 20%."
  },
  {
    "objectID": "06-proportions.html#contingency-tables",
    "href": "06-proportions.html#contingency-tables",
    "title": "1  Summary statistics for binary data",
    "section": "1.4 Contingency tables",
    "text": "1.4 Contingency tables\nAs introduced in PHCM9794: Foundations of Epidemiology, 2-by-2 contingency tables can be used to examine associations between two binary variables, most commonly an exposure and an outcome. The traditional form of a 2-by-2 contigency table is given in ?tbl-2-by-2-trad.\nNote for Stata users: It is important to note that Stata presents the exposure or intervention (present, absent) in the columns and the outcome or disease (present, absent) in the rows (e.g. Table 1.1). This is opposite to the way most epidemiological tables are presented, with exposure in rows and outcome in columns. Care must be taken when reading 2-by-2 tables generated from Stata.\n\n\n\n\nTable 1.1:  Stata format for presenting a contingency table \n\n Exposure presentExposure absentTotal\n\nOutcome presentaca+c\n\nOutcome absentbdb+d\n\nTotala+bc+dN\n\n\n\n\n\nWhen using a statistics program such, it is recommended that the outcome and exposure variables are coded by assigning ‘absent’ as 0 and ‘present’ as 1, for example ‘No’ = 0 and ‘Yes’ = 1. This is needed for some of the commands to work (e.g. the epidemiology table commands). This coding ensures that measures of association, such as the odds ratio or relative risk, are computed correctly by Stata."
  },
  {
    "objectID": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "href": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "title": "1  Summary statistics for binary data",
    "section": "1.5 A brief summary of epidemiological study types",
    "text": "1.5 A brief summary of epidemiological study types\nIn this section, we wil present a very brief summary of three study types commonly used in population health research. This topic is covered in much more detail in PHCM9794: Foundations of Epidemiology, and more detail can be found in Chapter 4 of Essential Epidemiology (3rd or 4th edition) Webb, Bain and Page (Webb, Bain, and Page (2016)).\n\n1.5.1 Randomised controlled trial\nA randomised controlled trial addresses the research question: what is the effect of an intervention on an outcome. In the simplest form of a randomised controlled trial, a group of participants is randomly allocated to a group that receives the treatment of interest or to a control group that does not receive the treatment of interest. Participants are followed up over time, and the outcome is measured at the conclusion of the study.\n\n\n\n\n\nThe design of a randomised controlled trial [Figure 4.1, Essential Epidemiology]\n\n\n\n\n\n\n1.5.2 Cohort study\nA cohort study is an observational study that addresses the research question: what is the effect of an exposure on an outcome. This research question is similar to that studied in a randomised controlled trial, but the exposure is defined by the participants’ circumstances, and not manipulated by the researchers. In a cohort study, participants without the outcome of interest are enrolled, followed over time, and information on their exposure to a factor is measured (either at baseline or over time). At the conclusion of the study, information on the outcome is measured to identify new (incident) cases.\n\n\n\n\n\nThe design of a cohort study [Figure 4.2, Essential Epidemiology]\n\n\n\n\n\n\n1.5.3 Case control study\nWhile the randomised controlled trial and cohort study begin with a population without the outcome, a case-control study begins by assembling a group with the outcome of interest (cases), and a group without the outcome of interest (controls). The researchers then ask the cases and controls about their previous exposures.\n\n\n\n\n\nThe design of a case-control trial [Figure 4.3, Essential Epidemiology]\n\n\n\n\n\n\n1.5.4 Cross-sectional study\nIn a cross-sectional study, the exposure and the outcome are measured at the same time. While this results in a study that is relatively quick to conduct, it does not allow for any temporal relationships to be assessed.\n\n\n\n\n\nThe design of a cross-sectional study [Figure 4.4, Essential Epidemiology]"
  },
  {
    "objectID": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "href": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "title": "1  Summary statistics for binary data",
    "section": "1.6 Measures of effect for epidemiological studies",
    "text": "1.6 Measures of effect for epidemiological studies\nWe can calculate a relative measure of association between an exposure and an outcome as either a relative risk or odds ratio. The relative risk is a direct comparison of the risk in the exposed group with the risk in the non-exposed group, and can only be calculated for a cohort study (including a randomised controlled trial) or a cross-sectional study (where it is also called a prevalence ratio).\nFor cohort studies, randomised controlled trials and cross-section studies, we can calculate an absolute measure of association between an exposure and an outcome as a difference in proportions (also known as an attributable risk).\nFor case-control studies, as we sample participants based on their outcome, we can not estimate the risk of the outcome. Hence, calculating a relative risk or risk difference is inappropriate. Instead of calculating risks in a case-control study, we instead calculate odds, where the odds of an event are calculated as the number with the event divided by the number without the event.\n\n\n\n\nTable 1.2:  Contingency table for a case-control study \n\n CasesControlsTotal\n\nExposure presentaba+b\n\nExposure absentcdc+d\n\nTotala+cb+dN\n\n\n\n\n\nIn the example in Table Table 1.2, we can calculate the odds of being exposed in the cases as \\(a \\div c\\). Similarly, we can calculate the odds of being exposed in the controls as \\(b \\div d\\). We can the calculate the odds ratio as:\n\\[\n\\begin{aligned}\n\\text{Odds ratio} &= (a \\div c) \\div (b \\div d) \\\\\n&= \\frac{a \\times d}{b \\times c} \\\\\n&= \\frac{ad}{bc}\n\\end{aligned}\n\\]\nNote that some authors say we should think of the odds ratio being based on the odds of being a case in the exposed group compared to the odds of being a case in the unexposed group. Here, the exposed group comprises cells “a” and “b”, so the odds of being a case in the exposed group is (a/b). Similarly, for the unexposed group, the odds of being exposed is (c/d). So our odds ratio becomes (a/b) / (c/d). If we rearrange this, we get the same odds ratio as above: (ad)/(bc).\nThe interpretation of an odds ratio is discussed in detail in PHCM9794: Foundations of Epidemiology, and an excerpt is presented here: The meaning of the calculated odds ratio as a measure of association between exposure and outcome is the same as for the rate ratio (relative risk) where:\n\nAn odds ratio >1 indicates that exposure is positively associated with disease (i.e. the exposure may be a cause of disease);\nAn odds ratio < 1 indicates that exposure is negatively associated with disease (i.e. the exposure may be protective against disease); and\nAn odds ratio = 1 indicates no association between the exposure and the outcome.\n\nIn some situations, related to how well controls are recruited into this study, the odds ratio is a close approximation of the relative risk. Therefore, you may see in some published papers of case control studies the OR interpreted as you would interpret a RR. This should be avoided in this course.\n\n1.6.1 Worked Example\nA randomised controlled trial was conducted among a group of patients to estimate the side effects of a drug. Fifty patients were randomly allocated to receive the active drug and 50 patients were allocated to receive a placebo drug. The outcome measured was the experience of nausea. The data is given in the files mod06_nausea.dta and mod06_nausea.rds.\nA summary table can be constructed as in Table 1.3.\n\ntibble::tribble(\n      ~` `, ~Nausea, ~`No nausea`, ~Total,\n  \"Active drug\",     15L,        35L,    50L,\n      \"Placebo\",      4L,        46L,    50L,\n        \"Total\",     19L,        81L,   100L\n  ) |> \n  huxtable() |>\n  theme_article() |> \n  set_width(0.8)\n\n\n\nTable 1.3:  Nausea status by drug exposure \n\n NauseaNo nauseaTotal\n\nActive drug153550\n\nPlacebo44650\n\nTotal1981100\n\n\n\n\n\nWe can use Stata or R to calculate the relative risk (RR=3.75) and its 95% confidence interval (1.34, 10.51). This tells us that nausea is 3.75 times more likely to occur in the active drug group compared with the placebo group. Because this is a randomised controlled trial, the relative risk would be an appropriate measure of association.\nWe can confirm the estimated relative risk:\n\\[\n\\begin{aligned}\n\\text{RR} &= \\frac{a / (a+b)}{c / (c+d)} \\\\\n  &= \\frac{15 / (15+35)}{4 / (4+46)} \\\\\n  &= \\frac{0.3}{0.08} \\\\\n  &= 3.75\n\\end{aligned}\n\\]\n\n\n1.6.2 Worked Example 6.5\nA case-control study investigated the association between human papillomavirus and oropharyngeal cancer (D'Souza, et al. NEJM 2007), and the results appear in Table 1.4.\n\n\n\n\nTable 1.4:  Association between human papillomavirus and oropharyngeal cancer \n\n CasesControlsTotal\n\n(Oropharyngeal cancer)(No oropharyngeal cancer)\n\nHPV Positive571471\n\nHPV Negative43186229\n\nTotal100200300\n\n\n\n\n\nThe odds ratio is the odds of being HPV positive in cases (those with oropharyngeal cancer) compared to the odds of being HPV positive in the controls (those without oropharyngeal cancer):\n\\[\n\\begin{aligned}\n\\text{OR} &= \\frac{a / c}{b /d} \\\\\n  &= \\frac{57 / 43}{14 / 186} \\\\\n  &= 17.6\n\\end{aligned}\n\\]\nWe can use Stata or R to estimate the odds ratio and its 95% confidence interval. We should use the Cornfield option in Stata to provide a better estimate of the 95% confidence interval. It appears that the jmv package in R does not use the Cornfield approximation to estimate the 95% confidence interval, but uses the Woolf method.\nThe odds ratio is estimated as 17.6, and its 95% confidence interval is estimated 9.0 to 34.3 (Cornfield, using Stata) or 9.0 to 34.5 (Woolf, using R).\nRisk statistics are usually only reported with one or two decimal places. The interpretation of the confidence intervals for both the relative risk and the odds ratio is the same as for the confidence intervals around other summary measures in that it shows the region in which we are 95% confident that the true population estimate lies."
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions",
    "href": "06-proportions.html#confidence-intervals-for-proportions",
    "title": "1  Summary statistics for binary data",
    "section": "1.7 95% confidence intervals for proportions",
    "text": "1.7 95% confidence intervals for proportions\nTo compute the 95% confidence interval for proportions, go to Statistics > Summaries, tables, and tests > Summary and descriptive statistics >Proportion CI calculator. In the cii dialog box as shown below, key in 215 as the Sample size and 47 as the number of Successes. Choose Wald for the normal approximation to binomial distribution CI [Command: cii 215 47, wald] or Wilson for Wilson CI [Command: cii 215 47, wilson].\n\n\n\n\n\n\nStata Output: 95% confidence interval (Wald method)\n\n. cii proportions 215 47, wald\n\n                                                         -- Binomial Wald ---\n    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]\n-------------+---------------------------------------------------------------\n             |        215    .2186047    .0281868        .1633595    .2738498\n\n\n\nStata Output: 95% confidence interval (Wilson method)\n\n. cii proportions 215 47, wilson\n\n                                                         ------ Wilson ------\n    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]\n-------------+---------------------------------------------------------------\n             |        215    .2186047    .0281868        .1685637    .2785246"
  },
  {
    "objectID": "06-proportions.html#significance-test-for-single-proportion",
    "href": "06-proportions.html#significance-test-for-single-proportion",
    "title": "1  Summary statistics for binary data",
    "section": "1.8 Significance test for single proportion",
    "text": "1.8 Significance test for single proportion\nTo perform a binomial test using the data from mod06_smoking_status.dta, it is a good idea to check that the variable is dichotomous and numerically coded in 0 and 1 by using the codebook command. [Command: codebook smoking_status]\n\n. codebook smoking_status \n\n--------------------------------------------------------------------------------\nsmoking_status                                                    Smoking status\n--------------------------------------------------------------------------------\n\n                  type:  numeric (double)\n                 label:  smoking_status\n\n                 range:  [0,1]                        units:  1\n         unique values:  2                        missing .:  0/300\n\n            tabulation:  Freq.   Numeric  Label\n                           246         0  Non-smokers\n                            54         1  Smokers\n\nAfter checking the data, perform the binomial probability test by going to Statistics > Summaries, tables, and tests > Classical tests of hypotheses > Binomial probability test. Select Smoking_status as the Binomial variable. The probability we want to test against is entered in the Probability of success box. To test that the sample proportion (0.18) is different from the population proportion of 0.2, we enter 0.2 as the Probability of success.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\n. bitest smoking_status == 0.2\n\n    Variable |        N   Observed k   Expected k   Assumed p   Observed p\n-------------+------------------------------------------------------------\nsmoking_st~s |      300         54           60       0.20000      0.18000\n\n  Pr(k >= 54)            = 0.825531  (one-sided test)\n  Pr(k <= 54)            = 0.215202  (one-sided test)\n  Pr(k <= 54 or k >= 66) = 0.427280  (two-sided test)\n\nA similar process can be used to conduct a z-test for a single proportion, by choosing Statistics > Summaries, tables, and tests > Classical tests of hypotheses > Proportion test:\n\n\n\n\n\n\n. prtest smoking_status == 0.2\n\nOne-sample test of proportion                   Number of obs      =       300\n\n------------------------------------------------------------------------------\n    Variable |       Mean   Std. Err.                     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nsmoking_st~s |        .18   .0221811                      .1365259    .2234741\n------------------------------------------------------------------------------\n    p = proportion(smoking_st~s)                                  z =  -0.8660\nHo: p = 0.2\n\n     Ha: p < 0.2                 Ha: p != 0.2                   Ha: p > 0.2\n Pr(Z < z) = 0.1932         Pr(|Z| > |z|) = 0.3865          Pr(Z > z) = 0.8068\n\nIf you have only the aggregate data then the binomial test can be carried out using the immediate command. For that you will need to go through perform the binomial probability test by going to Statistics > Summaries, tables, and tests > Classical tests of hypotheses > Binomial probability test calculator. Enter the numbers in the appropriate fields as shown below.\n\n\n\n\n\n[Command: bitesti 300 54 0.2]"
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "title": "1  Summary statistics for binary data",
    "section": "1.9 Computing a relative risk and its 95% confidence interval",
    "text": "1.9 Computing a relative risk and its 95% confidence interval\nUsing data file from Worked Example 6.4, to obtain relative risk and its 95% CI, go to Statistics > Epidemiology and related > Tables for epidemiologists > Cohort study risk ratio etc…\n\n\n\n\n\nIn the cs – cohort studies dialog box, select the variable side_effect in the Case variable box, and the variable group in the Exposed variable box.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\n                 | Group                  |\n                 |   Exposed   Unexposed  |      Total\n-----------------+------------------------+------------\n           Cases |        15           4  |         19\n        Noncases |        35          46  |         81\n-----------------+------------------------+------------\n           Total |        50          50  |        100\n                 |                        |\n            Risk |        .3         .08  |        .19\n                 |                        |\n                 |      Point estimate    |    [95% Conf. Interval]\n                 |------------------------+------------------------\n Risk difference |              .22       |    .0723899    .3676101 \n      Risk ratio |             3.75       |     1.33754     10.5137 \n Attr. frac. ex. |         .7333333       |    .2523589     .904886 \n Attr. frac. pop |         .5789474       |\n                 +-------------------------------------------------\n                               chi2(1) =     7.86  Pr>chi2 = 0.0050\n\n[Command: cs side_effect group]\nIf you only have the cross-tabulated data (i.e. aggregated), you can go to Statistics > Epidemiology and related > Tables for epidemiologists > Cohort study risk ratio etc. calculator. In the csi dialog box, key in the relevant numbers from the cross-tabulated data (similarly to the cci dialog box above). Click OK or Submit to obtain identical output.\n\n\n\n\n\n[Command: csi 15 4 35 46]"
  },
  {
    "objectID": "06-proportions.html#computing-an-odds-ratio-and-its-95ci",
    "href": "06-proportions.html#computing-an-odds-ratio-and-its-95ci",
    "title": "1  Summary statistics for binary data",
    "section": "1.10 Computing an odds ratio and its 95%CI",
    "text": "1.10 Computing an odds ratio and its 95%CI\nTo obtain an odds ratio and its 95% CI, go to Statistics > Epidemiology and related > Tables for epidemiologists > Case-control odds ratio. The cc dialog box is completed as for the cs dialog box.\nTo obtain the Cornfield confidence interval, click on the Options tab and select the Cornfield approximation radio button.\n\n\n\n\n\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |        57          43  |        100       0.5700\n        Controls |        14         186  |        200       0.0700\n-----------------+------------------------+------------------------\n           Total |        71         229  |        300       0.2367\n                 |                        |\n                 |      Point estimate    |    [95% Conf. Interval]\n                 |------------------------+------------------------\n      Odds ratio |          17.6113       |    9.043258    34.25468 (Cornfield)\n Attr. frac. ex. |         .9432183       |    .8894204    .9708069 (Cornfield)\n Attr. frac. pop |         .5376344       |\n                 +-------------------------------------------------\n                               chi2(1) =    92.26  Pr>chi2 = 0.0000\n\nIf you only have the cross-tabulated data (i.e. aggregated), you can go to Statistics > Epidemiology and related > Tables for epidemiologists > Case-control odds ratio calculator. In the cci dialog box, enter the numbers from the cross-tabulated data and select the Cornfield approximation radio button. For example, Worked example 6.5 would be entered as:"
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions-1",
    "href": "06-proportions.html#confidence-intervals-for-proportions-1",
    "title": "1  Summary statistics for binary data",
    "section": "1.11 95% confidence intervals for proportions",
    "text": "1.11 95% confidence intervals for proportions\nWe can use the BinomCI(x=, n=, method=) function within the DescTools package to compute 95% confidence intervals for proportions. Here we specify x: the number of successes, n: the sample size, and optionally, the method (which defaults to Wilson’s method).\n\nlibrary(DescTools)\n\nBinomCI(x=47, n=215, method='wald')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1633595 0.2738498\n\nBinomCI(x=47, n=215, method='wilson')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1685637 0.2785246"
  },
  {
    "objectID": "06-proportions.html#significance-test-for-single-proportion-1",
    "href": "06-proportions.html#significance-test-for-single-proportion-1",
    "title": "1  Summary statistics for binary data",
    "section": "1.12 Significance test for single proportion",
    "text": "1.12 Significance test for single proportion\nWe can use the binom.test function to perform a significance test for a single proportion: binom.test(x=, n=, p=). Here we specify x: the number of successes, n: the sample size, and p: the hypothesised proportion (which defaults to 0.5 if nothing is entered).\n\nbinom.test(x=54, n=300, p=0.2)\n\n\n    Exact binomial test\n\ndata:  54 and 300\nnumber of successes = 54, number of trials = 300, p-value = 0.4273\nalternative hypothesis: true probability of success is not equal to 0.2\n95 percent confidence interval:\n 0.1382104 0.2282394\nsample estimates:\nprobability of success \n                  0.18 \n\n\nNote that the binom.test function also produces a 95% confidence interval around the estimated proportion. This confidence interval is based on the inferior Wald method: the confidence interval derived from the Wilson method is preferred.\nWe can also conduct a z-test for a single proportion:\n\nprop.test(x=54, n=300, p=0.2, correct=FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  54 out of 300, null probability 0.2\nX-squared = 0.75, df = 1, p-value = 0.3865\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.1406583 0.2274332\nsample estimates:\n   p \n0.18"
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "title": "1  Summary statistics for binary data",
    "section": "1.13 Computing a relative risk and its 95% confidence interval",
    "text": "1.13 Computing a relative risk and its 95% confidence interval\nWe will use Worked Example 6.4 to demonstrate calculating a relative risk and its 95% CI:\n\nlibrary(jmv)\n\ndrug <- readRDS(\"data/examples/mod06_nausea.rds\")\n\nsummary(drug)\n\n     group       side_effect\n Placebo:50   No nausea:81  \n Active :50   Nausea   :19  \n\n\nBy using the head() function to view the first six lines of data, we see that both group and side_effect have been entered as factors. Notice the order in which the factor levels are presented: group has the Placebo level defined as the first level, and the Active level defined as the second; side_effect has No nausea defined as the first level, and the Nausea level defined as the second.\nWe will use jmv to calculate relative risks, odds ratios and risk differences. To calculate these estimates correctly, we must define the positive exposure and positive outcome to be the first level of a factor. When defining an exposure for example, we should define the active treatment or the positive exposure as the first category. When defining an outcome, we should define the category of interest (e.g. disease, or side effect) as the first category.\nIn this example, we will define Active as the first level in the group factor, and Nausea to be the first level of the side_effect factor.\nWe can do this using the relevel() function, which re-orders the levels of a factor so that the level specified is defined as the first level, and the others are moved down:\n\n# Define \"Active\" as the first level of group:\ndrug$group <- relevel(drug$group, ref=\"Active\")\n\n\n# Define \"Nausea\" as the first level of side_effect:\ndrug$side_effect <- relevel(drug$side_effect, ref=\"Nausea\")\n\nUpon re-leveling the factors, we can check that the levels of interest have been defined as the first levels:\n\nsummary(drug)\n\n     group       side_effect\n Active :50   Nausea   :19  \n Placebo:50   No nausea:81  \n\n\nTo construct the 2-by-2 table and calculate a relative risk, we use the contTables() function in jmv. We request the row-percents using pcRow = TRUE and the relative risk and confidence interval using relRisk = TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, relRisk = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared\n\n\nIf you only have the cross-tabulated data (i.e. aggregated), you will need to enter your data into a new data frame. For example, to recreate the above analyses, we can re-write the 2-by-2 table as follows:\n\n\n\nGroup\nside_effect\nNumber\n\n\n\n\nActive\nNausea\n15\n\n\nActive\nNo nausea\n35\n\n\nPlacebo\nNausea\n4\n\n\nPlacebo\nNo nausea\n46\n\n\n\nWe can enter these data in a dataframe, comprising three vectors, as follows:\n\ndrug_aggregated <- data.frame(\n  group = c(\"Active\", \"Active\", \"Placebo\", \"Placebo\"),\n  side_effect = c(\"Nausea\", \"No nausea\", \"Nausea\", \"No nausea\"),\n  n = c(15, 35, 4, 46)\n)\n\nWe need to define group and side_effect as factors. Here we must define the levels in the order we want the categories to appear in the table. Note that as group and side_effect are entered as text variables, we can omit labels command when defining the factors, and the factor will be labelled using the text entry:\n\ndrug_aggregated$group <- factor(drug_aggregated$group, levels=c(\"Active\", \"Placebo\"))\ndrug_aggregated$side_effect <- factor(drug_aggregated$side_effect, levels=c(\"Nausea\", \"No nausea\"))\n\nWe can calculate the relative risk using the summarised data in the same was done previously. However, we need to include the number of observations in each cell using the counts command:\n\ncontTables(data=drug_aggregated,\n           rows=group, cols=side_effect, count=n,\n           pcRow=TRUE, relRisk = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared"
  },
  {
    "objectID": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "title": "1  Summary statistics for binary data",
    "section": "1.14 Computing a difference in proportions and its 95% confidence interval",
    "text": "1.14 Computing a difference in proportions and its 95% confidence interval\nWe can use the contTables function to obtain a difference in proportions and its 95% CI, by specifying diffProp=TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, diffProp=TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                                      \n ───────────────────────────────────────────────────────────────────────── \n                                  Value          Lower         Upper       \n ───────────────────────────────────────────────────────────────────────── \n   Difference in 2 proportions    0.2200000 ᵃ    0.07238986    0.3676101   \n ───────────────────────────────────────────────────────────────────────── \n   ᵃ Rows compared"
  },
  {
    "objectID": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "title": "1  Summary statistics for binary data",
    "section": "1.15 Computing an odds ratio and its 95% confidence interval",
    "text": "1.15 Computing an odds ratio and its 95% confidence interval\nWe can use the contTables function to obtain an odds ratio and its 95% CI, by specifying odds=TRUE. Here we will use the summarised HPV data from Module 6.\n\nhpv <- data.frame(\n  hpv = c(\"HPV +\", \"HPV +\", \"HPV -\", \"HPV -\"),\n  cancer = c(\"Case\", \"Control\", \"Case\", \"Control\"),\n  n = c(57, 14, 43, 186)\n)\n\nhpv$cancer <- factor(hpv$cancer, levels=c(\"Case\", \"Control\"))\nhpv$hpv <- factor(hpv$hpv, levels=c(\"HPV +\", \"HPV -\"))\n\ncontTables(data=hpv, \n           rows=hpv, cols=cancer, count=n,\n           odds = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                    \n ───────────────────────────────────── \n   hpv      Case    Control    Total   \n ───────────────────────────────────── \n   HPV +      57         14       71   \n   HPV -      43        186      229   \n   Total     100        200      300   \n ───────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value       df    p            \n ────────────────────────────────────── \n   χ²    92.25660     1    < .0000001   \n   N          300                       \n ────────────────────────────────────── \n\n\n Comparative Measures                               \n ────────────────────────────────────────────────── \n                 Value       Lower       Upper      \n ────────────────────────────────────────────────── \n   Odds ratio    17.61130    8.992580    34.49041   \n ────────────────────────────────────────────────── \n\n\nNote that 95% confidence intervals for the odds ratio based on jmv differ from those calculated by Stata. It appears that jmv uses the Woolf method to calculate confidence intervals, but this is not documented."
  },
  {
    "objectID": "07-testing-proportions.html#learning-objectives",
    "href": "07-testing-proportions.html#learning-objectives",
    "title": "2  Hypothesis testing for categorical data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nUse and interpret the appropriate test for testing associations between categorical data;\nConduct and interpret an appropriate test for independent proportions;\nConduct and interpret a test for paired proportions;"
  },
  {
    "objectID": "07-testing-proportions.html#readings",
    "href": "07-testing-proportions.html#readings",
    "title": "2  Hypothesis testing for categorical data",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 17. [UNSW Library Link]\nBland (2015); Chapter 13. [UNSW Library Link]\nAcock (2010); Section 7.6."
  },
  {
    "objectID": "07-testing-proportions.html#introduction",
    "href": "07-testing-proportions.html#introduction",
    "title": "2  Hypothesis testing for categorical data",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn Module 6, we estimated the 95% confidence intervals of proportions and measures of association for categorical data and conducted a significance test comparing a sample proportion to a known value.\nWhen both the outcome variable and the exposure variable are categorical, a chi-squared test can be used as a formal statistical test to assess whether the exposure and outcome are related. The P-value obtained from a chi-squared test gives the probability of obtaining the observed association (or more extreme) if there is in fact no association between the exposure and outcome.\nIn this Module, we also include tests for a difference in proportion for paired data.\n\n2.1.1 Worked Example\nWe are using the randomised controlled trial as given in Worked Example 6.4 on the nauseating side effect of a drug.\nThe research question is whether the active drug resulted in a different rate of nausea than the placebo drug. This is equivalent to testing whether there is an association between nausea and type of drug received (active or placebo). Thus, we will test the null hypothesis that the experience of nausea and the treatment are not related to one another. The null hypothesis is:\n\nH0: The proportion with nausea in the active drug group is the same as the proportion with nausea in the placebo drug group.\n\nThe alternative hypothesis can be stated as:\n\nHa: The proportion with nausea in the active drug group is different to the proportion with nausea in the placebo drug group."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "href": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "title": "2  Hypothesis testing for categorical data",
    "section": "2.2 Chi-squared test for independent proportions",
    "text": "2.2 Chi-squared test for independent proportions\nA chi-squared test is used to test the null hypothesis that of no association between two categorical variables. First a contingency table is drawn up and then we estimate the counts of each cell (i.e. a, b, c and d) that would be expected if the null hypothesis was true. The row and column totals are used to calculate expected counts in each cell of the contingency table as follows:\nExpected count = (Row count × Column count) / Total count\nStata will do this for us, as described in the Stata Notes section in this Module.\nA chi-squared value is then calculated to compare the expected counts (E) in each cell with the observed (actual) cell counts (O). The calculation is as follows:\n\\(\\chi ^ 2 = \\sum \\frac{(O - E)}{E} ^2\\)\nWith [Number of rows \\(-\\) 1] \\(\\times\\) [Number of columns \\(-\\) 1] degrees of freedom.\nAs for many statistics, the deviations between the observed and expected values are squared to prevent the negative and positive values balancing one another out.\nIf the expected counts are close to the observed counts, the chi-squared statistic will be close to zero, and the P-value will be close to 1. The larger the difference between the observed and expected counts, the larger the chi-squared statistic becomes (and the smaller the P-value). A large chi-squared statistic provides more evidence of an association between the exposure and outcome.\nA chi-squared test to examine whether two independent proportions are associated can be obtained in Stata using the Tables for Epidemiologists: cc and cs commands. The test can also be conducted using the tab2 command, with the advantage that the tab2 command shows the expected counts. We will show why this is important shortly.\nThe Stata output for Worked Example 7.1 for the study on nausea is shown in Output 7.1a. It is always important to request the percentages using the options in ‘Cells’ to show the proportion of patients who have the outcome in each exposure group, i.e. the row percents.\n\n\nTable 2.1: Nausea side-effect data\n\n\n\n\n(a) Observed counts\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n35 (70%)\n15 (30%)\n50 (100%)\n\n\nPlacebo\n46 (92%)\n4 (8%)\n50 (100%)\n\n\nTotal\n81 (81%)\n19 (19%)\n100 (100%)\n\n\n\n\n\n\n(b) Expected counts\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n40.5\n9.5\n50\n\n\nPlacebo\n40.5\n9.5\n50\n\n\nTotal\n81\n19\n100\n\n\n\n\n\n\nWe can see from the row percentages that 8% of patients in the placebo group experienced nausea compared to 30% of patients in the active group. If no association existed, we would expect to find approximately the same percent of patients with nausea in each group. The ‘Expected’ counts are higher for the groups with ‘No nausea’ because ‘No nausea’ is more prevalent in the sample than ‘Nausea’.\nWhile the tab2 command will perform the chi-square test, the measure of effect is best obtained using the cs or cc commands, as discussed in Module 6. Using the relative risk obtained from Module 6, a conclusion from the above test can be written as:\nThe proportion with nausea in those who received the active drug is 30%, compared to 8% in those who received the placebo drug. Nausea was more frequent in those who received the active drug (Relative Risk = 3.75, 95% CI: 1.34 to 10.51). There is strong evidence that the proportion with nausea differs between the two groups (\\(\\chi ^2\\) = 7.86 with 1 df, P=0.005).\n\n2.2.1 Assumptions for using a Pearson’s chi-squared test\nThe assumptions that must be met when using Pearson’s chi-squared test are that:\n\neach observation must be independent;\neach participant is represented in the table once only;\nat least 80% of the expected cell counts should exceed a value of five;\nall expected cell counts should exceed a value of one.\n\nThe first two assumptions are dictated by the study design. The last two assumptions relate to the numbers in the cells and can be explored when running the test. There should not be too many cells with low expected counts. Sometimes, the only way to avoid small cell counts is to recruit a much larger sample size. If small cell counts cannot be avoided, Fisher’s exact test can be used instead. More information on Fisher’s exact test can be found in Chapter 13 of An Introduction to Medical Statistics, Bland (2015).\n\n\n2.2.2 Interpreting chi-squared tests\nFor the data being considered from Worked Example 7.1 all cells have an expected count greater than 5 and that the minimum cell count is 9.5. Therefore, it is appropriate to use the Pearson’s Chi-Squared test. If one or more cells have an expected cell count less than 5, then the Fisher’s exact test should be used. The P value associated with the Pearson’s chi-squared test is 0.005 indicating that we can reject the null hypothesis at a 5% level of significance. Thus, we can conclude that there is strong evidence that more patients who were randomised to receive the active treatment experienced nausea than patients randomised to the control (placebo) group."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "title": "2  Hypothesis testing for categorical data",
    "section": "2.3 Chi-squared tests for tables larger than 2-by-2",
    "text": "2.3 Chi-squared tests for tables larger than 2-by-2\nChi-squared tests can also be used for tables larger than a 2-by-2 dimension. When a contingency table larger than 2-by-2 is used, say a 4-by-2 table if there were 4 exposure groups, the Pearson’s chi-squared can still be used.\n\n2.3.1 Worked Example\nThe files mod07_allergy.dta and mod07_allergy.rds contain information about the severity of allergic reaction, coded as absent, slight, moderate or severe. We can test the hypothesis that the severity of allergy is not different between males and females. To do this we can use a two-way tabulation to obtain Table 2.2 which shows the counts, expected counts and the percent of females and males who fall into each severity group for allergy. The table shows that the percentage of males is higher in each of the categories of severity (slight, moderate, severe) than the percentage of females.\n\n\nTable 2.2: Allergy data\n\n\n\n\n(a) Observed counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n150 (62.0%)\n50 (20.7%)\n27 (11.2%)\n15 (6.2%)\n242 (100%)\n\n\nMale\n137 (53.1%)\n70 (27.1%)\n32 (12.4%)\n19 (7.4%)\n258 (100%)\n\n\nTotal\n287 (57.4%)\n120 (24.0%)\n59 (11.8%)\n34 (6.8%)\n500 (100.0%)\n\n\n\n\n\n\n\n\n(b) Expected counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n138.9\n58.1\n28.6\n16.5\n242.0\n\n\nMale\n148.1\n61.9\n30.4\n17.5\n258.0\n\n\nTotal\n287.0\n120.0\n59.0\n34.0\n500.0\n\n\n\n\n\n\nThe Pearson chi-squared statistic is calculated as 4.31, with 3 degrees of freedom, providing a P-value of 0.23. Therefore, there is little evidence of an association between gender and the severity of allergy."
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "href": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "title": "2  Hypothesis testing for categorical data",
    "section": "2.4 McNemar’s test for categorical paired data",
    "text": "2.4 McNemar’s test for categorical paired data\nIf a binary categorical outcome is measured in a paired study design, McNemar’s statistic is used. This statistic is a form of chi-square applied to a paired situation. A Pearson’s chi-squared test cannot be used because the measurements are not independent. However, McNemar’s test can be used to assess whether there is a significant change in proportions between two time points or between two conditions, or whether there is a significant difference in proportions between matched cases and controls.\nFor McNemar’s test, the data are displayed as shown in Table 7.1. Cells ‘a’ and ‘d’ called concordant cells because the response was the same at both baseline and follow-up or between matched cases and controls. Cells ‘b’ and ‘c’ are called discordant cells because the responses between the pairs were different. For a follow-up study, the participants in cell ‘c’ had a positive response at baseline and a negative response at follow-up. Conversely, the participants in cell ‘b’ had a negative response at baseline and a positive response at follow-up.\nFor other types of paired data such as twins or matched cases and controls, the data are similarly displayed with the responses of one of the pairs in the columns and the responses for the other of the pairs in the rows. For paired data, the grand total ‘N’ is always the number of pairs and not the total number of participants.\n\n\n\n\nTable 2.3:  Table layout for testing matched proportions \n\n Negative at follow-upPositive at follow-upTotal\n\nNegative at baselineaba + b\n\nPositive at baselinecdc + d\n\nTotala + cb + dN\n\n\n\n\n\n\n2.4.1 Worked Example 7.3\nTwo drugs labelled A and B have been administered to patients in random order so that each patient acts as their own control. The datasets mod07_drug_response.dta and mod07_drug_response.rds are available on Moodle. The null hypothesis is as follows:\n\nH0: The proportion of patients who do better on drug A is the same as the proportion of patients who do better on drug B\n\nCounts and overall percentages are presented in . From the “Total” row in the table, we can see that the number of patients who respond to drug A is 41 (68%) and from the “Total” column the number who respond to drug B is less at 35 (58%), that is there is a difference of 10%.\n\nPaired data\n\n\n\n\n\n\n\n\n\nResponse to Drug B\nNo response to Drug B\nTotal\n\n\n\n\nResponse to Drug A\n21 (35%)\n20 (33%)\n41 (68%)\n\n\nNo response to Drug A\n14 (23%)\n5 (8%)\n19 (32%)\n\n\nTotal\n35 (58%)\n25 (42%)\n60 (100%)\n\n\n\nThe difference in the paired proportions is calculated using the simple equation:\n\\[ p_{A} - p_{B} = \\frac{(b - c)}{N} \\]\nHere, \\(p_{A} - p_{B} = \\frac{(20 - 14)}{60} = 0.1\\)\nThe cell counts show that 20 patients responded to Drug A but not to drug B, and 14 patients responded to Drug B but not to drug A. McNemar’s statistic is computed from these two discordant pairs (labelled as ‘b’ and ‘c’) as follows:\n\\[ X^2 = \\frac{(b-c)^2}{b+c} \\]\nwith 1 degree of freedom. Using our worked example, the McNemar’s chi-squared statistic is calculated as 1.06 with 1 degree of freedom, giving a P-value of 0.3.\nAs described above, the difference in proportions can be calculated. A 95% confidence interval for this difference can be obtained using statistical software.\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-square=1.06 with 1 degree of freedom, P=0.3).\nADD TEXT AROUND THE EXACT TEST BEING BASED ON BINOMIAL TEST KIRKWOOD RECOMMENDS B+C TO BE BIGGER THAN 10, OTHERWISE USE EXACT BINOMIAL TEST"
  },
  {
    "objectID": "07-testing-proportions.html#summary",
    "href": "07-testing-proportions.html#summary",
    "title": "2  Hypothesis testing for categorical data",
    "section": "2.5 Summary",
    "text": "2.5 Summary\nIn Module 6, we estimated proportions and measures of association for categorical data and conducted a one-sample test of proportions. In this module, we conduct significance tests for two or more independent proportions using the chi-squared test. The chi-squared test can also be used to conduct a significance test when there are more than two categories in both variables. The McNemar’s test is used when we have paired data.\n\n\n\n\nAcock, Alan C. 2010. A Gentle Introduction to Stata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "08-correlation-and-regression.html",
    "href": "08-correlation-and-regression.html",
    "title": "3  Correlation and linear regression",
    "section": "",
    "text": "Stata notes\nWe will demonstrate using R for correlation and simple linear regression using the dataset mod08_lung_function.rds."
  },
  {
    "objectID": "08-correlation-and-regression.html#learning-objectives",
    "href": "08-correlation-and-regression.html#learning-objectives",
    "title": "3  Correlation and linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplore the association between two continuous variables using a scatter plot;\nEstimate and interpret correlation coefficients;\nEstimate and interpret parameters from a simple linear regression;\nDecide whether a regression model is valid;\nTest a hypothesis using regression coefficients;\nOutline the concept of multiple regression and its role in investigative epidemiology."
  },
  {
    "objectID": "08-correlation-and-regression.html#readings",
    "href": "08-correlation-and-regression.html#readings",
    "title": "3  Correlation and linear regression",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 10. [UNSW Library Link]\nBland (2015); Chapter 11. [UNSW Library Link]\nAcock (2010); Chapter 8."
  },
  {
    "objectID": "08-correlation-and-regression.html#introduction",
    "href": "08-correlation-and-regression.html#introduction",
    "title": "3  Correlation and linear regression",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn Module 5, we saw how to test whether the means from two groups are equal - in other words, whether a continuous variable is related to a categorical variable. We often want to know how closely two continuous variables are related. For example, we may want to know how closely blood cholesterol levels are related to dietary fat intake in adult men. To measure the strength of association between two continuously distributed variables, a correlation coefficient is used.\nWe may also want to know how well one continuous measurement predicts the value of another continuous measurement. For example, we may want to know how well height predicts values of lung capacity in a community of adults. A regression model allows us to use one measurement to predict another measurement.\nAlthough both correlation coefficients and regression models can be used to describe the degree of association between two continuous variables, the two methods provide very different statistical information. It is important to note that both methods only measures the strengths of an association between variables and does not imply a causal relationship."
  },
  {
    "objectID": "08-correlation-and-regression.html#correlation",
    "href": "08-correlation-and-regression.html#correlation",
    "title": "3  Correlation and linear regression",
    "section": "3.2 Correlation",
    "text": "3.2 Correlation\nWe use correlation to measure the strength of a linear relationship between two variables. Before calculating a correlation coefficient, a scatter plot should first be obtained to give an understanding of the nature of the relationship between the two variables.\n\n3.2.1 Worked Example\nThe file mod08_lung_function.csv has information about height and lung function collected from a sample of 120 adults. A random sample of adults was approached to take part in the research study, but the response rate was low at 45%. Information was collected on height (cm) and lung function, which was measured as forced vital capacity (FVC). We can obtain a scatter-plot shown in Figure 3.1. This shows that as height increases, lung function also increases, which is as expected. One or two of the data points are separated from the rest of the data but are not so far away as to be considered outliers because they do not seem to stand out of other observations.\n\n\n\n\n\nFigure 3.1: Plot\n\n\n\n\n\n\n3.2.2 Correlation coefficients\nA correlation coefficient (r) describes how closely the variables are related, that is the strength of linear association between two continuous variables. The range of the coefficient is from +1 to −1 where +1 is a perfect positive association, 0 is no association and −1 is a perfect inverse association. In general, an absolute (disregarding the sign) r value below 0.3 indicates a weak association, 0.3 to < 0.6 is fair association, 0.6 to < 0.8 is a moderate association, and \\(\\ge\\) 0.8 indicates a strong association.\nThe coefficient is positive when large values of one variable tend to occur with large values of the other, and small values of one variable (y) tend to occur with small values of the other (x) (Figure 3.2 (a and b)). For example, height and weight in healthy children or age and blood pressure.\nThe coefficient is negative when large values of one variable tend to occur with small values of the other, and small values of one variable tend to occur with large values of the other (Figure 3.2 (c and d)). For example, percentage immunised against infectious diseases and under-five mortality rate.\n\n\n\n\n\nFigure 3.2: Scatter plots demonstrating strong and weak, positive and negative associations\n\n\n\n\nThe P value associated with an r value is an estimate of whether the correlation coefficient is significantly different from zero. However, a correlation coefficient that does not have a significant P value does not imply that there is no relationship because the correlation coefficient only tests for a linear association and there may be a non-linear relationship such as a curved or irregular relationship.\nThe assumptions for using a Pearson’s correlation coefficient are that:\n\nobservations are independent;\nboth variables are continuous variables;\nthe relationship between the two variables is linear.\n\nThere is a further assumption that the data follow a bivariate normal distribution. This assumes: y follows a normal distribution for given values of x; and x follows a normal distribution for given values of y. This is quite a technical assumption that we do not discuss further.\nThere are two types of correlation coefficients– the correct one to use is determined by the nature of the variables as shown in Table 3.1.\n\n\n\n\nTable 3.1:  Correlation coefficients and their application \n\nCorrelation coefficientApplication\n\nPearson’s correlation coefficient: rBoth variables are continuous and a bivariate normal distribution can be assumed\n\nSpearman’s rank correlation: rhoBivariate normality cannot be assumed. Also useful when at least one of the variables is ordinal\n\n\n\n\n\nSpearman’s \\(\\rho\\) is calculated using the ranks of the data, rather than the actual values of the data. We will see further examples of such methods in Module 9, when we consider non-parametric tests, which are often based on ranks.\nCorrelation coefficients are often presented in the form of a correlation matrix which can display the correlation between a number of variables in a single table (Table 3.2).\n\n\nTable 3.2: Correlation matrix for Height and FVC\n\n\n\n\n\n\n\n\nHeight\nFVC\n\n\nHeight\n1\n0.70\nP < 0.0001\n\n\nFVC\n0.70\nP < 0.0001\n1\n\n\n\n\nThis correlation matrix shows that the Pearson’s correlation coefficient between height and lung function is 0.70 with P<0.0001 indicating very strong evidence of a linear association between height and FVC. A correlation matrix sometimes includes correlations between the same variable, indicated as a correlation coefficient of 1. For example, \\(Height\\) is perfectly correlated with itself (i.e. has a correlation coefficient of 1). Similarly, \\(FVC\\) is perfectly correlated with itself.\nThis r value was calculated for the full data set of 120 adults who had heights ranging from 160 to 172cms. If the r value is calculated for the 60 adults with a height less than 165cms, it is much lower at 0.433 although significant at P=0.001. In general, r values are higher for a wider range of values on the x axis even though the relationship between the two variables remains the same.\nCorrelation coefficients are rarely used as important statistics in their own right because they do not fully explain the relationship between the two variables and the range of the data has an important influence on the size of the coefficient. In addition, the statistical significance of the correlation coefficient is often over interpreted because a small correlation which is of no clinical importance can become statistically significant even with a relatively small sample size. For example, a poor correlation of 0.3 will be statistically significant if the sample size is large enough."
  },
  {
    "objectID": "08-correlation-and-regression.html#linear-regression",
    "href": "08-correlation-and-regression.html#linear-regression",
    "title": "3  Correlation and linear regression",
    "section": "3.3 Linear regression",
    "text": "3.3 Linear regression\nThe nature of a relationship between two variables is more fully described using regression. There are two principal purposes for building a regression model. The most common is to build a predictive model, for example in situations in which age and gender are used to predict normal values of characteristics such as lung size or body mass index. Normal values are the range of values that occur naturally in the general population.\nThe second purpose for using a regression model is for testing the hypothesis that there is a linear relationship between one or more explanatory variables and an outcome variable. For example, a regression model can be used to test the extent to which age predicts BMI or to test the hypothesis that two groups with a different dietary regime have significantly different BMI values after adjusting for age differences.\nFrom Worked Example 8.1, we can be also plot a regression line through the scatter. Figure @ref(fig:scatter-plot-line) shows the data overlayed with the fitted regression line.\n\n\n\n\n\nAssociation between height and lung function in 120 adults\n\n\n\n\nThe line through the plot is called the line of ‘best fit’ because the size of the deviations between the data points and the line is minimised in the calculation. The distance between each data point and the regression line is called a ‘residual’.\n\n3.3.1 Regression equations\nThe mathematical equation for the line explains the relationship between the two variables. The equation of the regression line is as follows:\n\\[y = \\beta_{0} + \\beta_{1}x\\]\nThis line is shown in Figure 3.3 using the notation shown in Table 3.3.\n\n\n\n\n\nFigure 3.3: Coefficients of a linear regression equation\n\n\n\n\n\n\n\n\nTable 3.3:  Notation for linear regression equation \n\nSymbolInterpretation\n\n\\(y \\)Observed value of the outcome variable\n\n\\(x \\)Observed value of the explanatory variable\n\n\\(\\beta_0\\)Intercept of the regression line\n\n\\(\\beta_1\\)Slope of the regression line\n\n\n\n\n\nThe intercept is the point at which the regression line intersects with the y-axis when the value of ‘x’ is zero. In most cases, the intercept does not have a biologically meaningful interpretation as the explanatory variable cannot take a value of zero. In our working example, the intercept is not meaningful as it is not possible for an adult to have a height of 0cm.\nThe slope of the line is the predicted change in the outcome variable ‘y’ as the explanatory explanatory variable ‘x’ increases by 1 unit.\nAn important concept is that regression predicts an expected value of ‘y’ given an observed value of ‘x’: any error around the explanatory variable is not taken into account. For this reason, measurements that can be taken accurately, such as age and height, make good explanatory variables."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "href": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "title": "3  Correlation and linear regression",
    "section": "3.4 Regression coefficients: estimation",
    "text": "3.4 Regression coefficients: estimation\nSoftware is always used to estimate the regression equation for a set of data, using the method of least squares. This method estimates the intercept and the slope, and also their variability (i.e. standard errors)."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-inference",
    "href": "08-correlation-and-regression.html#regression-coefficients-inference",
    "title": "3  Correlation and linear regression",
    "section": "3.5 Regression coefficients: inference",
    "text": "3.5 Regression coefficients: inference\nWe can use the estimated regression coefficients and their variability to calculate 95% confidence intervals. Here, a t-value from a t-distribution with \\(n - 2\\) degrees of freedom is used:\n\n95% confidence interval for intercept: \\(b_0 \\pm t_{n-2} \\times SE(b_0)\\)\n95% confidence interval for slope: \\(b_1 \\pm t_{n-2} \\times SE(b_1)\\)\n\nNote that as the constant (\\(b_0\\)) is not often biologically plausible, the 95% confidence interval for the constant is often not reported.\nThe significance of the estimated slope (and less commonly, intercept) can be tested using a t-test. The null hypotheses and the alternative hypothesis for testing the slope of a simple linear regression model are:\n\nH0: \\(\\beta_1 = 0\\)\nH1: \\(\\beta_1 \\ne 0\\)\n\nTo test the null hypothesis for the regression coefficient beta_1, the following t-test is used:\n\\[t = b_1 /SE(b_1)\\]\nThis will give a t statistic which can be referred to a t distribution with n − 2 degrees of freedom to calculate the corresponding P-value.\nTable 3.4 shows the estimated regression coefficients for our working example.\n\n\n\n\nTable 3.4:  Estimated regression coefficients \n\nTermEstimateStandard errort valueP value95% Confidence interval\n\nIntercept-18.9 2.19 t=-8.60, 118df<0.001-23.22 to -14.53\n\nHeight0.140.013t=10.58, 118df<0.0010.11 to 0.17\n\n\n\n\n\nFrom this output, we see that the slope is estimated as 0.14 with an estimated intercept of -18.87. Therefore, the regression equation is estimated as:\nFVC (L) = − 18.87 + (0.14 \\(\\times\\) Height in cm)\nThere is very strong evidence of a linear association between FVC and height in cm (P < 0.001).\nThis equation can be used to predict FVC for a person of a given height. For example, the predicted FVC for a person 165 cm tall is estimated as:\nFVC = − 18.87347 + (0.1407567 \\(\\times\\) 165.0) = 4.40 L.\nNote that for the purpose of prediction we have kept all the decimal places in the coefficients to avoid rounding error in the intermediate calculation.\n\n3.5.1 Fit of a linear regression model\nAfter fitting a linear regression model, it is important to know how well the model fits the observed data. One way of assessing the model fit is to compute a statistic called coefficient of determination, denoted by \\(R^2\\). It is the square of the Pearson correlation coefficient \\(r: r^2 = R^2\\). Since the range of \\(r\\) is from −1 to 1, \\(R^2\\) must lie between 0 and 1.\n\\(R^2\\) can be interpreted as the proportion of variability in y that can be explained by variability in x. Hence, the following conditions may arise:\nIf \\(R^2 = 1\\), then all variation in y can be explained by variation of x and all data points fall on the regression line.\nIf \\(R^2 = 0\\), then none of the variation in y is related to x at all, and the variable x explains none of the variability in y.\nIf \\(0 < R^2 <1\\), then the variability of y can be partially explained by the variability in x. The larger the \\(R^2\\) value, the better is the fit of the regression model.\n\n\n3.5.2 Assumptions for linear regression\nRegression is robust to moderate degrees of non-normality in the variables, provided that the sample size is large enough and that there are no influential outliers. Also, the regression equation describes the relationship between the variables and this is not influenced as much by the spread of the data as the correlation coefficient is.\nThe assumptions that must be met when using linear regression are as follows:\n\nobservations are independent;\nthe relationship between the explanatory and the outcome variable is linear;\nthe residuals are normally distributed.\n\nA residual is defined as the difference between the observed and predicted outcome from the regression model. If the predicted value of the outcome variable is denoted by \\(\\hat y\\) then:\n\\[ \\text{Residual} = \\text{observed} - \\text{predicted} = y - \\hat y\\]\nIt is important for regression modelling that the data are collected in a period when the relationship remains constant. For example, in building a model to predict normal values for lung function the data must be collected when the participants have been resting and not exercising and people taking bronchodilator medications that influence lung capacity should be excluded. In regression, it is not so important that the variables themselves are normally distributed, but it is important that the residuals are. Scatter plots and specific diagnostic tests can be used to check the regression assumptions. Some of these will not be covered in this introductory course but will be discussed in detail in the Regression Methods in Biostatistics course.\nThe distribution of the residuals should always be checked. Large residuals can indicate unusual points or points that may exert undue influence on the estimated regression slope.\nThe histogram of residuals from the model is shown in Figure 3.4. The residuals are approximately normally distributed, with no outlying values.\n\n\n\n\n\nFigure 3.4: Histogram of regression residuals\n\n\n\n\n\n\n3.5.3 Critical appraisal\nWhen reading the literature, it is important to be critical about how correlation coefficients are interpreted. It is a good idea to check if a scatter plot is shown to help interpret the relationship and to indicate if there are any potential outliers. Also, question whether the correlation coefficient has been calculated from a random sample and if not, what selected samples the value can be generalised to.\nWhen regression is reported it is essential that the axes are correctly presented so that the equation is predictive. Thus, the explanatory variable must be presented on the x axis and the outcome on the y axis. It is also a good idea to check that all the assumptions are met. Outliers which result in a non-normal distribution of the residuals can severely bias the regression coefficients."
  },
  {
    "objectID": "08-correlation-and-regression.html#multiple-linear-regression",
    "href": "08-correlation-and-regression.html#multiple-linear-regression",
    "title": "3  Correlation and linear regression",
    "section": "3.6 Multiple linear regression",
    "text": "3.6 Multiple linear regression\nIn the above example, we have only used a simple linear regression model of two continuous variables. Other more complex models can be built from this e.g. if we wanted to look at the effect of gender (male vs. female) as binary indicator in the model while adjusting for the effect of height. In that case we would include both the variables in the model as explanatory variables. In the same way we can include any number of explanatory variables (both continuous and categorical) in the model: this is called a multivariable model. Multivariable models are often used for building predictive equations, for example by using age, height, gender and smoking history to predict lung function, or to adjust for confounding and detect effect modification to investigate the association between an exposure and an outcome factor.\nMultiple regression has an important role in investigating causality in epidemiology. The exposure variable under investigation must stay in the model and the effects of other variables which can be confounders or effect-modifiers are tested. The biological, psychological or social meaning of the variables in the model and their interactions are of great importance for interpreting theories of causality.\nOther multivariable models include binary logistic regression for use with a binary outcome variable, or Cox regression for survival analyses. These models, together with multiple regression, will be taught in PHCM9517: Regression Methods in Biostatistics."
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "title": "3  Correlation and linear regression",
    "section": "3.7 Creating a scatter plot",
    "text": "3.7 Creating a scatter plot\nWe will demonstrate using Stata for correlation and simple linear regression using the dataset mod08_lung_function.dta.\nTo create a scatter plot to explore the association between height and FVC click: Graphics > Twoway graph (scatter, line, etc.). In the twoway dialog box, click Create…\n\n\n\n\n\nA new dialog box will open. Select the Basic plots radio button and highlight Scatter under Basic plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button in the Plot 1 dialog box to return to the twoway dialog box, then click the OK or Submit button to produce the scatter plot shown in Figure 8.1.\n[Command: twoway (scatter FVC Height)]\nTo add a fitted line, go back to the twoway dialog box. If you clicked the OK button, you can go to Graphics > Twoway graph (scatter, line, etc.) to bring it back again.\n\n\n\n\n\nClick Create…, then select the Fit plots radio button and Linear prediction under Fit plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button, then the OK or Submit button to produce the scatterplot below.\n[Command: twoway (scatter FVC Height) (lfit FVC Height)]\n\n\n\n\n\nNotice that a legend now appears, and the y-axis title is missing. To add a y-axis title, go to the Y axis tab in the twoway dialog box to enter your title as shown below.\n\n\n\n\n\nYou can click the Submit button to check how the scatter plot looks like. Next go the Legend tab and select the Hide legend radio button.\n\n\n\n\n\nClick the OK or Submit button when you are finished to produce Figure 8.3.\n[Command: twoway (scatter FVC Height) (lfit FVC Height), ytitle(Forced vital capacity (L)) legend(off)]\nTo save your graph, go to File > Save in the Graph window, and be sure to save your file as a PNG file:"
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "title": "3  Correlation and linear regression",
    "section": "3.8 Calculating a correlation coefficient",
    "text": "3.8 Calculating a correlation coefficient\nTo calculate the Pearson’s correlation using the dataset mod08_lung_function.dta go to: Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Pairwise correlations\nSelect the two variables, FVC and Height in the Variables box. You can click the Submit button to check the output. Next, tick the box for Print significance level for each entry to obtain the P-value and the box for Print number of observations for each entry to obtain the number of observations used as shown below.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Output 8.1,\n[Command: pwcorr Height FVC, obs sig]"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "title": "3  Correlation and linear regression",
    "section": "3.9 Fitting a simple linear regression model",
    "text": "3.9 Fitting a simple linear regression model\nWe will fit a simple linear regression model with mod08_lung_function.dta to quantify the relationship between FVC and height.\nChoose Statistics > Linear models and related > Linear regression\nIn the regress dialog box, select FVC as the Dependent variable, and Height as the Independent variable.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Outputs 8.2 and 8.3.\n[Command: reg FVC Height]"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "3  Correlation and linear regression",
    "section": "3.10 Plotting residuals from a simple linear regression",
    "text": "3.10 Plotting residuals from a simple linear regression\nTo obtain the residuals, go to Statistics > Post estimation after running the regress command.\nIn the Postestimation Selector dialog box, select Predictions and their SEs, leverage statistics, distance statistics, etc. in the list under Predictions as shown below.\n\n\n\n\n\nIn the predict dialog box, choose the Residuals button and enter a New variable name (e.g. FVC_resid) for the residuals from the regression model.\n\n\n\n\n\nClick OK button when you are done.\n[Command: predict FVC_resid, residuals]\nYou can now check the assumption that the residuals are normally distributed by creating a histogram with the normal curve using Graphics > Histogram as shown in Stata Notes section for Module 2. Below is the histogram dialog box used to produce the graph in Figure 8.5.\n\n\n\n\n\n[Command: histogram FVC_resid, bin(12) frequency normal]"
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "title": "3  Correlation and linear regression",
    "section": "3.11 Creating a scatter plot",
    "text": "3.11 Creating a scatter plot\nWe can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands:\n\nplot(x=lung$Height, y=lung$FVC, \n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\")\n\n\n\n\nTo add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with the lm() function, which fits a linear model.\nThe basic syntax of the lm() function is: lm(y ~ x) where y represents the outcome variable, and x represents the explanatory variable. Putting this all together:\n\nplot(x=lung$Height, y=lung$FVC,\n     xlab=\"Height (cm)\",\n     ylab=\"Forced vital capacity (L)\")\n\nabline(lm(lung$FVC ~ lung$Height))\n\n\n\n\nOr using the ggformula package, we form the basic plot using the following:\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") |>\n  gf_theme(theme = theme_minimal())\n\n\n\n\nWe can add an estimated linear regression line by piping the command gf_lm():\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") |>\n  gf_lm() |>\n  gf_theme(theme = theme_minimal())\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "title": "3  Correlation and linear regression",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nWe can use the corrMatrix function in the Jamovi package to calculate a Pearson’s correlation coefficient:\n\ncorrMatrix(data=lung, vars=c(Height, FVC))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                   \n ──────────────────────────────────────────────────── \n                            Height        FVC         \n ──────────────────────────────────────────────────── \n   Height    Pearson's r             —                \n             p-value                 —                \n                                                      \n   FVC       Pearson's r     0.6976280            —   \n             p-value        < .0000001            —   \n ────────────────────────────────────────────────────"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "title": "3  Correlation and linear regression",
    "section": "3.12 Fitting a simple linear regression model",
    "text": "3.12 Fitting a simple linear regression model\nWe can use the lm function to fit a simple linear regression model, specifying the model as y ~ x where y represents the outcome variable, and x represents the explanatory variable. Using mod08_lung_function.rds, we can quantify the relationship between FVC and height:\n\nlm(FVC ~ Height, data=lung)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nCoefficients:\n(Intercept)       Height  \n   -18.8735       0.1408  \n\n\nThe default output from the lm function is rather sparse. We can obtain much more useful information by defining the linear regression model as an object, then using the summary() function:\n\nmodel <- lm(FVC ~ Height, data=lung)\nsummary(model)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01139 -0.23643 -0.02082  0.24918  1.31786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -18.87347    2.19365  -8.604 3.89e-14 ***\nHeight        0.14076    0.01331  10.577  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3965 on 118 degrees of freedom\nMultiple R-squared:  0.4867,    Adjusted R-squared:  0.4823 \nF-statistic: 111.9 on 1 and 118 DF,  p-value: < 2.2e-16\n\n\nFinally, we can obtain 95% confidence intervals for the regression coefficients using the confint function:\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -23.2174967 -14.5294444\nHeight        0.1144042   0.1671092"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "title": "3  Correlation and linear regression",
    "section": "3.13 Plotting residuals from a simple linear regression",
    "text": "3.13 Plotting residuals from a simple linear regression\nWe can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a histogram in the usual way:\n\nresiduals <- resid(model)\nhist(residuals)\n\n\n\n\nA Normal curve can be overlaid if we plot the residuals using a probability scale.\n\nhist(residuals, probability = TRUE,\n     ylim = c(0, 1))\n\ncurve(dnorm(x, mean=mean(residuals), sd=sd(residuals)), \n      col=\"darkblue\", lwd=2, add=TRUE)\n\n\n\n\nUsing ggformula, we can plot the residuals as a histogram:\n\ngf_dhistogram(~ residuals, data=model) |>\n  gf_dist(\"norm\", \n          params=list(mean=mean(model$residuals), \n                      sd=sd(model$residuals)))\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\ngf_dhistogram(~ residuals, data=model) |>\n  gf_dens()"
  },
  {
    "objectID": "09-non-parametrics.html",
    "href": "09-non-parametrics.html",
    "title": "4  Analysing non-normal data",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "09-non-parametrics.html#learning-objectives",
    "href": "09-non-parametrics.html#learning-objectives",
    "title": "4  Analysing non-normal data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nTransform non-normally distributed variables;\nExplain the purpose of non-parametric statistics and key principles for their use;\nCalculate ranks for variables;\nConduct and interpret a non-parametric independent samples significance test;\nConduct and interpret a non-parametric paired samples significance test;\nCalculate and interpret the Spearman rank correlation coefficient."
  },
  {
    "objectID": "09-non-parametrics.html#readings",
    "href": "09-non-parametrics.html#readings",
    "title": "4  Analysing non-normal data",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 13. [UNSW Library Link]\nBland (2015); Chapter 12. [UNSW Library Link]\nAcock (2010); Section 7.11."
  },
  {
    "objectID": "09-non-parametrics.html#introduction",
    "href": "09-non-parametrics.html#introduction",
    "title": "4  Analysing non-normal data",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn general, parametric statistics are preferred for reporting data because the summary statistics (mean, standard deviation, standard error of the mean etc) and the tests used (t-tests, correlation, regression etc) are familiar and the results are easy to communicate. However, non-parametric tests can be used if data are not normally distributed. Non-parametric tests make fewer assumptions about the distribution of the data."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables",
    "title": "4  Analysing non-normal data",
    "section": "4.2 Transforming non-normally distributed variables",
    "text": "4.2 Transforming non-normally distributed variables\nWhen a variable has a skewed distribution, one possibility is to transform the data to a new variable to try and obtain a normal or near normal distribution. Methods to transform non-normally distributed data include logarithmic transformation of each data point, or using the square root or the square or the inverse (i.e. 1/x) etc.\n\n4.2.1 Worked Example\nWe have data from 132 patients who had a hospital stay following admission to ICU available on Moodle (mod09_infection.dta and mod09_infection.rds). The distribution of the length of stay for these patients is shown in the histogram in Figure 4.1. As is common with variables that record time, the data are skewed with many patients having relatively short stays and a few patients having very long hospital stays. Clearly, it would be inappropriate to use parametric statistical methods for these data.\n\n\n\n\n\nFigure 4.1: Length of hospital stay for 132 patients\n\n\n\n\nWhen data are positively skewed, as shown in Figure 4.1, a logarithmic transformation can often make the data closer to being normally distributed. This is the most common transformation used. You should note, however, that the logarithmic function cannot handle 0 or negative values. One way to deal with zeros in a set of data is to add 1 to each value before taking the logarithm.\nWe would generate a new variable, as shown in the Stata or R notes. As the minimum length of stay in these sample data was 0, we have added 1 to each length of stay before taking the logarithm. The distribution of the logarithm of (length of stay + 1) is shown in Figure 4.2.\n\n\n\n\n\nFigure 4.2: istribution of log transformed (length of stay + 1)\n\n\n\n\nThe distribution now appears much more bell shaped. Table 4.1 shows the descriptive statistics for length of stay before and after logarithmic transformation. Before transformation, the SD is almost as large as the mean value which indicates that the data are skewed and that these statistics are not an accurate description of the centre and spread of the data.\n\n\nTable 4.1: Summary statistics for untransformed and transformed length of stay\n\n\n\n\n\n\n\n\nLength of stay\nlog(Length of stay + 1)\n\n\n\n\nMean (Standard deviation)\n38.1 (35.78)\n3.41 (0.715)\n\n\nMean: 95% confidence interval\n31.9 to 44.2\n3.29 to 3.53\n\n\nMedian [Interquartile range]\n27 [21 to 42]\n3.3 [3.1 to 3.8]\n\n\nRange\n0 to 244\n0 to 5.5\n\n\n\n\nThe mean and standard deviation of the transformed length of stay are in log base e (i.e. ln) units. If we raise the mean of the log of length of stay to the power of \\(e\\), it returns a value of 30.2 days (\\(e^{3.41}=30.2\\)).\nTechnically, this is called the geometric mean of the data, and it has a different interpretation to the usual mean, the arithmetic mean. This is a much better estimate in this case of the “average” length of stay than the mean of 38.1 days (95% CI 31.9, 44.2 days) obtained from the non-transformed positively skewed data. Note that, if you have added 1 to your data to deal with 0 values, the back-transformed estimate is approximately equal to the geometric mean.\nThis set of data also includes a variable summarising whether a patient acquired a nosocomial infection (also known as healthcare-associated infections), which are infections that develop while undergoing medical treatment but were absent at the time of admission.\nIf we were testing the hypothesis that there was a difference in length of stay between groups (status of nosocomial infection), t-tests should not be used with length of stay, but could be used for the log transformed variable, which is approximately normally distributed. The output from the t-test of the log-transformed length of stay is shown in Table 4.2. This is done using the t-test shown in Module 5.\n\n\nTable 4.2: Summary statistics for transformed length of stay\n\n\nNosocomial infection\nn\nMean (SE)\n95% Confidence interval\n\n\n\n\nNo\n106\n3.33 (0.068)\n3.19 to 3.46\n\n\nYes\n26\n3.73 (0.136)\n3.45 to 4.01\n\n\nDifference (Yes - No)\n\n0.39 (0.153)\n0.09 to 0.70\n\n\n\n\nHere, a two-sample t-test gives a test statistic of 2.59 with 130 degrees of freedom, and a P-value of 0.01.\nAs explained above, the estimated statistics would need to be converted back to the units in which the variable was measured. From Output 9.2, we can take the exponential of the corresponding log-transformed values:\n\nthe geometric mean of the infected group is approximately 41.5 days with a 95% confidence interval from 31.4 to 55.0 days.\nthe geometric mean of the uninfected group is approximately 27.9 days with a 95% confidence interval from 24.4 to 31.9 days."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-significance-tests",
    "href": "09-non-parametrics.html#non-parametric-significance-tests",
    "title": "4  Analysing non-normal data",
    "section": "4.3 Non-parametric significance tests",
    "text": "4.3 Non-parametric significance tests\nIt is often not possible or sensible to transform a non-normal distribution, for example if there are too many zero values or when we simply want to compare groups using the unit in which the measurement was taken (e.g. length of stay). For this, non-parametric significance tests can be used but the general idea behind these tests is that the data values are replaced by ranks. This also protects against outliers having too much influence.\n\n4.3.1 Ranking variables\nTable 9.1 shows how ranks are calculated for the first 21 patients in the length-of-stay data. First the data are sorted in order of their magnitude (from the lowest value to the highest) ignoring the group variable. Each data point is then assigned a rank. Data points that are equal are assigned the mean of their ranks. Thus, the two lengths of stay of 11 days share the ranks 4 and 5, and have a mean rank of 4.5. Similarly, there are 5 people with a length of stay of 14 days and these share the ranks 9 to 13, the mean of which is 11. Once ranks are computed they are assigned to each of the two groups and summed within each group.\n\n\n\n\nTransforming data to ranks: first 21 participants\nIDInfectionLength of stayRankInfection=noInfection=Yes\n\n32No011\n\n33No122\n\n12No933\n\n22No114.54.5\n\n16No114.54.5\n\n28Yes1266\n\n27No137.57.5\n\n20No137.57.5\n\n24No141111\n\n11No141111\n\n130No141111\n\n10No141111\n\n25No141111\n\n19No1515.515.5\n\n30No1515.515.5\n\n23No1515.515.5\n\n14No1515.515.5\n\n15No1720.520.5\n\n13No1720.520.5\n\n21Yes1720.520.5\n\n17No1720.520.5\n\n\n\n\nBy assigning ranks to individuals, we lose information about their actual values and this makes it more difficult to detect a difference. However, outliers and extreme values in the data are brought back closer to the data so that they are less influential. For this reason, non-parametric tests have less power than parametric tests and they require much larger differences in the data to show statistical significance between groups."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-two-independent-samples-wilcoxon-ranked-sum-test",
    "title": "4  Analysing non-normal data",
    "section": "4.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)",
    "text": "4.4 Non-parametric test for two independent samples (Wilcoxon ranked sum test)\nThe non-parametric equivalent to an independent samples t-test (Module 5) is the Wilcoxon ranked sum test, also known as the Mann-Whitney U test. In Stata, this can be obtained using the ranksum command.\nThe assumption for this test is that the distributions of the two populations have the same general shape. If this assumption is met, then this test evaluates the null hypothesis that the medians of the two populations are equal. This test does not assume that the populations are normally distributed, nor that their variances are equal.\nFor the length of stay data in the Worked Example 9.1, we first get a ranks table as shown in Output 9.3. The rank sum table gives us a direction of effect that the ranks are higher than expected in patients who had nosocomial infection. While the positive infection group has a lower sum of ranks because there were fewer people who contracted an infection, it is higher than expected, i.e. they have a longer length of stay compared with the negative infection group. This ranks table does not provide any summary statistics of direction of effect, central tendency or spread that describe the data.\n\nStata Output 9.3: Wilcoxon rank-sum test\n\nTwo-sample Wilcoxon rank-sum (Mann-Whitney) test\n\n      infect |      obs    rank sum    expected\n-------------+---------------------------------\n          No |      106        6620        7049\n         Yes |       26        2158        1729\n-------------+---------------------------------\n    combined |      132        8778        8778\n\nunadjusted variance    30545.67\nadjustment for ties      -53.87\n                     ----------\nadjusted variance      30491.80\n\nHo: los(infect==No) = los(infect==Yes)\n             z =  -2.457\n    Prob > |z| =   0.0140\n    Exact Prob =   0.0135\n\nThe test statistics are shown under the rank sum table in Output 9.3. The variance shown immediately under the table are used to conduct the test, and are not reported on.\nFrom Output 9.3, there are two P-values shown: one assuming normality of the ranks (not the underlying data), and an “Exact” P-value. The exact P-value is calculated when the sample size is not too large (less than 200), and is preferred. The rounded exact P value is 0.014 which indicates that the there is evidence of a difference in length of stay between the groups. This P-value should be provided alongside non-parametric summary statistics such as medians and inter-quartile ranges.\nUsing the summary command with the detail option in Stata and splitting the LOS variable by the Infect variable (as shown in Module 5), we can obtain the median length of stay values of 24 (Interquartile Range: 19 to 40 days) in the group with no infection and 37 (Interquartile Range: 24 to 50 days) in the group with infection."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "href": "09-non-parametrics.html#non-parametric-test-for-paired-data-wilcoxon-signed-rank-test",
    "title": "4  Analysing non-normal data",
    "section": "4.5 Non-parametric test for paired data (Wilcoxon signed-rank test)",
    "text": "4.5 Non-parametric test for paired data (Wilcoxon signed-rank test)\nThere are two types of non-parametric tests for paired data, called the Sign test and the Wilcoxon signed rank test. In practice, the Sign test is rarely used and will not be discussed in this course.\nIf the differences between two paired measurements are not normally distributed, a non-parametric equivalent of a paired t-test (Module 5) should be used. The equivalent test is the Wilcoxon matched-pairs signed rank test, also simply called the Wilcoxon matched-pairs test. This test is resistant to outliers in the data, however the proportion of outliers in the sample should be small. This test evaluates the null hypothesis that the median of the paired differences is equal to zero.\nIn this test, the absolute differences between the paired scores are ranked and the difference scores that are equal to zero (i.e. scores where there is no difference between the pairs) are excluded. Thus, the test is not suitable when a large proportion of the differences are zero because the effective sample size is reduced considerably.\n\n4.5.1 Worked Example\nA crossover trial is done to compare symptom scores for two drugs in 11 people with arthritis (higher scores indicate more severe symptoms). The data are contained in Stata datafile file mod09_arthritis.csv. The data are shown in Table 9.2. The descriptive statistics indicate that the differences are not normally distributed. You can use the Explore function in Stata to determine this.\n\n\n\n\nArthritis symptom scores for 11 patients after administering two drugs\nPatient IDScore: Drug 1Score: Drug 2Difference (Drug 2 – Drug 1)\n\n1341\n\n2275\n\n3341\n\n48102\n\n5682\n\n661-5\n\n7264\n\n8374\n\n9583\n\n109101\n\n11781\n\n\n\n\nBefore doing the analysis let us examine the distribution of the difference of symptom scores between the two drugs. As in Module 5, we first need to compute the difference between the symptom scores. To examine the distribution, we plot a histogram as shown in Figure @ref(fig:mod09-diff-hist).\n\n\n\n\n\nDistribution of difference in symptom scores between Drug 1 and Drug 2\n\n\n\n\nThe histogram shows that the differences are not normally distributed. The data looks negatively skewed with a gap in the histogram between the values of -5 and 0. Therefore, it would not be appropriate to conduct a paired t-test. Hence, we conduct a non-parametric paired test (Wilcoxon matched-pairs signed-rank test).\nA non-parametric paired test can be obtained in Stata using the signrank command and the results of the test are shown in Output 9.4.\n\nStata Output 9.4: Wilcoxon matched-pairs signed-rank test\n\nWilcoxon signed-rank test\n\n        sign |      obs   sum ranks    expected\n-------------+---------------------------------\n    positive |        1        10.5          33\n    negative |       10        55.5          33\n        zero |        0           0           0\n-------------+---------------------------------\n         all |       11          66          66\n\nunadjusted variance      126.50\nadjustment for ties       -1.63\nadjustment for zeros       0.00\n                     ----------\nadjusted variance        124.88\n\nHo: drug_1 = drug_2\n             z =  -2.013\n    Prob > |z| =   0.0441\n    Exact Prob =   0.0459\n\nThe table in Output 9.4 shows that there is 1 person who has a positive difference, where the symptom score on drug 2 that is smaller than that for drug 1 (i.e., drug 2 is better than drug 1); and 10 people who have a negative difference. No one has the same score for both drugs. The difference scores are ranked and the observed and expected sum of the ranks are shown in the output. This provides no intuitive summary statistics except to indicate which drug has higher ranks.\nThe test statistics are also shown under the table in Output 9.4. From the output, the exact P value of 0.046 indicates that there is evidence of a difference in symptom score between the two drugs."
  },
  {
    "objectID": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "href": "09-non-parametrics.html#non-parametric-estimates-of-correlation",
    "title": "4  Analysing non-normal data",
    "section": "4.6 Non-parametric estimates of correlation",
    "text": "4.6 Non-parametric estimates of correlation\nEstimating correlation using Pearson’s correlation coefficient can be problematic when bivariate Normality cannot be assumed, or in the presence of outliers or skewness. There are two commonly used non-parametric alternatives to Pearson’s correlation coefficient: Spearman’s rank correlation (\\(\\rho\\) or rho), and Kendall’s rank correlation (\\(\\tau\\) or tau).\nWhen estimating the correlation between x and y, Spearman’s rank correlation essentially replaces the observations x and y by their ranks, and calculates the correlation between the ranks. Kendall’s rank correlation compares the ranks between every possible combination of pairs of data to measure concordance: whether high values for x tend to be associated with high values for y (positively correlated) or low values of y (negatively correlated).\nIn terms of which is the more appropriate measure to use, the following passage from An Introduction to Medical Statistics (Bland (2015)) provides some guidance:\n\n“Why have two different rank correlation coefficients? Spearman’s \\(\\rho\\) is older than Kendall’s \\(\\tau\\), and can be thought of as a simple analogue of the product moment correlation coefficient, Pearson’s r. Kendall’s \\(\\tau\\) is a part of a more general and consistent system of ranking methods, and has a direct interpretation, as the difference between the proportions of concordant and discordant pairs. In general, the numerical value of \\(\\rho\\) is greater than that of \\(\\tau\\). It is not possible to calculate \\(\\tau\\) from \\(\\rho\\) or \\(\\rho\\) from \\(\\tau\\), they measure different sorts of correlation. \\(\\rho\\) gives more weight to reversals of order when data are far apart in rank than when there is a reversal close together in rank, \\(\\tau\\) does not. However, in terms of tests of significance, both have the same power to reject a false null hypothesis, so for this purpose it does not matter which is used.”\n\nWe will illustrate estimating rank correlation using the data mod08_lung_function.dta or mod08_lung_function.rds, which has information about height and lung function collected from a sample of 120 adults.\nThe Spearman rank correlation coefficient is estimated as 0.75, demonstrating a positive association between height and FVC. The Kendall rank correlation coefficient is estimated as 0.56, again demonstrating a positive association between height and FVC."
  },
  {
    "objectID": "09-non-parametrics.html#summary",
    "href": "09-non-parametrics.html#summary",
    "title": "4  Analysing non-normal data",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nIn this module, we have presented methods to conduct a hypothesis test with data that are not normally distributed. Non-parametric methods do not assume any distribution for the data and use significance tests based on ranks or sign (or both). A non-parametric test is always less powerful than its equivalent parametric test if the data are normally distributed and so whenever possible parametric significance tests should be used. In some cases when data are not normally distributed with a reasonably large sample size, the data can be transformed (most commonly by log transformation) to make the distribution normal. A parametric significance test should then be used with the transformed data to test the hypothesis."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-1",
    "title": "4  Analysing non-normal data",
    "section": "4.8 Transforming non-normally distributed variables",
    "text": "4.8 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram (for example, Figure 9.1).\nYou can calculate a new, transformed, variable using the generate command in Stata from the menu Data > Create or change data > Create new variable. Below are the instructions for creating a log to the base e, referred to as “ln” of the length of stay data for mod09_infection.dta.\nGo to Data > Create or change data > Create new variable. In the generate dialog box, type a name for your new variable into the Variable name: box: for example, ln_los. To include the ln function, you can either:\n\nsimply type ln(los + 1) directly into the Specify a value or an expression text box, or;\nclick the Create button to bring up the Expression Builder dialog box. Double click on Functions to expand the list in the Categories box, then click on Mathematical to display the list of mathematical functions in the box on the right. Scroll down to the ln() function and double click on it to bring it to the main text box. Next scroll down the Categories box to Variables and click on it to check the variables you have in the dataset. In the text box, replace x with los + 1 as shown below.\n\n\n\n\n\n\nClick the OK button when you are done to transfer the expression to the Specify a value or an expression box in the generate dialog box as shown below.\n\n\n\n\n\nClick OK and the new variable will appear in your dataset. You can check in your Variables window or your Data Editor window.\n[Command: gen ln_los=ln(los + 1)]\nYou can now check whether this variable is normally distributed as described in Module 2, for example with the histogram command as shown in Figure 9.2.\nTo obtain the back-transformed mean shown in Output 9.1, go to Data > Other Utilities > Hand calculator. In the display dialog box, expand the Functions list in the Categories box and select Mathematical. On the right-hand-side box, double-click on exp(). Replace x with the mean, 3.407232 as shown below.\nClick OK when you are done, then OK or Submit in the display dialog box.\n\n\n\n\n\n[Command: di exp(3.407232)]\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results."
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test",
    "title": "4  Analysing non-normal data",
    "section": "4.9 Wilcoxon ranked-sum test",
    "text": "4.9 Wilcoxon ranked-sum test\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.dta. To perform the Wilcoxon ranked-sum test go to: Statistics > Summaries, tables, and tests > Nonparametric tests of hypotheses > Wilcoxon ranked-sum test.\nIn the ranksum dialog box, select los as the Variable and select infect as the Grouping variable as shown below.\n\n\n\n\n\nClick OK or Submit to obtain Output 9.3.\n[Command: ranksum los, by(infect)]"
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test",
    "title": "4  Analysing non-normal data",
    "section": "4.10 Wilcoxon matched-pairs signed-rank test",
    "text": "4.10 Wilcoxon matched-pairs signed-rank test\nThe Wilcoxon matched-pairs signed-rank test in Stata will be demonstrated using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.\nTo do the analysis, go to: Statistics > Summaries, tables, and tests > Nonparametric tests of hypotheses > Wilcoxon matched-pairs sign-rank test. In the signrank dialog box, select drug_1 in the Variable box and type drug_2 in the Expression box. The dialog box will look like:\n\n\n\n\n\nClick OK or Submit to obtain Output 9.4. [Command: signrank drug_1 = drug_2]"
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients",
    "title": "4  Analysing non-normal data",
    "section": "4.11 Estimating rank correlation coefficients",
    "text": "4.11 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways.\nStatistics > Nonparametric analysis > Tests of hypotheses > Spearman’s rank correlation\n\n\n\n\n\nGiving the following output:\n\n. spearman Height FVC\n\n Number of obs =     120\nSpearman's rho =       0.7476\n\nTest of Ho: Height and FVC are independent\n    Prob > |t| =       0.0000\n\nStatistics > Nonparametric analysis > Tests of hypotheses > Kendall’s rank correlation\n\n\n\n\n\n\n. ktau Height FVC\n\n  Number of obs =     120\nKendall's tau-a =       0.5431\nKendall's tau-b =       0.5609\nKendall's score =    3878\n    SE of score =     439.463   (corrected for ties)\n\nTest of Ho: Height and FVC are independent\n     Prob > |z| =       0.0000  (continuity corrected)\n\nStata provides two versions of the Kendall rank correlation coefficient: we would use tau-b (\\(\\tau_b\\)) as it allows for tied observations."
  },
  {
    "objectID": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "href": "09-non-parametrics.html#transforming-non-normally-distributed-variables-2",
    "title": "4  Analysing non-normal data",
    "section": "4.12 Transforming non-normally distributed variables",
    "text": "4.12 Transforming non-normally distributed variables\nOne option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram.\nYou can calculate a new, transformed, variable using standard commands. For example, to create a new column of data based on the log of length of stay:\n\nlibrary(jmv)\n\nhospital <- readRDS(\"data/examples/mod09_infection.rds\")\n\nhospital$ln_los <- log(hospital$los+1)\ndescriptives(data=hospital, vars=c(los, ln_los))\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         los         ln_los      \n ─────────────────────────────────────────────── \n   N                          132          132   \n   Missing                      0            0   \n   Mean                  38.05303     3.407232   \n   Median                27.00000     3.332205   \n   Standard deviation    35.78057    0.7149892   \n   Minimum               0.000000     0.000000   \n   Maximum               244.0000     5.501258   \n ─────────────────────────────────────────────── \n\n\nYou can now check whether this logged variable is normally distributed as described in Module 2, for example by plotting a histogram as shown in Figure 9.2.\nTo obtain the back-transformed mean, we can use the exp command to anti-log the mean:\n\nexp(3.407232)\n\n[1] 30.18159\n\n\nIf your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results."
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "href": "09-non-parametrics.html#wilcoxon-ranked-sum-test-1",
    "title": "4  Analysing non-normal data",
    "section": "4.13 Wilcoxon ranked-sum test",
    "text": "4.13 Wilcoxon ranked-sum test\nWe use the wilcox.test function to perform the Wilcoxon ranked-sum test:\nwilcox.test(continuous_variable ~ group_variable, data=df)\nNote that the implementation of R’s Wilcoxon rank-sum test uses a “continuity correction” for calculating the P-value from the ranks. This differs from Stata which does not use the continuity correction. While the use of the continuity correction is preferable, in most cases the difference in P-values between the methods will be minimal.\nTo obtain results that are consistent with Stata, the correct=FALSE option can be used:\nwilcox.test(continuous_variable ~ group_variable, data=df, correct=FALSE)\nThe Wilcoxon ranked-sum test will be demonstrated using the length of stay data in mod09_infection.rds. Here, out continuous variable is los and the grouping variable is infect.\n\nwilcox.test(los ~ infect, data=hospital)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  los by infect\nW = 949, p-value = 0.01413\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "href": "09-non-parametrics.html#wilcoxon-matched-pairs-signed-rank-test-1",
    "title": "4  Analysing non-normal data",
    "section": "4.14 Wilcoxon matched-pairs signed-rank test",
    "text": "4.14 Wilcoxon matched-pairs signed-rank test\nThe wilcox.test function can also be used to conduct the Wilcoxon matched-pairs signed-rank test. The specification of the variables is a little different, in that each variable is specified as dataframe$variable:\nwilcox.test(df$continuous_variable_1, df$continuous_variable_1, paired=TRUE)\nWe will demonstrate using the dataset on the arthritis drug cross-over trial (mod09_arthritis.csv). Like the paired t-test the paired data need to be in separate columns.\n\narthritis <- read.csv(\"data/examples/mod09_arthritis.csv\")\n\narthritis$difference = arthritis$drug_1 - arthritis$drug_2\n\nhist(arthritis$difference, xlab=\"Difference\", main=\"Histogram of differences in pain scores\")\n\n\n\nwilcox.test(arthritis$drug_1, arthritis$drug_2, \n            paired=TRUE)\n\nWarning in wilcox.test.default(arthritis$drug_1, arthritis$drug_2, paired =\nTRUE): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  arthritis$drug_1 and arthritis$drug_2\nV = 10.5, p-value = 0.04898\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "href": "09-non-parametrics.html#estimating-rank-correlation-coefficients-1",
    "title": "4  Analysing non-normal data",
    "section": "4.15 Estimating rank correlation coefficients",
    "text": "4.15 Estimating rank correlation coefficients\nThe analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways:\n\nlung <- readRDS(\"data/examples/mod08_lung_function.rds\")\n\ncor.test(lung$Height, lung$FVC, method=\"spearman\")\n\nWarning in cor.test.default(lung$Height, lung$FVC, method = \"spearman\"): Cannot\ncompute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lung$Height and lung$FVC\nS = 72699, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7475566 \n\ncor.test(lung$Height, lung$FVC, method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  lung$Height and lung$FVC\nz = 8.8244, p-value < 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5609431"
  },
  {
    "objectID": "10-sample-size.html",
    "href": "10-sample-size.html",
    "title": "5  Sample size estimation",
    "section": "",
    "text": "Stata notes\nThe Power, precision and sample size menu item is located at the very bottom of the Statistics menu. You may need to scroll down the menu to locate the item.\nThe Power, precision and sample-size analysis dialog box appears as shown below.\nMany power and sample size procedures are available in the epiR package. We will also use one function from the pwr package.\nWe will use three functions from the epiR package in this module:\nWe will use one function from the pwr package:"
  },
  {
    "objectID": "10-sample-size.html#learning-objectives",
    "href": "10-sample-size.html#learning-objectives",
    "title": "5  Sample size estimation",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplain the issues involved in sample size estimation for epidemiological studies;\nEstimate sample sizes for descriptive and analytic studies;\nCompute the sample size needed for planned statistical tests;\nAdjust sample size calculations for factors that influence study power."
  },
  {
    "objectID": "10-sample-size.html#readings",
    "href": "10-sample-size.html#readings",
    "title": "5  Sample size estimation",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 35. [UNSW Library Link]\nBland (2015); Chapter 18. [UNSW Library Link]\nFor interest: Woodward (2013); Chapter 8. [UNSW Library Link]"
  },
  {
    "objectID": "10-sample-size.html#introduction",
    "href": "10-sample-size.html#introduction",
    "title": "5  Sample size estimation",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nDetermining the appropriate sample size (the number of participants in a study) is one of the most critical issues when designing a research study. A common question when planning a project is “How many participants do I need?” The sample size needs to be large enough to ensure that the results can be generalised to the population and will be accurate, but small enough for the study question to be answered with the resources available. In general, the larger the sample size, the more precise the study results will be.\nUnfortunately, estimating the sample size required for a study is not straightforward and the method used varies with the study design and the type of statistical test that will be conducted on the data collected. In the past, researchers calculated the sample size by hand using complicated mathematical formula. More recently, look-up tables have been created which has removed the need for hand calculations. Now, most researchers use computer programs where parameters relevant to the particular study design are entered and the sample size is automatically calculated. In this module, we will use an abbreviated look-up table to demonstrate the parameters that need to be considered when estimating sample sizes for a confidence interval and use software for all other sample size calculations. The look-up table allows you to see at a glance, the impact of different factors on the sample size estimation.\n\n5.1.1 Under and over-sized studies\nIn health research, there are different implications for interpreting the results if the sample size is too small or too large.\nAn under-sized study is one which lacks the power to find an effect or association when, in truth, one exists. If the sample size is too small, an important difference between groups may not be statistically significant and so will not be detected by the study. In fact, it is considered unethical to conduct a health study which is poorly designed so that it is not possible to detect an effect or association if it exists. Often, Ethics Committees request evidence of sample size calculations before a study is approved.\nA classic paper by Freiman et al examined 71 randomised controlled trials which reported an absence of clinical effect between two treatments.(Freiman et al. 1978) Many of the trials were too small to show that a clinically important difference was statistically significant. If the sample size of an analytic study is too small, then only very limited conclusions can be drawn about the results.\nIn general, the larger the sample size the more precise the estimates will be. However, large sample sizes have their own effect on the interpretation of the results. An over-sized study is one in which a small difference between groups, which is not important in clinical or public health terms, is statistically significant. When the study sample is large, the null hypothesis could be rejected in error and research resources may be wasted. This type of study may be unethical due to the unnecessary enrolment of a large number of people."
  },
  {
    "objectID": "10-sample-size.html#sample-size-estimation-for-descriptive-studies",
    "href": "10-sample-size.html#sample-size-estimation-for-descriptive-studies",
    "title": "5  Sample size estimation",
    "section": "5.2 Sample size estimation for descriptive studies",
    "text": "5.2 Sample size estimation for descriptive studies\nTo estimate the sample size required for a descriptive study, we usually focus on specifying the width of the confidence interval around our primary estimate. For example, to estimate the sample size for a study designed to measure a prevalence we need to:\n\nnominate the expected prevalence based on other available evidence;\nnominate the required level of precision around the estimate. For this, the width of the 95% confidence interval (i.e. the distance equal to 1.96 \\(\\times\\) SE) is used.\n\nTable 10.1 is an abbreviated look-up table that we can use to estimate the sample size for this type of study. Note that the sample size required to detect an expected population prevalence of 5% is the same as to detect a prevalence of 95%. Similarly 10% is equivalent to 90% etc. It is symmetric about 50%. From Table 5.1, you can see that the sample size required increases as the expected prevalence approaches 50% and as the precision increases (i.e. the required 95% CI becomes narrower).\n\n\n\n\nTable 5.1:  Sample size required to calculate a 95% confidence interval with a given precision \n\nWidth of 95% confidence interval (precision)\n\nPrevalence1%1.5%2%2.5%3%3.5%4%5%10%15%\n\n5% or 95%1,825812457292203149115\n\n10% or 90%3,4581,537865554385283217139\n\n15% or 85%4,8992,1771,22578454540030719649\n\n20% or 80%6,1472,7321,5379846835023852466228\n\n25% or 75%7,2033,2021,8011,1538015884512897333\n\n\n\n\n\n\n5.2.1 Worked Example\nA descriptive cross-sectional study is designed to measure the prevalence of bronchitis in children age 0-2 years with a 95% CI of \\(\\pm\\) 4%. The prevalence is expected to be 20%. From the table, a sample size of at least 385 will be required for the width of the 95% CI to be \\(\\pm\\) 4% (i.e. the reported precision of the summary statistic will be 20% (95% CI 16% to 24%)).\nIf the prevalence turns out to be higher than the researchers expected or if they decided that they wanted a narrower 95% CI (i.e. increase precision), a larger sample size would be required.\n\nWhat sample size would be required if the prevalence was 15% and the desired 95% CI was \\(\\pm\\) 3%?\nAnswer: 545"
  },
  {
    "objectID": "10-sample-size.html#sample-size-estimation-for-analytic-studies",
    "href": "10-sample-size.html#sample-size-estimation-for-analytic-studies",
    "title": "5  Sample size estimation",
    "section": "5.3 Sample size estimation for analytic studies",
    "text": "5.3 Sample size estimation for analytic studies\nAnalytic study designs are used to compare characteristics between different groups in the population. The main study designs are analytic cross-sectional studies, case-control studies, cohort studies and randomised controlled trials. For analytic study designs, the outcome measure of interest can be a mean value, a proportion or a relative risk if a random sample has been enrolled. For case-control studies the most appropriate measure of association is an odds ratio.\n\n5.3.1 Factors to be considered\nThe first important decision in estimating a required sample size for an analytic study is to select the type of statistical test that will be used to report or analyse the data. Each type of test is associated with a different method of sample size estimation.\nOnce the statistical method has been determined, the following issues need to be decided:\n\nStatistical power: the chance of finding a difference if one exists, e.g. 80%;\nLevel of significance: the P value that will be considered significant, e.g. P<0.05;\nMinimum effect size of interest: the size of the difference between groups e.g. the difference in the proportion of parents who oppose immunisation in two different regions or the difference in mean values of blood pressure in two groups of people with different types of cardiac disease;\nVariability: the spread of the measurements, e.g. the expected standard deviation of the main outcome variable (if continuous), or the expected proportions;\nResources: an estimate of the number of participants available and amount of funding to run the study.\n\nIn addition to deciding the level of power and probability that will be used, the difference between groups that is regarded as being important has to be estimated. The smallest difference between study groups that we want to detect is described as the minimum expected effect size. This is determined on the basis of clinical judgement, public health importance and expertise in the condition being researched, or may it be need to be determined from a pilot study or a literature review. The smaller the expected effect or association, the larger the sample size will need to obtain statistical significance. We also need some knowledge of how variable the measurement is expected to be. For this we often use the standard deviation for a continuous measure. As measurement variability increases, the sample size will need to increase in order to detect the expected difference between the groups. Above all, a study has to be practical in terms of the availability of a population from which to draw sufficient numbers for the study and in terms of the funds that are available to conduct the study.\n\n\n5.3.2 Power and significance level\n\nThe power of a study, which was discussed in Module 4, is the chance of finding a statistically significant difference when one exists, i.e. the probability of correctly rejecting the null hypothesis. The relationship between the power of a study and statistical significance is shown in Table 5.2.\n\n\n\n\nTable 5.2:  Comparison of study result with the truth \n\nStudy resultTruth\n\n EffectNo effect\n\nEvidence✓ɑ\n\nNo evidenceβ✓\n\n\n\n\n\n\nThe power of a study is expressed as 1– β where β is the probability of a false negative (that is, the probability of a Type II error - incorrectly not rejecting the null hypothesis. In most research, power is generally set to 80% (a Type II error rate of 20%). However, in some studies, especially in some clinical trials where rigorous results are required, power is set to 90% (a Type II error rate of 10%).\nThe significance level, or α level, is the level at which the P value of a test is considered to be statistically significant. The α level is usually set at 5% indicating a probability of <0.05 will be regarded as statistically significant. Occasionally, especially if several outcome measures are being compared, the α level is set at 1% indicating a probability of <0.01 will be regarded as statistically significant.\nThe calculation of sample sizes for analytic studies are based on calculations that are somewhat tedious to compute by hand. Software packages are the standard method of calculating sample sizes for these types of study, and examples from both R and Stata will be provided."
  },
  {
    "objectID": "10-sample-size.html#detecting-the-difference-between-two-means",
    "href": "10-sample-size.html#detecting-the-difference-between-two-means",
    "title": "5  Sample size estimation",
    "section": "5.4 Detecting the difference between two means",
    "text": "5.4 Detecting the difference between two means\nThe test that is used to show that two mean values are significantly different from one another is the independent samples t-test (Module 5). The sample size needed for this test to have sufficient power can be calculated using R and Stata as shown in the Worked Example below.\n\n5.4.1 Worked Example\nThere is a hypothesis that the use of the oral contraceptive (OC) pill in premenopausal women can increase systolic blood pressure. A study was planned to test this hypothesis using a two sided t-test. The investigators are interesting in detecting an increase of at least 5 mm Hg systolic blood pressure in the women using OC compared to the non-OC users with 90% power at a 5% significance level. A pilot study shows that the SD of systolic blood pressure in the target group is 25 mm Hg and the mean systolic blood pressure of non-OC user women is 90 mm Hg. What is the minimum number of women in each group that need to be recruited for the study to detect this difference?\nSolution The effect size of interest is 5 mm Hg and the associated standard deviation is 25 mm Hg. For power of 90% and alpha of 5%, the sample size calculation using the Stata can be calculated using the power twomeans command:\n\nOutput 10.1: Two independent samples t-test sample size calculation\n\n. power twomeans 90 95, sd(25) power(0.9)\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nHo: m2 = m1 versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n        delta =    5.0000\n           m1 =   90.0000\n           m2 =   95.0000\n           sd =   25.0000\n\nEstimated sample sizes:\n\n            N =     1,054\n  N per group =       527\n\nWe can use the epi.sscompc function within the epiR package in R to calculate the sample size:\n\nlibrary(epiR)\nlibrary(pwr)\n\nepi.sscompc(treat=90, control=95, n=NA, sigma=25, power=0.9)\n\n$n.total\n[1] 1052\n\n$n.treat\n[1] 526\n\n$n.control\n[1] 526\n\n$power\n[1] 0.9\n\n$delta\n[1] 5\n\n\nNote that Stata and R provide slightly different estimated sample sizes. This difference is immaterial from a practical point of view, and highlights the importance of referencing which software package has been used when writing up results.\nFrom the output, we can see that with 90% power we will need 526 or 527 participants in each group, i.e., 1052 or 1054 participants in total.\nIf the above were carried out by taking baseline measures of systolic blood pressure, and then again when the women were taking the OC pills, it would be a matched-pair study. Computing sample sizes for paired studies requires an estimate of the correlation between the paired obervations. If we do not have any estimates for this correlation, we can assume a value of 0. If the correlation is positive, a zero for correlation would give a more conservative estimate of sample size required (i.e. estimate a sample size larger than necessary). While a negative correlation would require a bigger sample size than a zero correlation, it is relatively uncommon to encounter negative correlations between pairs. Any discussions on the effect of correlation on sample size is beyond the scope of this course. Thus, we will always assume a correlation of zero between paired measurements in this course.\nWe can compute the required sample size in Stata using the power pairedmeans command:\n\n\nOutput 10.2: Paired samples t-test sample size using Worked Example 10.2\n\n. power pairedmeans 90 95, corr(0) power(0.9) sd(25)\n\nPerforming iteration ...\n\nEstimated sample size for a two-sample paired-means test\nPaired t test assuming sd1 = sd2 = sd\nHo: d = d0  versus  Ha: d != d0\n\nStudy parameters:\n\n        alpha =    0.0500          ma1 =   90.0000\n        power =    0.9000          ma2 =   95.0000\n        delta =    0.1414           sd =   25.0000\n           d0 =    0.0000         corr =    0.0000\n           da =    5.0000\n         sd_d =   35.3553\n\nEstimated sample size:\n\n            N =       528\n\nAs discussed in the R notes, calculating the sample size required for a paired t-test is a little more cumbersome in R. Here, only the output of the process is provided - refer to the R notes for detail on the code.\n\n\nOutput 10.2: Paired samples t-test sample size using Worked Example 10.2\n\n\n\n     Paired t test power calculation \n\n              n = 527.2954\n              d = 0.1414214\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nAssuming a correlation of 0 between the two sets of measurements, we can see that we will need 528 pairs of measurements to achieve a power of 90% (virtually the same as for an independent samples study)."
  },
  {
    "objectID": "10-sample-size.html#detecting-the-difference-between-two-proportions",
    "href": "10-sample-size.html#detecting-the-difference-between-two-proportions",
    "title": "5  Sample size estimation",
    "section": "5.5 Detecting the difference between two proportions",
    "text": "5.5 Detecting the difference between two proportions\nThe statistical test for deciding if there is a significant difference between two independent proportions is a Pearson’s chi-squared test (Module 7).\nOther than the power and alpha required for the test, the expected prevalence or incidence rate of the outcome factor needs to be estimated for each of the two groups being compared, based on what is known from other studies or what is expected. Occasionally, we may not know the expected proportion in one of the groups, e.g. in a randomised control trial of a novel intervention. In the sample size calculation for such a study, we should instead justify the minimum expected difference between the proportions based on what is important from a clinical or public health perspective. Based on the minimum difference, we can then derive the expected proportion for both groups. Note that the smaller the difference, the larger the sample size required.\nThe sample size required in each group to observe a difference in two independent proportions can be calculated using the power twoproportions command in Stata.\n\n5.5.1 Worked Example\nIf we expect that the prevalence of smoking in two comparison groups (e.g. males and females) will be 35% and 20%. The sample size required in each group to show that the prevalences are significantly different at P<0.05 with 80% power is shown in Output 10.3.\n\nOutput 10.3: Sample size calculation for two independent proportions\n\nEstimated sample sizes for a two-sample proportions test\nPearson's chi-squared test \nHo: p2 = p1  versus  Ha: p2 != p1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n        delta =   -0.1500  (difference)\n           p1 =    0.3500\n           p2 =    0.2000\n\nEstimated sample sizes:\n\n            N =       276\n  N per group =       138\n\nFrom Output 10.3, we see that we would need 138 males and 138 females (i.e. a total sample size of 276 participants).\nWhat sample size would be required if the prevalence of smoking among men was 30%?\nAnswer = 294 men and 294 women would be needed.\n[Command: power twoproportions .3 .2, test(chi2)]\nTo do the same problem using R, we use the epi.sscohortc function. We need to specify the risk of the the outcome each group (labelled group 0 and 1) and the desired power:\n\nepi.sscohortc(irexp1=0.35, irexp0=0.2, n=NA, power=0.8) \n\n$n.total\n[1] 276\n\n$n.exp1\n[1] 138\n\n$n.exp0\n[1] 138\n\n$power\n[1] 0.8\n\n$irr\n[1] 1.75\n\n$or\n[1] 2.153846"
  },
  {
    "objectID": "10-sample-size.html#detecting-an-association-using-a-relative-risk",
    "href": "10-sample-size.html#detecting-an-association-using-a-relative-risk",
    "title": "5  Sample size estimation",
    "section": "5.6 Detecting an association using a relative risk",
    "text": "5.6 Detecting an association using a relative risk\nThe relative risk is used to describe the association between an exposure and an outcome variable if the sample has been randomly selected from the population. This statistic is often used to describe the effect or association of an exposure in a cross-sectional or cohort study or the effect/association of a treatment in an randomised controlled trial. To estimate the sample size required for the RR to have a statistically significant P value, i.e. to show a significant association, we need to define: - the size of the RR that is considered to be of clinical or public health importance; - the event rate (rate of outcome) among the group who are not exposed to the factor of interest (reference group); - the desired level of significance (usually 0.05); - the desired power of the study (usually 80% or 90%).\nIn general, a RR of 2.0 or greater is considered to be of public health importance, however, a smaller RR can be important when exposure is high. For example, there may be a relatively small risk of respiratory infection among young children with a parent who smokes (RR ~ 1.2). If 25% of children are exposed to smoking in their home, then the high exposure rate leads to a very large number of children who have preventable respiratory infections across the community.\n\n5.6.1 Worked Example\nA study is planned to investigate the effect of an environmental exposure on the incidence of a certain common disease. In the general (unexposed) population the incidence rate of the disease is 50% and it is assumed that the incidence rate would be 75% in the exposed population. Thus the relative risk of interest would be 1.5 (i.e. 0.75 / 0.50). We want to detect this effect with 90% power at a 5% level of significance.\nWe can use the Stata command power twoproportions as below:\n\n5.6.1.1 Output 10.4: Sample size calculation for relative risk\n\nEstimated sample sizes for a two-sample proportions test\nPearson's chi-squared test \nHo: p2 = p1  versus  Ha: p2 != p1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n        delta =    1.5000  (relative risk)\n           p1 =    0.5000\n           p2 =    0.7500\n        rrisk =    1.5000\n\nEstimated sample sizes:\n\n            N =       154\n  N per group =        77\n\nFrom Output 10.4, we can see that for a control proportion of 0.5 and RR of 1.5, we need a total sample size of 154, that is 77 people would be needed in each of the exposure groups.\nThe epiR package does not have a function to estimate sample size and power directly for a relative risk, but we can use the epi.sscohortc function. To do this, we recognise that the assumed rate in the exposed group will equal the rate in the unexposed group multiplied by the relative risk.\nHere we define the risk in the unexposed group as 0.5 and the desired relative risk to detect is 1.5. So we specify irexp0 = 0.5 and irexp1 = 1.5 * 0.5:\n\nepi.sscohortc(irexp1=1.5*0.5, irexp0=0.5, n=NA, power=0.9)\n\n$n.total\n[1] 154\n\n$n.exp1\n[1] 77\n\n$n.exp0\n[1] 77\n\n$power\n[1] 0.9\n\n$irr\n[1] 1.5\n\n$or\n[1] 3"
  },
  {
    "objectID": "10-sample-size.html#detecting-an-association-using-an-odds-ratio",
    "href": "10-sample-size.html#detecting-an-association-using-an-odds-ratio",
    "title": "5  Sample size estimation",
    "section": "5.7 Detecting an association using an odds ratio",
    "text": "5.7 Detecting an association using an odds ratio\nIf we are designing a case-control study, the appropriate measure of effect is an odds ratio. The method for estimating the sample size required to detect an odds ratio of interest is slightly different to that for the relative risk. However, the same parameters are required for the estimation:\n\nthe minimum OR to be considered clinically important;\nthe proportion of exposed among the control group;\nthe desired level of significance (usually 0.05);\nthe desired power of the study (usually 80% or 90%).\n\n\n5.7.1 Worked Example\nA case-control study is designed to examine an association between an exposure and outcome factor. Existing literature shows that 30% of the controls are expected to be exposed. We want to detect a minimum OR of 2.0 with 90% power and 5% level of significance.\n\n. power twoproportions .3, test(chi2) oratio(2) power(0.9)\nEstimated sample sizes for a two-sample proportions test\nPearson's chi-squared test \nHo: p2 = p1  versus  Ha: p2 != p1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n        delta =    2.0000  (odds ratio)\n           p1 =    0.3000\n           p2 =    0.4615\n   odds ratio =    2.0000\n\nEstimated sample sizes:\n\n            N =       376\n  N per group =       188\n\nWe can use the epi.sscc function in R to calculate a sample size based on an odds ratio in a case-control study:\n\nepi.sscc(OR=2, p0=0.3, n=NA, power=0.9)\n\n$n.total\n[1] 376\n\n$n.case\n[1] 188\n\n$n.control\n[1] 188\n\n$power\n[1] 0.9\n\n$OR\n[1] 2\n\n\nWe find that 188 controls and 188 cases are required i.e. a total of 376 participants.\nThis sample size would be smaller if we increased the effect size (OR) or reduced the study power to 80%. You could try this in either Stata or R (answer: 141 per group if power is reduced to 80%)."
  },
  {
    "objectID": "10-sample-size.html#factors-that-influence-power",
    "href": "10-sample-size.html#factors-that-influence-power",
    "title": "5  Sample size estimation",
    "section": "5.8 Factors that influence power",
    "text": "5.8 Factors that influence power\n\n5.8.1 Dropouts\nIt is common to increase estimated sample sizes to allow for drop-outs or non-response. To account for drop-outs, the estimated sample size can be divided by (1 minus the dropout rate). Consider the following case:\n\nn-completed: the number who will complete the study (i.e. n after drop-out)\nn-recruited: the number who should be recruited (i.e. n before drop-out)\nd: drop-out rate (as a proportion - i.e. a number between 0 and 1)\n\nThen n-completed = n-recruited × (1 - d)\nRe-arranging this formula gives: n-recruited = n-completed ÷ (1 - d).\n\n\n5.8.2 Unequal groups\nMany factors that come into play in a study can reduce the estimated power of a study. In clinical trials, it is not unusual for recruitment goals to be much harder to achieve than expected and therefore for the target sample size to be impossible to realise within the timeframe planned for recruitment.\nIn case-control studies, the number of potential case participants available may be limited but study power can be maintained by enrolling a greater number of controls than cases. Or in an experimental study, more participants may be randomised to the new treatment group to test its effects accurately when much is known about the effect of standard care and a more precise estimate of the new treatment effect is required.\nHowever, there is a trade-off between increasing the ratio of group size and the total number that needs to be enrolled. Consider Worked Example @ref(wex10-5): selecting an equal number of controls and cases would require 188 cases and 188 controls, a total of 376 participants.\nWe may want to reduce the number of cases required, by selecting 2 controls for every case. When performing sample size calculations with unequal groups, Stata refers to cases as N2, and controls as N1. Selecting 2 controls (N1) per case (N2) (corresponding to a ratio of N2/N1 0.5 in Stata) would require 140 cases and 280 controls, a total of 420 participants. We can extend this example and investigate the impact of changing the ratio of controls per case.\n\n\n\n\nIncreasing the number of controls per case\nControls per caseStata's allocation ratio (N2/N1)     Number of cases requiredNumber of controls requiredTotal participants required\n\n11     188188376\n\n20.5   140280420\n\n30.3333124371495\n\n40.25  116462578\n\n50.2   111553664\n\n60.1666108644752\n\n70.1429105734839\n\n80.125 104825929\n\n90.11111029161,018\n\n100.1   1011,0061,107\n\n\n\n\nThis can be visualised graphically, as in Figure 5.1.\n\n\n\n\n\nFigure 5.1: Increasing the number of controls per case\n\n\n\n\nWe can see that the number of cases required drops off if we go from 1 to 2 controls per case, and again from 2 to 3 controls per case. Once we go from 3 to 4 controls per case, we only reduce the number of cases by 8 (124 vs 116 cases), but at an increase of 91 (371 vs 462) controls. Clearly, this reduction in cases is not offset by the extra controls required.\nIn Stata, nratio is experimental group / control group. I’m not sure the ratio in R is correct!\n\n\n\n\n\n\n\n\n\nStata\nR\n\n\n\n\nDifference in means\nn-exposed / n-controls\nn-exposed / n-unexposed\n\n\nDifference in proportions\nn-exposed / n-controls\nn-exposed / n-unexposed\n\n\nRelative risk\nn-exposed / n-controls\nn-exposed / n-unexposed\n\n\nOdds ratio\nn-cases / n-controls\nn-controls / n-cases"
  },
  {
    "objectID": "10-sample-size.html#limitations-in-sample-size-estimations",
    "href": "10-sample-size.html#limitations-in-sample-size-estimations",
    "title": "5  Sample size estimation",
    "section": "5.9 Limitations in sample size estimations",
    "text": "5.9 Limitations in sample size estimations\nIn this module we have seen how to use Stata for estimating the sample size requirement of a study given the statistical test that will be used and the expected characteristics of the sample. However, once a study is underway, it is not unusual for sample size to be compromised by the lack of research resources, difficulties in recruiting participants or, in a clinical trial, participants wanting to change groups when information about the new experimental treatment rapidly becomes available in the press or on the internet.\nOne approach that is increasingly being used is to conduct a blinded interim analysis say when 50% of the total data that are planned have been collected. In this, a statistician external to the research team who is blinded to the interpretation of the group code is asked to measure the effect size in the data with the sole aim of validating the sample size requirement. It is rarely a good idea to use an interim analysis to reduce the planned sample size and terminate a trial early because the larger the sample size, the greater the precision with which the treatment effect is estimated. However, interim analyses are useful for deciding whether the sample size needs to be increased in order to answer the study question and avoid a Type II error."
  },
  {
    "objectID": "10-sample-size.html#summary",
    "href": "10-sample-size.html#summary",
    "title": "5  Sample size estimation",
    "section": "5.10 Summary",
    "text": "5.10 Summary\nIn this module we have discussed the importance of conducting a clinical or epidemiological study with enough participants so that an effect or association can be identified if it exists (i.e. study power), and how this has to be balanced by the need to not enrol more participants than necessary because of resource issues. We have looked at the parameters that need to be considered when estimating the sample size for different studies and have used a look-up table to estimate required sample size for a prevalence study and Stata to estimate appropriate sample sizes in epidemiological research under the most straightforward situations. The common requirement in all the situations is that the researchers need to specify the minimum effect measure (e.g. difference in means, OR, RR etc) they want to detect with a given probability (usually 80% to 90%) at a certain level of significance (usually P<0.05). The ultimate decision on the sample size depends on a compromise among different objectives such as power, minimum effect size, and available resources. To make the final decision, it is helpful to do some trial calculations using revised power and the minimum detectable effect measure."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-two-independent-samples-t-test",
    "href": "10-sample-size.html#sample-size-calculation-for-two-independent-samples-t-test",
    "title": "5  Sample size estimation",
    "section": "5.11 Sample size calculation for two independent samples t-test",
    "text": "5.11 Sample size calculation for two independent samples t-test\nTo do the problem discussed in Worked Example 10.2, you can expand the Means item under Population parameter on the left-hand-side of the Power and sample-size analysis dialog box, then choose Two independent samples. On the right-hand-side of the dialog box, choose Test comparing two independent means.\n\n\n\n\n\nThe power twomeans dialog box will appear. Based on the information in Worked Example 10.2, change Power to 0.9 and enter 25 as the Common standard deviation. For the means, we can assume that Control mean systolic blood pressure is 90 mmHg and the the Experimental mean systolic blood pressure is 5 mmHg higher at 95 mmHg. We use equal numbers in each group (allocation ratio of 1 which is the default) because that would give us the smallest total sample size required.\n\n\n\n\n\nClick OK or Submit to produce Output 10.1.\n[Command: power twomeans 90 95, sd(25) power(0.9)]"
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-paired-samples-t-test",
    "href": "10-sample-size.html#sample-size-calculation-for-paired-samples-t-test",
    "title": "5  Sample size estimation",
    "section": "5.12 Sample size calculation for paired samples t-test",
    "text": "5.12 Sample size calculation for paired samples t-test\nFor a paired sample t-test, go back to the Power and sample-size analysis dialog box and click on Two paired samples under Means on the left-hand-side, then choose Paired test comparing two correlated means, specify correlation between paired observations as shown below.\n\n\n\n\n\nIn the power pairedmeans dialog box, change Power to 0.9 and enter 25 as the Common standard deviation as shown below. For the means, we can assume that Control mean systolic blood pressure is 90 mmHg and the the Experimental mean systolic blood pressure is 5 mmHg higher at 95 mmHg as we had for the independent samples.\n\n\n\n\n\nClick OK or Submit to produce Output 10.2.\n[Command: power pairedmeans 90 95, corr(0) power(0.9) sd(25)]"
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-difference-between-two-independent-proportions",
    "href": "10-sample-size.html#sample-size-calculation-for-difference-between-two-independent-proportions",
    "title": "5  Sample size estimation",
    "section": "5.13 Sample size calculation for difference between two independent proportions",
    "text": "5.13 Sample size calculation for difference between two independent proportions\nTo do the problem discussed in Worked Example 10.3, you can expand the Proportions item under Population parameter on the left-hand-side of the Power and sample-size analysis dialog box, then choose Two independent samples. On the right-hand-side of the dialog box, choose Chi-squared test comparing two independent proportions.\n\n\n\n\n\nThe power twoproportions dialog box will appear. Based on the information in Worked Example 10.3, check the Power is 0.8 and Significance level is 0.05 (these are the default values), then enter 0.35 and 0.2 as the Proportions as shown below. As with the two independent samples example, we can assume equal numbers in each group (Allocation ratio of 1 which is the default).\n\n\n\n\n\nClick OK or Submit to obtain Output 10.3.\n[Command: power twoproportions .35 .2, test(chi2)]\nNote: It doesn’t matter if you swap the proportions for the Control and Experimental group, i.e. the command power twoproportions .2 .35 , test(chi2) gives the same results.\nIf you had difference in proportion, relative risk or odds ratio for the sample size calculation, you can choose them from the drop-down list under Experimental as shown below."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-with-relative-risk-and-odds-ratio",
    "href": "10-sample-size.html#sample-size-calculation-with-relative-risk-and-odds-ratio",
    "title": "5  Sample size estimation",
    "section": "5.14 Sample size calculation with relative risk and odds ratio",
    "text": "5.14 Sample size calculation with relative risk and odds ratio\nUsing information from Worked Example 10.4, we change Power to 0.9, enter 0.5 as the Control Proportion. As we are working with a relative risk, choose Relative risk as the effect size, and enter 1.5 as shown below.\n\n\n\n\n\nClick OK or Submit to obtain Output 10.4.\nNow we calculate the sample size for Worked Example 10.5. Enter 0.3 as the Control Proportion, choose Odds ratio as the estimate from the dropdown list and enter 2 as the Odds ratio. In this example, you can also try entering two values for Power: 0.9 and 0.8 as shown below.\n\n\n\n\n\nClick OK or Submit to obtain the output below.\n\nEstimated sample sizes for a two-sample proportions test\nPearson's chi-squared test \nHo: p2 = p1  versus  Ha: p2 != p1\n\n  +-------------------------------------------------------------------------+\n  |   alpha   power       N      N1      N2   delta      p1      p2  oratio |\n  |-------------------------------------------------------------------------|\n  |     .05      .8     282     141     141       2      .3   .4615       2 |\n  |     .05      .9     376     188     188       2      .3   .4615       2 |\n  +-------------------------------------------------------------------------+"
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-the-independent-samples-t-test",
    "href": "10-sample-size.html#sample-size-calculation-for-the-independent-samples-t-test",
    "title": "5  Sample size estimation",
    "section": "5.15 Sample size calculation for the independent samples t-test",
    "text": "5.15 Sample size calculation for the independent samples t-test\nTo do the problem discussed in Worked Example 10.2, we use the epi.sscompc function:\nepi.sscompc(treat, control, n, sigma, power, \n   r = 1, design = 1, sided.test = 2, nfractional = FALSE, conf.level = 0.95)\nThe first line contains parameters that we usually specify, with the second line usually left as the defaults. We must define the expected mean in the treatment and control groups, and the standard deviation of the measure. We specify one of n or power to be the measure to estimate, by specifying the unknown value as being equal to R’s missing value, NA.\nFor example, to calculate the required sample size in Worked Example 10.2, we specify:\n\nthe assumed mean in the experimental, or treatment, group: 90mmHg\nthe assumed mean in the control group: 95mmHg\nthe standard deviation of blood pressure: 25mmHg\nthe required power, 0.9 (representing 90%)\n\nThe values on the second line of the function are defined by default, and we can leave these as default.\nPutting this all together, and specifying the sample size as unknown:\n\nepi.sscompc(treat=90, control=95, n=NA, sigma=25, power=0.9) \n\n$n.total\n[1] 1052\n\n$n.treat\n[1] 526\n\n$n.control\n[1] 526\n\n$power\n[1] 0.9\n\n$delta\n[1] 5\n\n\nThe results indicate that we need 526 participants in each group, or 1052 in total. Note that these numbers are slightly different from the Stata estimates (527 in each group).\nWe can define whether we want unequal numbers in each group by specifying r: the number in the treatment group divided by the number in the control group."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-the-paired-t-test",
    "href": "10-sample-size.html#sample-size-calculation-for-the-paired-t-test",
    "title": "5  Sample size estimation",
    "section": "5.16 Sample size calculation for the paired t-test",
    "text": "5.16 Sample size calculation for the paired t-test\nCalculating the sample size required for a paired t-test is a little more cumbersome. We can use the following code to specify:\n\nm1: the mean of the first paired observations\nm2: the mean of the second paired observations\ns_group: the common standard deviation\ncorr: the assumed correlation between the paired observations (conservatively set to 0)\n\nThe code below then uses the pwr.t.test function within the pwr library to estimate the number of pairs. For example, to replicate Output 10.2:\n\nm1 <- 90\nm2 <- 95\ns_group <- 25\ncorr <- 0\n\ns_paired <- sqrt(2 * s_group^2 - 2*corr*s_group^2)\n\nd <- ((m1 - m2)/s_paired)\n\npwr.t.test(d=d, power=0.9, type=\"paired\")\n\n\n     Paired t test power calculation \n\n              n = 527.2954\n              d = 0.1414214\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nAs per the Stata calculations, we require 528 pairs of observations (noting that sample sizes are always rounded up)."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-for-difference-between-two-independent-proportions-1",
    "href": "10-sample-size.html#sample-size-calculation-for-difference-between-two-independent-proportions-1",
    "title": "5  Sample size estimation",
    "section": "5.17 Sample size calculation for difference between two independent proportions",
    "text": "5.17 Sample size calculation for difference between two independent proportions\nTo do the problem discussed in Worked Example 10.3, we use the epi.sscohortc function:\nepi.sscohortc(irexp1, irexp0, pexp = NA, n = NA, power = 0.80, \n   r = 1, N, design = 1, sided.test = 2, finite.correction = FALSE, \n   nfractional = FALSE, conf.level = 0.95)\nWe can enter:\n\nirexp1: the assumed risk of the outcome in the exposed group: here 0.35\nirexp0: the assumed risk of the outcome in the unexposed group: here 0.2\nn: the total sample size, to be determined\npower: the required power: here 0.8 (representing 80%)\n\n\nepi.sscohortc(irexp1=0.35, irexp0=0.2, n=NA, power=0.8) \n\n$n.total\n[1] 276\n\n$n.exp1\n[1] 138\n\n$n.exp0\n[1] 138\n\n$power\n[1] 0.8\n\n$irr\n[1] 1.75\n\n$or\n[1] 2.153846\n\n\nNote: It doesn’t matter if you swap the proportions for the exposed and unexposed groups, i.e. the command epi.sscohortc(irexp1=0.2, irexp0=0.35, n=NA, power=0.8) gives the same results."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-with-a-relative-risk",
    "href": "10-sample-size.html#sample-size-calculation-with-a-relative-risk",
    "title": "5  Sample size estimation",
    "section": "5.18 Sample size calculation with a relative risk",
    "text": "5.18 Sample size calculation with a relative risk\nThe epiR package does not have a function to estimate sample size and power directly for a relative risk, but we can use the epi.sscohortc function. To do this, we recognise that the assumed rate in the exposed group will equal the rate in the unexposed group multiplied by the relative risk.\nHere we will replicate Output 10.4, where p0=0.5 and the desired relative risk to detect is 1.5. So we specify irexp0 = 0.5 and irexp1 = 1.5 * 0.5:\n\nepi.sscohortc(irexp1=1.5*0.5, irexp0=0.5, n=NA, power=0.9)\n\n$n.total\n[1] 154\n\n$n.exp1\n[1] 77\n\n$n.exp0\n[1] 77\n\n$power\n[1] 0.9\n\n$irr\n[1] 1.5\n\n$or\n[1] 3\n\n\nHence we require 77 participants in each group, or 154 participants in total."
  },
  {
    "objectID": "10-sample-size.html#sample-size-calculation-with-an-odds-ratio",
    "href": "10-sample-size.html#sample-size-calculation-with-an-odds-ratio",
    "title": "5  Sample size estimation",
    "section": "5.19 Sample size calculation with an odds ratio",
    "text": "5.19 Sample size calculation with an odds ratio\nWe can use the epi.sscc function to calculate a sample size based on an odds ratio in a case-control study:\nepi.sscc(OR, p1 = NA, p0, n, power, r = 1, \n   phi.coef = 0, design = 1, sided.test = 2, nfractional = FALSE, \n   conf.level = 0.95, method = \"unmatched\", fleiss = FALSE)\nUsing information from Worked Example 10.4, we specify:\n\nOR: the odds ratio to be detected, here 1.5\np0: the proportion of the outcome in the controls, here 0.5\nn: the sample size, here to be calculated\npower: the required study power, here 0.9\n\n\nepi.sscc(OR=1.5, p0=0.5, n=NA, power=0.9)\n\n$n.total\n[1] 1038\n\n$n.case\n[1] 519\n\n$n.control\n[1] 519\n\n$power\n[1] 0.9\n\n$OR\n[1] 1.5\n\n\nNow we calculate the sample size for Worked Example 10.5:\n\nepi.sscc(OR=2, p0=0.3, n=NA, power=0.9)\n\n$n.total\n[1] 376\n\n$n.case\n[1] 188\n\n$n.control\n[1] 188\n\n$power\n[1] 0.9\n\n$OR\n[1] 2\n\n\nHere we see that we require a total of 376 participants to detect an odds ratio of 2.0 with 90% power;\n\nepi.sscc(OR=2, p0=0.3, n=NA, power=0.8)\n\n$n.total\n[1] 282\n\n$n.case\n[1] 141\n\n$n.control\n[1] 141\n\n$power\n[1] 0.8\n\n$OR\n[1] 2\n\n\nor a total of 282 participants to detect an odds ratio of 2.0 with 80% power."
  },
  {
    "objectID": "10-sample-size.html#estimating-sample-sizes-with-unequal-groups",
    "href": "10-sample-size.html#estimating-sample-sizes-with-unequal-groups",
    "title": "5  Sample size estimation",
    "section": "5.20 Estimating sample sizes with unequal groups",
    "text": "5.20 Estimating sample sizes with unequal groups\nTo change the allocation ratio in any of these study type, we can specify r. Note that the definition of r differs slightly depending on the function used, so it pays to check the help-file for each function. In summary:\n\nfor epi.sscompc(), r is the number in the treatment group divided by the number in the control group\nfor epi.sscohortc(), r is the number in the exposed group divided by the number in the unexposed group\nfor epi.sscc(), r is the number in the control group divided by the number in the case group"
  }
]