[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Preface\n\nlibrary(huxtable)\nlibrary(magrittr)\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nHere is a method for including mathematical notation in tables:\n\n\n\n\nTable 1.1:  A table with mathematical symbols \n\nSymbolInterpretation\n\n\\(y \\)Observed value of the outcome variable\n\n\\(x \\)Observed value of the explanatory variable\n\n\\(\\beta_0\\)Intercept of the regression line\n\n\\(\\beta_1\\)Slope of the regression line"
  },
  {
    "objectID": "03-precision.html#learning-objectives",
    "href": "03-precision.html#learning-objectives",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplain the purpose of sampling, different sampling methods and their implications for data analysis;\nDistinguish between standard deviation of a sample and standard error of a mean;\nRecognise the importance of the central limit theorem;\nCalculate the standard error of a mean;\nCalculate and interpret confidence intervals for a mean;\nBe familiar with the t-distribution and when to use it."
  },
  {
    "objectID": "03-precision.html#readings",
    "href": "03-precision.html#readings",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapters 4 and 6. [UNSW Library Link]\nBland (2015); Sections 3.3 and 3.4, 8.1 to 8.3. [UNSW Library Link]"
  },
  {
    "objectID": "03-precision.html#introduction",
    "href": "03-precision.html#introduction",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nTo describe the characteristics of a population we can gather data about the entire population (as is undertaken in a national census) or we can gather data from a sample of the population. When undertaking a research study, taking a sample from a population is far more cost-effective and less time consuming than collecting information from the entire population. When a sample of a population is selected, summary statistics that describe the sample are used to make inferences about the total population from which the sample was drawn. These are referred to as inferential statistics.\nHowever, for the inferences about the population to be valid, a random sample of the population must be obtained. The goal of using random sampling methods is to obtain a sample that is representative of the target population. In other words, apart from random error, the information derived from the sample is expected to be much the same as the information collected from a complete population census as long as the sample is large enough."
  },
  {
    "objectID": "03-precision.html#sampling-methods",
    "href": "03-precision.html#sampling-methods",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "2.2 Sampling methods",
    "text": "2.2 Sampling methods\nMethods have been designed to select participants from a population such that each person in the target population has an equal probability of being chosen. Methods that use this approach are called random sampling methods. Examples include simple random sampling and stratified random sampling.\nIn simple random sampling, every person in the population from which the sample is drawn has the same random chance of being selected into the sample. To implement this method, every person in the population is allocated an ID number and then a random sample of the ID numbers is selected. Software packages can be used to generate a list of random numbers to select the random sample.\nIn stratified sampling, the population is divided into distinct non-overlapping subgroups (strata) according to an important characteristic (e.g. age or sex) and then a random sample is selected from each of the strata. This method is used to ensure that sufficient numbers of people are sampled from each stratum and therefore each subgroup of interest is adequately represented in the sample.\nThe purpose of using random sampling is to minimise selection bias to ensure that the sample enrolled in a study is representative of the population being studied. This is important because the summary statistics that are obtained can then be regarded as valid in that they can be applied (generalised) back to the population.\nA non-representative sample can occur when random sampling is used, simply by chance. However, non-random sampling methods, such as using a convenient study population, will often result in a non-representative sample being selected so that the summary statistics from the sample cannot be generalised back to the population from which the participants were drawn. The effects of non-random error are much more serious than the effects of random error. Concepts such as non-random error (i.e. systematic bias), selection bias, validity and generalisability are discussed in more detail in PHCM9476: Foundations of Epidemiology."
  },
  {
    "objectID": "03-precision.html#standard-error-and-precision",
    "href": "03-precision.html#standard-error-and-precision",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "2.3 Standard error and precision",
    "text": "2.3 Standard error and precision\nModule 1 introduced the mean, variance and standard deviation as measures of central tendency and spread for continuous measurements from a sample or a population. As described in Module 1, we rarely have data on the entire population but we can infer information about the population (e.g. the mean weight of people in the population) based on a sample. However, a sample taken from a population is usually a small proportion of the total population. If the sample is very small, we would not expect our estimate of the population mean value to be precise. If the sample is very large, we would expect a more precise estimate of the population mean, i.e. the estimated mean value would be much closer to the true mean value in the population.\n\n2.3.1 The standard error of the mean\nA point estimate is a single best guess of the true value in the population. Instead of trying to guess the true value, it may be preferable to give a range of values in which we think the true value lies. For example, suppose we want to estimate the average weight of a population, and found a sample mean of 65 kg. Rather than saying we believe the true mean to be 65 kg, we could say we believe it is somewhere between, say, 58 kg and 72 kg.\nOften in papers, one will see something like “the mean is 70.24 \\(\\pm\\) 1.78 kg”. The value 1.78 is the standard error of the mean (sometimes shortened to S.E.M. or S.E.). The standard error of the mean measures the extent to which we expect the means from different samples to vary because of chance error in the sampling process. The standard error is a measure of precision of the point estimate. This statistic is directly proportional to the standard deviation of the variable, and inversely proportional to the size of the sample. The standard error of the mean for a continuously distributed measurement for which the SD is an accurate measure of spread is computed as follows:\n\\[ \\text{SE}(\\bar{x}) = \\frac{\\text{SD}}{\\sqrt{n}} \\] For our sample of weight data from 30 patients in Module 1:\n\\[ \\text{SE}(\\bar{x}) = \\frac{\\text{5.04}}{\\sqrt{30}} = 0.92 \\] Because the calculation uses the sample size (n) (i.e. the number of study participants) in the denominator, the SE will become smaller when the sample size becomes larger. A smaller SE indicates that the estimated mean value is more precise.\nThe standard error is an important statistic that is related to sampling variation. When a random sample of a population is selected, it is likely to differ in some characteristic compared with another random sample selected from the same population. Also, when a sample of a population is taken, the true population mean is an unknown value.\nJust as the standard deviation measures the spread of the data around the population mean, the standard error of the mean measures the spread of the sample means. Note that we do not have different samples, only one. It is a theoretical concept which enables us to conduct various other statistical analyses."
  },
  {
    "objectID": "03-precision.html#central-limit-theorem",
    "href": "03-precision.html#central-limit-theorem",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "2.4 Central limit theorem",
    "text": "2.4 Central limit theorem\nEven though we now have an estimate of the mean and its standard error, we might like to know what the mean from a different random sample of the same size might be. To do this, we need to know how sample means are distributed. In determining the form of the probability distribution of the sample mean (\\(\\bar{x}\\)), we consider two cases:\n\n2.4.1 When the population distribution is unknown:\nThe central limit theorem for this situation states:\n\nIn selecting random samples of size \\(n\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean \\(\\bar{x}\\) approaches a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\tfrac{\\sigma}{\\sqrt{n}}\\) as the sample size becomes large.\n\nThe sample size n = 30 and above is a rule of thumb for the central limit theorem to be used. However, larger sample sizes may be needed if the distribution is highly skewed.\n\n\n2.4.2 When the population is assumed to be normal:\nIn this case the sampling distribution of \\(\\bar{x}\\) is normal for any sample size."
  },
  {
    "objectID": "03-precision.html#confidence-interval-of-the-mean",
    "href": "03-precision.html#confidence-interval-of-the-mean",
    "title": "2  Precision, standard errors and confidence intervals",
    "section": "2.5 95% confidence interval of the mean",
    "text": "2.5 95% confidence interval of the mean\nIn Module 2, we showed that the characteristics of a Standard Normal Distribution are that 95% of the data lie within 1.96 standard deviations from the mean (Figure 2.2). Because the central limit theorem states that the sampling distribution of the mean is approximately Normal in large enough samples, we expect that 95% of the mean values would fall within 1.96 × SE units above and below the measured mean population value.\nFor example, if we repeated the study on weight 100 times using 100 different random samples from the population and calculated the mean weight for each of the 100 samples, approximately 95% of the values for the mean weight calculated for each of the 100 samples would fall within 1.96 × SE of the population mean weight.\nThis interpretation of the SE is translated into the concept of precision as a 95% confidence interval (CI). A 95% CI is a range of values within which we have 95% confidence that the true population mean lies. If an experiment was conducted a very large number of times, and a 95%CI was calculated for each experiment, 95% of the confidence intervals would contain the true population mean.\nThe calculation of the 95% CI for a mean is as follows:\n\\[  \\bar{x} \\pm 1.96 \\times \\text{SE}( \\bar{x} ) \\] This is the generic formula for calculating 95% CI for any summary statistic. In general, the mean value can be replaced by the point estimate of a rate or a proportion and the same formula applies for computing 95% CIs, i.e.\n\\[ 95\\% \\text{ CI} = \\text{point estimate} \\pm 1.96 \\times \\text{SE}(\\text{point estimate)} \\]\nThe main difference in the methods used to calculate the 95% CI for different point estimates is the way the SE is calculated. The methods for calculating 95% CI around proportions and other ratio measures will be discussed in Module 6.\nThe use of 1.96 as a general critical value to compute the 95% CI is determined by sampling theory. For the confidence interval of the mean, the critical value (1.96) is based on normal distribution (true when the population SD is known). However, in practice, Stata and other statistical packages will provide slightly different confidence intervals because they use a critical value obtained from the t-distribution. The t-distribution approaches a normal distribution when the sample size approaches infinity, and is close to a normal distribution when the sample size is ≥30.The critical values obtained from the t-distribution are always larger than the corresponding critical value from the normal distribution. The difference gets smaller as the sample size becomes larger. For example, when the sample size n=10, the critical value from the t-distribution is 2.26 (rather than 1.96); when n= 30, the value is 2.05; when n=100, the value is 1.98; and when n=1000, the critical value is 1.96.\nThe critical value multiplied by SE (for normal distribution, 1.96 × SE) is called the maximum likely error for 95% confidence.\n\n2.5.1 Worked Example 3.1: 95% CI of a mean\nFor our sample of weights data with standard error of 0.92:\n\\[\n\\begin{aligned}\n\\ 95\\% \\text{ CI}(\\bar{x}) &=  \\bar{x} \\pm 1.96 \\times \\text{SE}(\\bar{x}) \\\\\n&= 70.0 \\pm 1.96 \\times 0.92 \\\\\n&= 68.2 \\text{ to } 71.8 \\text{kg}\n\\end{aligned}\n\\] We interpret this confidence interval as: we are 95% confident that the true mean of the population from which our sample was drawn lies between 68.2 kg and 71.8 kg.\nThis calculation takes into account both the sample mean of 70.0 kg and the sampling error that has arisen by chance due to the sample size of 30 people.\nFor a 95% CI to be reported around a mean value, the data values need to be approximately normally distributed, as discussed in Module 2.\n\n\n2.5.2 The t-distribution and when should I use it?\nThe population standard deviation (\\(\\sigma\\)) is required for calculation of the standard error. Usually, \\(\\sigma\\) is not known and the sample standard deviation (\\(s\\)) is used to estimate it. It is known, however, that the sample standard deviation of a normally distributed variable underestimates the true value of \\(\\sigma\\), particularly when the sample size is small.\nSomeone by the pseudonym of Student came up with the Student’s t distribution with (\\(n-1\\)) degrees of freedom to account for this underestimation. It looks very much like the standardised normal distribution, only that it has fatter tails (Figure 2.1). As the degrees of freedom increase (i.e. as \\(n\\) increases), the t-distribution gradually approaches the standard normal distribution. With a sufficiently large sample size, the Student’s t-distribution closely approximates the standardised normal distribution.\n\n\n\n\n\nFigure 2.1: The normal (Z) and the student’s t-distribution with 2, 5 and 30 degrees of freedom\n\n\n\n\nIf a variable \\(X\\) is normally distributed and the population standard deviation \\(\\sigma\\) is known, using the normal distribution is appropriate. However, if \\(\\sigma\\) is not known then one should use the student t-distribution with (\\(n – 1\\)) degrees of freedom.\n\n\n2.5.3 Worked Example 3.2\nThe publication of a study using a sample of 30 patients reported a sample mean of 70 kg and a sample standard deviation of 6 kg. Find the 95% confidence interval estimate for the mean weight from this sample.\nIn Stata we use the cii means command to compute the 95% confidence interval given the sample mean, sample standard deviation and the sample size (i.e. without using individual data from a dataset). This command uses the t-distribution, and the output is shown below:\n\nStata Output 3.2: 95%CI for a given sample mean, sample standard deviation and sample size\n\n    Variable |        Obs        Mean    Std. Err.       [95% Conf. Interval]\n-------------+---------------------------------------------------------------\n             |         30          70    1.095445        67.75956    72.24044\n\nWe are 95% confident that the true mean weight of the population from which the sample was drawn lies between 67.8 kg and 72.2 kg.\n\n\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "03.1-precision-stata.html#calculating-a-95-confidence-interval-of-a-mean",
    "href": "03.1-precision-stata.html#calculating-a-95-confidence-interval-of-a-mean",
    "title": "3 Stata notes",
    "section": "Calculating a 95% confidence interval of a mean",
    "text": "Calculating a 95% confidence interval of a mean\n\nIndividual data\nTo demonstrate the computation of the 95% confidence interval of a mean we have used data from Example_1.3.dta. To calculate the 95% confidence interval, go to Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Confidence intervals. In the ci dialog box, select weight as the Variable.\n\n\n\n\n\nFigure 1: Calculating a confidence interval from individual data\n\n\n\n\nClick OK or Submit to obtain Output 3.1.\n[Command: ci means weight]\n\n\nSummarised data\nFor Worked Example 3.2 where we are given the sample mean, sample standard deviation and sample size, we use the cii means command. To calculate the 95% CI, go to Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Normal mean CI calculator. In the cii dialog box, check that the Normal mean button is selected, and enter 30 as the Sample size, 70 as the Sample mean, 6 as the Sample standard deviation and check that 95 in entered as the Confidence level.\n\n\n\n\n\nFigure 2: Calculating a confidence interval from summarised data\n\n\n\n\nClick OK or Submit to obtain Output 3.2.\n[Command: cii means 30 70 6]"
  },
  {
    "objectID": "03.2-precision-R.html#calculating-a-95-confidence-interval-of-a-mean",
    "href": "03.2-precision-R.html#calculating-a-95-confidence-interval-of-a-mean",
    "title": "3 R notes",
    "section": "Calculating a 95% confidence interval of a mean",
    "text": "Calculating a 95% confidence interval of a mean\n\nIndividual data\nTo demonstrate the computation of the 95% confidence interval of a mean we have used data from Example_1.3.rds which contains the weights of 30 students:\n\nlibrary(jmv)\n\nstudents <- readRDS(\"data/examples/Example_1.3.rds\")\n\nsummary(students)\n\n     weight         gender  \n Min.   :60.00   Male  :16  \n 1st Qu.:67.50   Female:14  \n Median :70.00              \n Mean   :70.00              \n 3rd Qu.:74.38              \n Max.   :80.00              \n\n\nThe mean and its 95% confidence interval can be obtained many ways in R. We will use the t.test() function installed in R to calculate the confidence interval:\n\nt.test(students$weight)\n\n\n    One Sample t-test\n\ndata:  students$weight\nt = 76.029, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 68.11694 71.88306\nsample estimates:\nmean of x \n       70 \n\n\nThe output of the t.test() function gives us the sample mean (70.0 kg) as well as the 95% confidence interval around the mean: 68.1 to 71.9 kg.\nNote: the descriptives() function within the jmv package also calculates a 95% confidence interval around the mean. It is recommended not to use this function as it currently (as of June 2022) uses a z value to calculate the confidence interval, rather than a t value.\n\n\nSummarised data\nFor Worked Example 3.2 where we are given the sample mean, sample standard deviation and sample size. R does not have a built-in function to calculate a confidence interval from summarised data, but we can write our own.\nNote: writing your own functions is beyond the scope of this course. You should copy and paste the code provided to do this.\n\n### Copy this section\nci_mean <- function(n, mean, sd, width=0.95, digits=3){\n  lcl <- mean - qt(p=(1 - (1-width)/2), df=n-1) * sd/sqrt(n)\n  ucl <- mean + qt(p=(1 - (1-width)/2), df=n-1) * sd/sqrt(n)\n  \n  print(paste0(width*100, \"%\", \" CI: \", format(round(lcl, digits=digits), nsmall = digits),\n               \" to \", format(round(ucl, digits=digits), nsmall = digits) ))\n\n}\n### End of copy\n\nci_mean(n=30, mean=70, sd=6, width=0.95)\n\n[1] \"95% CI: 67.760 to 72.240\"\n\nci_mean(n=30, mean=70, sd=6, width=0.99)\n\n[1] \"99% CI: 66.981 to 73.019\""
  },
  {
    "objectID": "03.3-precision-activities.html",
    "href": "03.3-precision-activities.html",
    "title": "3 Learning Activities",
    "section": "",
    "text": "Activity 3.1\nAn investigator wishes to study people living with agoraphobia (fear of open spaces). The investigator places an advertisement in a newspaper asking for volunteer participants. A total of 100 replies are received of which the investigator randomly selects 30. However, only 15 volunteers turn up for their interview.\n\nWhich of the following statements is true?\n\n\nThe final 15 participants are likely to be a representative sample of the population available to the investigator\nThe final 15 participants are likely to be a representative sample of the population of people with agoraphobia\nThe randomly selected 30 participants are likely to be a representative sample of people with agoraphobia who replied to the newspaper advertisement\nNone of the above\n\n\nThe basic problem confronted by the investigator is that:\n\n\nThe accessible population might be different from the target population\nThe sample has been chosen using an unethical method\nThe sample size was too small\nIt is difficult to obtain a sample of people with agoraphobia in a scientific way\n\n\n\nActivity 3.2\nA dental epidemiologist wishes to estimate the mean weekly consumption of sweets among children of a given age in her area. After devising a method which enables her to determine the weekly consumption of sweets by a child, she conducted a pilot survey and found that the standard deviation of sweet consumption by the children per week is 85 gm (assuming this is the population standard deviation, \\(\\sigma\\)). She considers taking a random sample for the main survey of:\n\n25 children, or\n100 children, or\n625 children or\n3,000 children.\n\n\nEstimate the standard error and maximum likely (95% confidence) error of the sample mean for each of these four sample sizes.\nWhat happens to the standard error as the sample size increases? What can you say about the precision of the sample mean as the sample size increases?\n\n\n\nActivity 3.3\nThe dataset for this activity is the same as the one used in Activity 1.4 in Module 1. The file is Activity1.4.dta on Moodle.\n\nPlot a histogram of diastolic BP and describe the distribution.\nUse Stata to obtain an estimate of the mean, standard error of the mean and the 95% confidence interval for the mean diastolic blood pressure.\nInterpret the 95% confidence interval for the mean diastolic blood pressure.\n\n\n\nActivity 3.4\nSuppose that a random sample of 81 newborn babies delivered in a hospital located in a poor neighbourhood during the last year had a mean birth weight of 2.7 kg and a standard deviation of 0.9 kg. Calculate the 95% confidence interval for the unknown population mean. Interpret the 95% confidence interval."
  },
  {
    "objectID": "04-hypothesis-testing.html#learning-objectives",
    "href": "04-hypothesis-testing.html#learning-objectives",
    "title": "3  Hypothesis testing",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nFormulate a research question as a hypothesis;\nUnderstand the concepts of a hypothesis test;\nConsider the difference between statistical significance and clinical importance;\nUse 95% confidence intervals to conduct an informal hypothesis test;\nPerform and interpret a one-sample t-test;\nExplain the concept of one and two tailed statistical tests."
  },
  {
    "objectID": "04-hypothesis-testing.html#readings",
    "href": "04-hypothesis-testing.html#readings",
    "title": "3  Hypothesis testing",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 8. [UNSW Library Link]\nBland (2015); Sections 9.1 to 9.7; Sections 10.1 and 10.2. [UNSW Library Link]\nAcock (2010); Section 7.4."
  },
  {
    "objectID": "04-hypothesis-testing.html#introduction",
    "href": "04-hypothesis-testing.html#introduction",
    "title": "3  Hypothesis testing",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn earlier modules, we examined sampling and how summary statistics can be used to make inferences about a population from which a sample is drawn. In this module, we introduce hypothesis testing as the basis of the statistical tests that are important for reporting results from research and surveillance studies, and that you will be learning in the remainder of this course.\nWe use hypothesis testing to answer questions such as whether two groups have different health outcomes or whether there is an association between a treatment and a health outcome. For example, we may want to know:\n\nwhether a safety program has been effective in reducing injuries in a factory, i.e. whether the frequency of injuries in the group who attended a safety program is lower than in the group who did not receive the safety program;\nwhether a new drug is more effective in reducing blood pressure than a conventional drug, i.e. whether the mean blood pressure in the group receiving the new drug is lower than the mean blood pressure in the group receiving the conventional medication;\nwhether an environmental exposure increases the risk of a disease, i.e. whether the frequency of disease is higher in the group who have been exposed to an environmental factor than in the non-exposed group.\n\nWe may also want to know something about a single group. For example, whether the mean blood pressure of a sample is the same as the general population.\nThese questions can be answered by setting up a null hypothesis and an alternative hypothesis, and performing a hypothesis test (also known as a significance test)."
  },
  {
    "objectID": "04-hypothesis-testing.html#hypothesis-testing",
    "href": "04-hypothesis-testing.html#hypothesis-testing",
    "title": "3  Hypothesis testing",
    "section": "3.2 Hypothesis testing",
    "text": "3.2 Hypothesis testing\nHypothesis testing is a statistical technique that is used to quantify the evidence against a null hypothesis. A null hypothesis (H0) is a statement that there is no difference in a summary statistic between groups. For example, a null hypothesis may be stated as follows:\nH0 = there is no difference in mean systolic blood pressure between a group taking a conventional drug and a group taking a newly developed drug\nWe also have an alternative hypothesis that is opposite or contrasting to the null hypothesis. In our example above, the alternative hypothesis above we be that there is a difference between groups. The alternative hypothesis is usually of most interest to the researcher but in practice, formal statistical tests are used to test the null hypothesis (not the alternative hypothesis). The hypotheses are always in reference to the population from which the sample is drawn, not the sample itself.\nAfter setting up our null and alternative hypotheses, we use the data to generate a test statistic. The particular test statistic differs depending on the type of data being analyses (e.g. continuous or categorical), the study design (e.g. paired or independent) and the question being asked.\nThe test statistic is then compared to a known distribution to calculate the probability of observing a test statistic which is as large or larger than the observed test statistic, if the null hypothesis was true. The probability is known as the P-value. Informally, the P-value can be interpreted as the probability of observing data like ours, or more extreme, if the null hypothesis was true.\nIf the P-value is small, it is unlikely that we would observe data like ours or more extreme if the null hypothesis was true. In other words, our data are not consistent with the null hypothesis, and we conclude that we have evidence against the null hypothesis. If the P-value is not small, the probability of observing data like ours or more extreme is not unlikely. We therefore have little or no evidence against the null hypothesis. In hypothesis testing, the null hypothesis cannot be proven or accepted; we can only find evidence to refute the null hypothesis.\nTo summarise:\n\na small P-value gives us evidence against the null hypothesis;\na P-value that is not small provides little or no evidence against null hypothesis;\nthe smaller the P-value, the stronger the evidence against the null hypothesis.\n\nHistorically, a value of 0.05 has been used as a cut-point for finding evidence against the null hypothesis. A P-value less than 0.05 would be interpreted as “statistically significant”, and would allow us to “reject the null hypothesis”. A P-value greater than 0.05 would be interpreted as “not significant”, and we would “fail to reject the null hypothesis”. This arbitrary dichotomy is overly simplistic, and a more nuanced view is now recommended. Possible interpretations for P-values are given in Table 3.1.\n\n\n\n\nTable 3.1:  Interpretation of P-values \n\nSize of P valueStrength of evidence\n\n<0.001Very strong evidence\n\n0.001 to <0.01Strong evidence\n\n0.01 to <0.05Evidence\n\n0.05 to <0.1Weak evidence\n\n≥0.1Little or no evidence\n\n\n\n\n\nP-values are usually generated using statistical software although other methods such as statistical tables or Excel functions can be used to generate test statistics and determine the P-value. In traditional statistics, the probability level was described as a lower-case p but in many journals today, probability is commonly described by upper case P. Both have the same meaning."
  },
  {
    "objectID": "04-hypothesis-testing.html#effect-size",
    "href": "04-hypothesis-testing.html#effect-size",
    "title": "3  Hypothesis testing",
    "section": "3.3 Effect size",
    "text": "3.3 Effect size\nIn hypothesis testing, P-values convey only part of the information about the hypothesis and need to be accompanied by an estimation of the effect size, that is, a description of the magnitude of the difference between the study groups. The effect size is a summary statistic that conveys the size of the difference between two groups. For continuous variables, it is usually calculated as the difference between two mean values.\nIf the variable is binary, the effect size can be expressed as the absolute difference between two proportions (attributable risk), or as an odds ratio or relative risk.\nReporting the effect size enables clinicians and other researchers to judge whether a statistically significant result is also a clinically important finding. The size of the difference or the risk statistic provides information to help health professionals decide whether the observed effect is large and important enough to warrant a change in current health care practice, is equivocal and suggests a need for further research, or is small and clinically unimportant."
  },
  {
    "objectID": "04-hypothesis-testing.html#statistical-significance-and-clinical-importance",
    "href": "04-hypothesis-testing.html#statistical-significance-and-clinical-importance",
    "title": "3  Hypothesis testing",
    "section": "3.4 Statistical significance and clinical importance",
    "text": "3.4 Statistical significance and clinical importance\nWhen applying statistical methods in health and medical research, we need to make an informed decision about whether the effect size that led to a statistically significant finding is also clinically important (see Figure 3.1)). The decision about whether a statistically significant result is also clinically important depends on expert knowledge and is best made by practitioners with experience in the field.\n\n\n\nFigure 3.1: Statistical significance vs. clinical importance (Source: Armitage P, Berry G, Matthews JNS. (2001)\n\n\nIt is possible when conducting significance tests, particularly in very large studies, that a small effect is found to be statistically significant. For example, say in a large study of over 1000 patients, a new medication was found to lower blood pressure on average by 1 mmHg more than a currently accepted drug and this was statistically significant (P < 0.05). However, such a small decrease in blood pressure would probably not be considered clinically important. The cost and side effects of prescribing the new medication would need to be weighed against the very small average benefit that would be expected. In this case, although the null hypothesis would be rejected (i.e. the result is statistically significant), the result would not be clinically important. This is the situation described in scenario (c) of Figure Figure 3.1.\nConversely, it is possible to obtain a large, clinically important difference between groups, but a P value that does not demonstrate a statistically significant difference.\nFor example, consider a study to measure the rate of hospital admissions. We may find that 80% of children who present to the Emergency Department are admitted before an intervention is introduced compared to only 65% of children after the intervention. However, the P value may be calculated as 0.11 and is non-significant. This is because only 60 children were surveyed in each period. Here, the reduction in the admission rate by 15% represents a clinically important difference, but not statistically significant. This situation is represented in scenario (d) of Figure 3.1.\nThe important thing to remember is that statistical significance does not always correspond to clinical importance. A statistically significant result may be clinically unimportant, and a statistically non-significant results may be clinically important."
  },
  {
    "objectID": "04-hypothesis-testing.html#errors-in-significance-testing",
    "href": "04-hypothesis-testing.html#errors-in-significance-testing",
    "title": "3  Hypothesis testing",
    "section": "3.5 Errors in significance testing",
    "text": "3.5 Errors in significance testing\nThere are two conclusions we can draw when conducting a hypothesis test: if the P-value is small, there is strong evidence against the null hypothesis and we reject the null hypothesis. If the P-value is not small, there is little evidence against the null hypothesis and we fail to reject the null hypothesis. As discussed above, the “small” cut-point for the P-value is often taken as 0.05. We refer to this value as \\(\\alpha\\) (alpha).\nWe can conduct a thought experiment and compare our hypothesis test conclusion to reality. In reality, either the null hypothesis is true, or it is false. Of course, if we knew what reality was, we would not need to conduct a hypothesis test. But we can compare our possible hypothesis test conclusions to the true (unobserved) reality.\nIf the null hypothesis was true in reality, our hypothesis test can fail to reject the null hypothesis – this would be a correct conclusion. However, the hypothesis test could lead us to rejecting the null hypothesis – this would be an incorrect conclusion. We call this scenario a Type I error, and it has a probability of \\(\\alpha\\).\nThe other situation is where, in reality, the null hypothesis is false. A correct conclusion would be where our hypothesis test rejects the null hypothesis. However, if our hypothesis test fails to reject the null hypothesis, we have made a Type II error. The probability of making a Type II error is denoted \\(\\beta\\) (beta). We will see in Module 10 that \\(\\beta\\) is determined by the size of the study.\nThe error in falsely rejecting the null hypothesis when it is true (type I error), or in falsely accepting the null hypothesis when it is not true (type II error) is summarised in Table 3.2. We will return to these concepts in Module 10, when discussing how to determine the appropriate sample size of a study.\n\n\n\n\nTable 3.2:  Comparison of study result with the truth \n\nStudy resultTruth\n\n EffectNo effect\n\nEvidenceCorrect conclusionɑ\n\nNo evidenceβCorrect conclusion"
  },
  {
    "objectID": "04-hypothesis-testing.html#confidence-intervals-in-hypothesis-testing",
    "href": "04-hypothesis-testing.html#confidence-intervals-in-hypothesis-testing",
    "title": "3  Hypothesis testing",
    "section": "3.6 Confidence intervals in hypothesis testing",
    "text": "3.6 Confidence intervals in hypothesis testing\nIn Module 3, the 95% confidence interval around a mean value was calculated to show the precision of the summary statistic. The 95% confidence intervals around other summary statistics can also be calculated.\nFor example, if we were comparing the means of two groups, we would want to test the null hypothesis that the difference in means is zero, that there is no true difference between the groups.\nFrom the data from the two groups, we could estimate the difference in means, the standard error of the difference in means and the 95% confidence interval around the difference. To estimate the 95% confidence interval, we use the formula given in Module 3, that is:\n\\[ 95\\% \\text{ CI} = \\text{Difference in means} \\pm 1.96 \\times \\text{SE}(\\text{Difference in means)} \\]\nIt is important to remember that the 95% CI is estimated from the standard error, and that the standard error has a direct relationship to the sample size. For small sample sizes, the standard error is large and the 95% CI becomes wider. Conversely, the larger the sample size, the smaller the standard error and the narrower the 95% CI becomes indicating a more precise estimate of the mean difference.\nThe 95% CI tells us the region in which we are 95% confident that the true difference between the groups in the population lies. If this region contains the null value of no difference, we can say that we are 95% confident that there is no true difference between the groups and therefore we would not reject the null hypothesis. This is shown in the top two estimates in Figure 3.2. If the zero value lies outside the 95% confidence interval, we can conclude that there is evidence of a difference between the groups because we are 95% confident that the difference does not encompass a zero value (as shown in the lower two estimates in Figure 3.2.\n\n\n\n\n\nFigure 3.2: Using confidence intervals as informal hypothesis tests\n\n\n\n\nFor relative risk and odds ratio measures, when the 95% CI includes the value of 1 it indicates that we can be 95% confident that the true RR or OR of the association between the study factor and outcome factor includes 1.0 in the source population. This indicates little evidence of an association between the study factor and the outcome factor, e.g. if the results of a study were reported as RR = 1.10 (95% CI 0.95 to 1.25). The P-value can be calculated to assess this (discussed in Module 7).\n\n\n\n\nTable 3.3:  Values indicating no effect \n\nType of outcomeMeasure of effectNull value\n\n(indicating no difference)\n\nContinuousDifference in means0\n\nBinaryDifference in proportions0\n\nRelative risk1\n\nOdds ratio1"
  },
  {
    "objectID": "04-hypothesis-testing.html#one-sample-t-test",
    "href": "04-hypothesis-testing.html#one-sample-t-test",
    "title": "3  Hypothesis testing",
    "section": "3.7 One-sample t-test",
    "text": "3.7 One-sample t-test\nA one-sample t-test tests whether a sample mean is different to a hypothesised value. The t-distribution and its relation to normal distribution has been discussed in detailed in Module 3.\nIn a one-sample t-test, a t-value is computed as the sample mean divided by the standard error of the mean. The significance of the t-value is then computed using software, or can be obtained from a statistical table.\nThe principles of this test can be used for applications such as testing whether the mean of a sample is different from a known population mean, for example testing whether the IQ of a group of children is different from the population mean of 100 IQ points or testing whether the number of average hours worked in an adult sample is different from the population mean of 38 hours.\n\n3.7.1 Worked Example\nThe mean diastolic blood pressure (BP) of the general US population is known to be 71 mm Hg. The diastolic blood pressure of 733 female Pima indigenous Americans was measured and a histogram showed that the data were approximately normally distributed. The mean diastolic blood pressure in the sample was 72.4 mm Hg with a standard deviation of 12.38 mm Hg.\nWe can use Stata or R to conduct a one sample t-test using the data available on Moodle (Example_4.1.csv). The results from this test are summarised below.\n\n\nTable 3.4: Summary of blood pressure from female Pima indigenous Americans\n\n\n\n\n\n\n\n\n\nn\nMean\nStandard deviation\nStandard error\n95% confidence interval of the mean\n\n\n\n\n733\n72.4\n12.38\n0.46\n71.5 to 73.3\n\n\n\n\nThe test statistic for the one-sample t-test is calculated as t732=3.07, with a P-value of 0.002.\nThe mean diastolic blood pressure of females from Pima is estimated as 72.4 mmHg (95% CI: 71.5 to 73.3 mmHg), which is higher than that of the general US population. Note that this interval does not contain the mean of the general US population (71 mm Hg), providing some indication that the mean diastolic blood pressure of female Pima people is higher than that of the general US population.\nThe result from the formal hypothesis test gives strong evidence that the mean diastolic BP of the female Pima people is higher than that of the general US population (t732=3.07, P=0.002)."
  },
  {
    "objectID": "04-hypothesis-testing.html#one-and-two-tailed-tests",
    "href": "04-hypothesis-testing.html#one-and-two-tailed-tests",
    "title": "3  Hypothesis testing",
    "section": "3.8 One and two tailed tests",
    "text": "3.8 One and two tailed tests\nMost statistical tests are two tailed tests, that is, we conduct a test that allows for the summary statistic in the group of interest to be either higher or lower than in the comparison group. For a t-test, this requires that we obtain a two-tailed P value which gives us the probability of the t-value being in either one of the two tails of the t-distribution as shown in Figure 3.3. The shaded regions show the t values that indicate a P value less than 0.05.\n\n\n\n\n\nFigure 3.3: P-value for a 2-tailed test\n\n\n\n\nOccasionally, one tailed tests are conducted in which the summary statistic in the group of interest can only be higher or lower than the comparison group, i.e. a difference is specified to occur in one direction only. This makes it easier to reject the null hypothesis because the consequence is that the P value is essentially halved. The P value for a one tailed test would be 0.025 i.e. the shaded region for a one-tailed test would be doubled on one side of the distribution and eliminated from the other side of the distribution as shown in Figure 3.4.\n\n\n\n\n\nFigure 3.4: P-value for 1-tailed tests\n\n\n\n\nIf a one tailed P value is reported, and is considered to be an invalid decision, it is usually easily converted to a two tailed value by doubling its numeric value. For example, for the same test statistic and sample size:\n\nOne tailed P value = 0.042 i.e. statistically significant\nTwo tailed P value = 0.084 i.e. non-significant\n\nObviously, the choice of whether to use a one or two tailed test is not as important when the P value is highly significant or clearly non-significant but can make a difference to the conclusions when the P value is on the margins of significance.\nIn most health research, the use of a one tailed test is rarely justified because it is unusual to be certain of the direction of effect prior to the research study being undertaken. It has been suggested that if the researchers were sure enough to consider using a one-tailed test, the research study would not be needed.\nIn most studies, two tailed tests of significance are used to allow for the possibility that the effect size could occur in either direction. In clinical trials, this would mean allowing for a result that can indicate a benefit or an adverse effect in response to a new treatment. In epidemiological studies, two tailed tests are used to allow for the fact that exposure to a factor of interest may be adverse or may be beneficial. This conservative approach is usually adopted to prevent missing important effects that occur in the opposite direction to that expected by the researchers."
  },
  {
    "objectID": "04-hypothesis-testing.html#a-note-on-p-values-displayed-by-software",
    "href": "04-hypothesis-testing.html#a-note-on-p-values-displayed-by-software",
    "title": "3  Hypothesis testing",
    "section": "3.9 A note on P-values displayed by software",
    "text": "3.9 A note on P-values displayed by software\nYou will often see P-values generated by statistical software (including Stata) presented as 0.000 or 0.0000. As P-values can never be equal to zero, any P-value displayed in this way should be converted to <0.001 or <0.0001 respectively (i.e. replace the last 0 with a 1, and use the less-than symbol).\nR can display P-values in a very cryptic way: \\(\\text{6.478546e-05}\\) for example. This is translated as:\n\\[\n\\begin{aligned}\n\\text{6.478546e-05} &= 6.478546 \\times 10^{-5} \\\\\n  &= 6.478546 \\times 0.00001 \\\\\n  &= 0.00006478546\n\\end{aligned}\n\\]\nAs for the Stata output, such a P-value would be better presented as P<0.0001."
  },
  {
    "objectID": "04-hypothesis-testing.html#decision-tree",
    "href": "04-hypothesis-testing.html#decision-tree",
    "title": "3  Hypothesis testing",
    "section": "3.10 Decision Tree",
    "text": "3.10 Decision Tree\nIn the following modules in this course, several formal statistical tests will be described to analyse different types of data sets that have been collected to test set null hypotheses. It is important that the correct statistical test is selected to generate P-values and estimate effect size. If an incorrect statistical test is used, the assumptions of the test may be violated, the effect size may be biased and the P value generated may be incorrect.\nSelecting the correct test to use in each situation depends on the study design and the nature of the variables collected. Figure 1 in the Appendix shows a decision tree which enables you to decide the type of test to select based on the nature of the data.\n\n\n\n\nAcock, Alan C. 2010. A Gentle Introduction to Stata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "04.1-hypothesis-testing-stata.html#one-sample-t-test",
    "href": "04.1-hypothesis-testing-stata.html#one-sample-t-test",
    "title": "4 Stata notes",
    "section": "One sample t-test",
    "text": "One sample t-test\nWe will use data from Example_4.1.csv to demonstrate how a one-sample t-test is conducted in Stata. To perform the test, go to Statistics > Summaries, tables, and tests > Classical tests of hypotheses > t test (mean-comparison test).\nEnsure that the One-sample option is selected, then choose dbp as the Variable name from the drop-down list. Enter the Hypothesised mean value (71 in this example) as shown below.\n\n\n\n\n\nClick OK or Submit to obtain the output below.\n[Command: ttest dbp == 71]\n\n. ttest dbp == 71\n\nOne-sample t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]\n---------+--------------------------------------------------------------------\n     dbp |     733    72.40518    .4573454    12.38216    71.50732    73.30305\n------------------------------------------------------------------------------\n    mean = mean(dbp)                                              t =   3.0725\nHo: mean = 71                                    degrees of freedom =      732\n\n    Ha: mean < 71               Ha: mean != 71                 Ha: mean > 71\n Pr(T < t) = 0.9989         Pr(|T| > |t|) = 0.0022          Pr(T > t) = 0.0011\n\nThe table gives the sample mean and standard deviation, as well as the standard error and 95% confidence interval of the mean. The test statistics are given under the table: t and the degrees of freedom as well as the P values from two-sided (Ha: mean != 71) and one-sided (Ha: mean <71 and Ha: mean > 71) tests. Refer to the previous sections of this module on the appropriate test to use."
  },
  {
    "objectID": "04.2-hypothesis-testing-R.html#one-sample-t-test",
    "href": "04.2-hypothesis-testing-R.html#one-sample-t-test",
    "title": "4 R notes",
    "section": "One sample t-test",
    "text": "One sample t-test\nWe will use data from Example_4.1.rds to demonstrate how a one-sample t-test is conducted in R.\n\nlibrary(jmv)\n\nbloodpressure <- read.csv(\"data/examples/Example_4.1.csv\")\n\ndescriptives(bloodpressure)\n\n\n DESCRIPTIVES\n\n Descriptives                       \n ────────────────────────────────── \n                         dbp        \n ────────────────────────────────── \n   N                          733   \n   Missing                     35   \n   Mean                  72.40518   \n   Median                      72   \n   Standard deviation    12.38216   \n   Minimum                     24   \n   Maximum                    122   \n ────────────────────────────────── \n\n\nTo test whether the mean diastolic blood pressure of the population from which the sample was drawn is equal to 71, we can use the t.test command:\n\nt.test(bloodpressure$dbp, mu=71)\n\n\n    One Sample t-test\n\ndata:  bloodpressure$dbp\nt = 3.0725, df = 732, p-value = 0.002202\nalternative hypothesis: true mean is not equal to 71\n95 percent confidence interval:\n 71.50732 73.30305\nsample estimates:\nmean of x \n 72.40518 \n\n\nThe output gives a test statistic, degrees of freedom and a P values from the two-sided test. The mean of the sample is provided, as well as the 95% confidence interval."
  },
  {
    "objectID": "04.3-hypothesis-testing-activities.html",
    "href": "04.3-hypothesis-testing-activities.html",
    "title": "4 Learning Activities",
    "section": "",
    "text": "Activity 4.1\nIn each of the following situations, what decision should be made about the null hypothesis if the researcher indicates that:\n\nP < 0.01\nP > 0.05\n“ns”\n“significant differences exist”\n\n\n\nActivity 4.2\nFor the following hypothetical situations, formulate the null hypothesis and alternative hypothesis and write a conclusion about the study results:\n\nA study was conducted to investigate whether the mean systolic blood pressure of males aged 40 to 60 years was different to the mean systolic blood pressure of females aged 40 to 60 years. The result of the study was that the mean systolic blood pressure was higher in males by 5.1 mmHg (95% CI 2.4 to 7.6; P = 0.008).\nA case-control study was conducted to investigate the association between obesity and breast cancer. The researchers found an OR of 3.21 (95% CI 1.15 to 8.47; P = 0.03).\nA cohort study investigated the relationship between eating a healthy diet and the incidence of influenza infection among adults aged 20 to 60 years. The results were RR = 0.88 (95% CI 0.65 to 1.50; P = 0.2).\n\n\n\nActivity 4.3\nA pilot study was conducted to compare the mean daily energy intake of women aged 25 to 30 years with the recommended intake of 7750 kJ/day. In this study, the average daily energy intake over 10 days was recorded for 12 healthy women of that age group. The data are in the the Excel file Activity_4.3.xls. Import the file into Stata for this activity.\n\nState the research question\nFormulate the null hypothesis\nFormulate the alternative hypothesis\nAnalyse the data in Stata and report your conclusions\n\n\n\nActivity 4.4\nWhich procedure gives the researcher the better chance of rejecting a null hypothesis?\n\ncomparing the data-based p-value with the level of significance at 5%\ncomparing the 95% CI with a nominated value\nneither procedure\n\n\n\nActivity 4.5\nSetting the significance level at P < 0.10 instead of the more usual P < 0.05 increases the likelihood of:\n\na Type I error\na Type II error\nrejecting the null hypothesis\nNot rejecting the null hypothesis\n\n\n\nActivity 4.6\nFor a fixed sample size setting the significance level at a very extreme cutoff such as P < 0.001 increases the chances of: a) obtaining a significant result b) rejecting the null hypothesis c) a Type I error d) a Type II error"
  },
  {
    "objectID": "05-comparing-two-means.html#learning-objectives",
    "href": "05-comparing-two-means.html#learning-objectives",
    "title": "4  Comparing the means of two groups",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nDecide whether to use an independent samples t-test or a paired t-test to compare two the means of two groups;\nConduct and interpret the results from an independent samples t-test;\nDescribe the assumptions of an independent samples t-test;\nConduct and interpret the results from a paired t-test;\nDescribe the assumptions of a paired t-test;\nConduct an independent samples t-test and a paired t-test in Stata;\nReport results and provide a concise summary of the findings of statistical analyses."
  },
  {
    "objectID": "05-comparing-two-means.html#readings",
    "href": "05-comparing-two-means.html#readings",
    "title": "4  Comparing the means of two groups",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Sections 7.1 to 7.5. [UNSW Library Link]\nBland (2015); Section 10.3. [UNSW Library Link]\nAcock (2010); Section 7.7, 7.8."
  },
  {
    "objectID": "05-comparing-two-means.html#introduction",
    "href": "05-comparing-two-means.html#introduction",
    "title": "4  Comparing the means of two groups",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn Module 4, a one-sample t-test was used for comparing a single mean to a hypothesised value. In health research, we often want to compare the means between two groups. For example, in an observational study, we may want to compare cholesterol levels in people who are normal weight to the levels in people who are overweight. In a clinical trial, we may want to compare cholesterol levels in people who have been randomised to a dietary modification or to usual care. In this module, we show how to compare the means of two groups where the analysis variable is normally distributed.\nFrom the decision tree presented in the Appendix, we can see that if we have a continuous outcome measure and two categorical groups that are not related, i.e. a binary exposure measurement, the test for such data is an independent samples t-test. The test is also sometimes called a 2-sample t-test.\nHowever, in research, data are often ‘paired’ or ‘matched’, that is the two data points are related to one another. This occurs when measurements are taken:\n\nFrom each participant on two occasions, e.g. at baseline and follow-up in an experimental study or in a longitudinal cohort study;\nFrom related people, e.g. a mother and daughter or a child and their sibling;\nFrom related sites in the same person, e.g. from both limbs, eyes or kidneys;\nFrom matched participants e.g. in a matched case-control study;\nIn cross-over clinical trials where the patient receives both drugs, often in random order.\n\nAn independent samples t-test cannot be used for analysing paired or matched data because the assumption that the two groups are independent is violated. Treating paired or matched measurements as independent samples would artificially inflate the sample size and lead to inaccurate P values. When the data are related in a paired or matched way and the outcome is continuous, a paired t-test is the appropriate statistic to use if the data are normally distributed."
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test",
    "href": "05-comparing-two-means.html#independent-samples-t-test",
    "title": "4  Comparing the means of two groups",
    "section": "4.2 Independent samples t-test",
    "text": "4.2 Independent samples t-test\nAn independent samples t-test is a parametric test that is used to assess whether the mean values of two groups are different from one another. Thus, the test is used to assess whether two mean values are similar enough to have come from the same population or whether the difference between them is so large that the two groups can be considered to have come from separate populations with different characteristics.\nThe null hypothesis is that the mean values of the two groups are not different, that is:\nH0: (Mean2 - Mean1) = 0\nRejecting the null hypothesis using an independent samples t-test indicates that the difference between the means of the two groups is large in relation to the variability in the samples and is unlikely to be due to chance or to sampling variation.\n\n4.2.1 Assumptions for an independent samples t-test\nThe assumptions that must be met before an independent samples t-test can be used are:\n\nThe two groups are independent\nThe measurements are independent\nThe outcome variable must be continuous and must be normally distributed in each group\nThe variance in the two groups is similar (homogenous)\n\nThe first two assumptions are determined by the study design. The two samples must be independent, i.e. if a person is in one group then they cannot be included in the other group, and the measurements within a sample must be independent, i.e. each person must be included in their group once only.\nThe third assumption of normality is important although t-tests are robust to some degree of non-normality as long as there are no influential outliers and, more importantly, if the sample size is large. We examined how to assess normality in Module 2. If the data are not normally distributed, it may be possible to transform them using a mathematical function such as a logarithmic transformation. If not, then we may need to use non-parametric tests. This is examined in Module 9.\nThe final assumption is homogeneity of variance between the groups. This can be verified by checking that the standard deviation (square root of the variance) of each group is similar. If the variances are different, then Welch’s t-test, an alternative version of the t-test can be used.\n\n\n4.2.2 Worked Example\nIn an observational study of a random sample of 100 full term babies from the community, birth weight and gender were measured. There were 44 male babies and 56 female babies in the sample. The research question asked whether there was a difference in birth weights between boys and girls. The two groups are independent of each other and therefore an independent samples t-test can be used to test the null hypothesis that there is no difference in weight between the genders.\nSome preliminary descriptive statistics of the distribution of the variable of interest in each group should always be obtained before a t-test is undertaken to ensure that the assumptions are met. Box plots and histograms are ideal for this. Histrograms and box plots of the data obtained in Stata using Graphics > Box plot is shown in Figure Figure 4.1. The dataset Example_5.1.dta is available on Moodle.\n\n\n\n\n\nFigure 4.1: Histograms and box plots of birth weight by gender\n\n\n\n\nThe plots show that the data are approximately normally distributed: the histograms are relatively bell shaped and symmetric, and the boxes are fairly symmetrical, there are no outliers as indicated by dots, and the spread is similar in both groups as the similar length of the whiskers suggesting that the variance is homogenous.\nWe can also describe the data using summary statistics:\n\n\n\n\nTable 4.1:  Summary of birthweight by gender \n\nCharacteristic\nFemale\nMale\n\n\nBirthweight\n\nN5644\n\nMean (SD)3.59 (0.36)3.42 (0.35)\n\nMedian (IQR)3.53 (3.33, 3.87)3.43 (3.16, 3.63)\n\nRange2.95, 4.252.75, 4.10\n\n\n\n\n\nThe table shows that girls have a mean weight of 3.59 kg (SD 0.36) and boys have a mean weight of 3.42 kg (SD 0.35) with females being heavier than males. The variabilities of birth weight, as indicated by the standard deviations, are similar.\n\n\n4.2.3 Conducting and interpreting an independent samples t-test\nAn independent samples t-test provides us with a t statistic from which we can compute a P value. The computation of the t statistic is as follows:\n\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{SE({\\overline{x}}_{1} - {\\overline{x}}_{2})}\\]\nwith n1 + n2 – 2 degrees of freedom.\nLooking at the formula for the t-statistic, we can see that the \\(t\\) is an estimate of how different the mean values are compared to their variability. So \\(t\\) will become larger as the difference in means increases with respect to the variability.\nStatistical software will calculate both the t and P values. If the t-value is large, the P value will be small, providing evidence against the null hypothesis of no difference between the groups.\nTable 4.2 summarises the results of an independent samples t-test using Example_5.1.dta. The process of conducting the t-test is summarised for Stata and R in the following sections.\nPresent SD or SE??? Inconsistent between the previous two tables\n\n\n\n\nTable 4.2:  Birthweight (kg) by sex \n\nSexnMean (SE)95% Confidence Interval\n\nFemale563.59 (0.049)3.49 to 3.68\n\nMale443.42 (0.053)3.31 to 3.53\n\nDifference0.17 (0.072)0.02 to 0.31\n\n\n\n\n\nHere we see that girls are heavier than boys, and the mean difference in weights between the genders is 0.17 kg (95% CI 0.02, 0.31). We are 95% confident that the true mean difference lies between 0.02 and 0.31. Note that this interval does not contain the null value of 0.\nHere we are testing the null hypothesis of no difference in mean birthweights between females and males: a two-sided test. The t-value is calculated as 2.297 with 98 degrees of freedom, and yields a two-sided P value of 0.024, providing evidence of a difference in mean birthweight between sex."
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-tests",
    "href": "05-comparing-two-means.html#paired-t-tests",
    "title": "4  Comparing the means of two groups",
    "section": "4.3 Paired t-tests",
    "text": "4.3 Paired t-tests\nIf the outcome of interest is the difference in the continuously outcome measurement between each pair of observations, a paired t-test is used. In effect, a paired t-test is used to assess whether the mean of the differences between the two related measurements is significantly different from zero. In this sense, a paired t-test is very closely aligned with a one sample t-test.\nWhen using a paired t-test, the variation between the pairs of measurements is the most important statistic. The variation between the participants is of little interest.\nFor related measurements, the data for each pair of values must be entered on the same row of the spreadsheet. Thus, the number of rows in the data sheet is the number of pairs of observations. Thus, the effective sample size is the total number of pairs and not the total number of measurements.\n\n4.3.1 Assumptions for a paired t-test\nThe assumptions for a paired t-test are:\n\nthe outcome variable is continuous\nthe differences between the pair of the measurements are normally distributed\n\nFor a paired samples t-test, it is important to test whether the differences between the two measurements are normally distributed. If the assumptions for a paired t-test cannot be met, a non-parametric equivalent is a more appropriate test to use (Module 9).\n\n\n4.3.2 Computing a paired t-test\nThe null hypothesis for using a paired t-test is as follows:\nH0: Mean (Measurement1 – Measurement2) = 0\nTo compute a t-value, the size of the mean difference between the two measurements is compared to the standard error of the paired differences, i.e.\n\\[t = \\frac{\\overline{d}}{SE(\\overline{d})}\\]\nwith n–1 degrees of freedom, where n is the number of pairs.\nBecause the standard error becomes smaller as the sample size becomes larger, the t-value increases as the sample size increases for the same mean difference.\n\n\n4.3.3 Worked Example 5.2\nA total of 107 people were recruited into a study to assess whether ankle blood pressure measured in two different sites would be the same. For each person, systolic blood pressure (SBP) was measured in two sites: dorsalis pedis and tibialis posterior.\nThe dataset Example_5.2.dta is available on Moodle. First, we need to compute the pairwise difference between SBP measured in the two sites in Stata using the generate command. This is shown in the Stata manual at the end of this module (Checking the assumptions for a Paired t-test). The distribution of the difference between SBP measured in dorsalis pedis and tibialis posterior is shown in Figure 4.2. The differences approximate a normal distribution and therefore a paired t-test can be used.\n\n\n\n\n\nFigure 4.2: Distribution of differences in ankle SBP between two sites of 107 participants\n\n\n\n\nThe paired t-test can be performed using statistical software, with a summary of the results presented in Table 4.3. We can see that the mean SBP is very similar in the two sites.\n\n\n\n\nTable 4.3:  Systolic blood pressure (mmHg) measured at two sites on the ankle \n\nSitenMean (SE)95% Confidence Interval\n\nDorsalis pedis107116.7 (3.46)(109.9 to 123.6)\n\nTibialis posterior107118.0 (3.43)(111.2 to 124.8)\n\nDifference107-1.3 (1.31)(-3.9 to 1.3)\n\n\n\n\n\nThe t-value is calculated as −0.96 with 106 degrees of freedom, providing a two-sided P-value of 0.34. Thus these data provide no evidence of a difference in systolic blood pressure between the two sites.\n\n\n\n\nAcock, Alan C. 2010. A Gentle Introduction to Stata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "05.1-comparing-two-means-stata.html#checking-data-for-the-independent-samples-t-test",
    "href": "05.1-comparing-two-means-stata.html#checking-data-for-the-independent-samples-t-test",
    "title": "5 Stata notes",
    "section": "Checking data for the independent samples t-test",
    "text": "Checking data for the independent samples t-test\n\nProducing histograms and boxplots by a second variable\nTo obtain the histograms in Figure 4.1 using the Example_5.1.dta data, go to Graphics > Histogram. Select birthweight as the Variable in the Main tab. Next go to the By tab, tick Draw subgraphs for unique values of variable and select gender as the variable as shown below:\n\n\n\n\n\nNote that we have also improved the basic histogram definition on the Main tab, by defining the Lower limit of first bin to be 2.5, and the Bin width to be 0.25kg:\n\n\n\n\n\nA similar process is used to obtain the boxplots shown in Figure 4.1: go to Graphics > Box plot. In the graph box – Box plots dialog box, select birthweight as the Variable in the Main tab. Next go to the Categories tab, tick Group 1 and select gender as the Grouping variable as shown below.\n\n\n\n\n\nClick OK or Submit to produce the box plot.\n[Command: graph box birthweight, over(gender)]\n\n\nProducing split summary statistics\nTo produce summary statistics for a continuous variable, split by a second binary categorical variable, as in Worked Example 5.1 using Example_5.1.dta, go to Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Summary statistics . In the summarize dialog box, select birthweight as the Variable in the Main tab of the dialog box. Next go to the by/if/in tab, tick Repeat command by groups and select gender as the Variable that define groups as shown below.\n\n\n\n\n\nClick OK or Submit to obtain the output as shown below.\n[Command: by gender, sort : summary birthweight]\n\n. by gender, sort : summ birthweight\n\n--------------------------------------------------------------------------------\n-> gender = Female\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n birthweight |         56    3.587411    .3629788       2.95       4.25\n\n--------------------------------------------------------------------------------\n-> gender = Male\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n birthweight |         44    3.421364    .3536165       2.75        4.1\n\nThe output above is easy for copying into a report. You could also submit the summarize command with the detail option to compare the mean with the median (50th percentile) and check the minimum and maximum values for implausible values.\n[Command: by gender, sort : sum birthweight , detail]\n\n. by gender, sort : summ birthweight, detail\n\n--------------------------------------------------------------------------------\n-> gender = Female\n\n                         Birthweight\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         2.95           2.95\n 5%         3.03           2.97\n10%         3.14           3.03       Obs                  56\n25%        3.325           3.07       Sum of Wgt.          56\n\n50%         3.53                      Mean           3.587411\n                        Largest       Std. Dev.      .3629788\n75%         3.88            4.2\n90%         4.15            4.2       Variance       .1317536\n95%          4.2            4.2       Skewness       .2453238\n99%         4.25           4.25       Kurtosis       1.962126\n\n--------------------------------------------------------------------------------\n-> gender = Male\n\n                         Birthweight\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         2.75           2.75\n 5%         2.82           2.79\n10%         2.85           2.82       Obs                  44\n25%         3.15           2.85       Sum of Wgt.          44\n\n50%         3.43                      Mean           3.421364\n                        Largest       Std. Dev.      .3536165\n75%        3.635           3.94\n90%          3.9           3.97       Variance       .1250446\n95%         3.97           4.06       Skewness      -.0895932\n99%          4.1            4.1       Kurtosis       2.325761"
  },
  {
    "objectID": "05.1-comparing-two-means-stata.html#independent-samples-t-test",
    "href": "05.1-comparing-two-means-stata.html#independent-samples-t-test",
    "title": "5 Stata notes",
    "section": "Independent samples t-test",
    "text": "Independent samples t-test\nTo carry out an independent sample t-test, go to Statistics > Summaries, tables, and tests > Classical tests of hypotheses > t test (mean-comparison test). In the ttest dialog box, choose the Two-sample using groups button, then select birthweight as the Variable name and gender as the Group variable name as shown below. Because the variances of birthweight are similar for males and females, we can leave the Unequal variances box unchecked.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]\n---------+--------------------------------------------------------------------\n  Female |      56    3.587411    .0485051    .3629788    3.490204    3.684617\n    Male |      44    3.421364    .0533097    .3536165    3.313854    3.528873\n---------+--------------------------------------------------------------------\ncombined |     100     3.51435    .0366567    .3665666    3.441615    3.587085\n---------+--------------------------------------------------------------------\n    diff |            .1660471    .0723027                .0225648    .3095293\n------------------------------------------------------------------------------\n    diff = mean(Female) - mean(Male)                              t =   2.2966\nHo: diff = 0                                     degrees of freedom =       98\n\n    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0\n Pr(T < t) = 0.9881         Pr(|T| > |t|) = 0.0238          Pr(T > t) = 0.0119\n\n[Command: ttest birthweight, by(gender)]"
  },
  {
    "objectID": "05.1-comparing-two-means-stata.html#checking-the-assumptions-for-a-paired-t-test",
    "href": "05.1-comparing-two-means-stata.html#checking-the-assumptions-for-a-paired-t-test",
    "title": "5 Stata notes",
    "section": "Checking the assumptions for a Paired t-test",
    "text": "Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset Example_5.2.dta to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences.\nGo to Data > Create new variable.\nIn the generate – Create a new variable dialog box, assign a name (e.g. difference) to your new variable in the Variable name box. We want to compute difference as sbp_dp − sbp_tp, so we enter this in the Specify a value or an expression box as shown below.\n\n\n\n\n\nClick Submit or OK.\n[Command: generate difference = sbp_dp-sbp_tp]\nCheck that the new variable appears in your Data Editor window.\nCreate a histogram with a normal curve under Graphics > Histogram, as shown previously in Module 2. You might want to vary the minimum bar value and the bar width: for example, Figure 4.2 was defined to have a lower limit of -40 and a bin width of 10. We have also requested a Normal curve be plotted in the Density plots tab.\n\n\n\n\n\n[Command: histogram difference, width(10) start(-40) frequency normal]"
  },
  {
    "objectID": "05.1-comparing-two-means-stata.html#paired-t-test",
    "href": "05.1-comparing-two-means-stata.html#paired-t-test",
    "title": "5 Stata notes",
    "section": "Paired t-Test",
    "text": "Paired t-Test\nTo perform a paired t-test we will use the dataset Example_5.2.dta. For a paired t-test, data must be organised into two variables (i.e. two columns). Go to Statistics > Summaries, tables, and tests > Classical tests of hypotheses > t test (mean-comparison test) as you had for the Independent samples t-test. In the ttest dialog box, choose the Paired button then Select sbp_dp from the dropdown list under First variable and select sbp_tp from the dropdown list under Second variable. The dialog box will look like:\n\n\n\n\n\nClick Submit or OK. The output will look as follows:\n\nPaired t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]\n---------+--------------------------------------------------------------------\n  sbp_dp |     107     116.729    3.460296    35.79358    109.8686    123.5893\n  sbp_tp |     107    117.9907    3.431356    35.49422    111.1877    124.7937\n---------+--------------------------------------------------------------------\n    diff |     107   -1.261682    1.311368    13.56489   -3.861596    1.338232\n------------------------------------------------------------------------------\n     mean(diff) = mean(sbp_dp - sbp_tp)                           t =  -0.9621\n Ho: mean(diff) = 0                              degrees of freedom =      106\n\n Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0\n Pr(T < t) = 0.1691         Pr(|T| > |t|) = 0.3382          Pr(T > t) = 0.8309\n\n[Command: ttest sbp_dp == sbp_tp]"
  },
  {
    "objectID": "05.2-comparing-two-means-R.html#setting-an-observation-to-missing",
    "href": "05.2-comparing-two-means-R.html#setting-an-observation-to-missing",
    "title": "5 R notes",
    "section": "Setting an observation to missing",
    "text": "Setting an observation to missing\nSetting an incorrect observation to missing is straightforward in Stata by using the Data Editor. While RStudio allows browsing a data set as a spreadsheet, it will not let a user replace an observation with a missing value: this should be done using code.\nA missing value in R is denoted NA, and this is consistent for any variable type: continuous, string (i.e. character) and even a factor.\nRecall the weights data used in Module 2. In viewing a boxplot of weight, we saw an obvious outlier of 700.2kg for ID 58:\n\nlibrary(jmv)\n\nsample <- read.csv(\"data/examples/Weight_s2.csv\")\n\nboxplot(sample$weight, xlab=\"Weight (kg)\", main=\"Boxplot of 1000 weights\")\n\n\n\nsubset(sample, weight>200)\n\n   id weight\n58 58  700.2\n\n\nWe previously set this value to 70.2kg using an ifelse() command. Here, let’s create a new, cleaned weight variable, and set the incorrect value to missing:\n\nsample$weight_clean = ifelse(sample$weight==700.2, NA, sample$weight)\n\nOur code will create a new column (called weight_clean) in the sample dataframe. We will test whether weight is equal to 700.2; if this is true, we will assign weight_clean to be NA (i.e. missing), otherwise weight_clean will equal the value of weight.\nLet’s view the data from ID 58, and summarise the cleaned weight variable using descriptives() and a boxplot:\n\nsubset(sample, sample$id==58)\n\n   id weight weight_clean\n58 58  700.2           NA\n\ndescriptives(data=sample, vars=weight_clean)\n\n\n DESCRIPTIVES\n\n Descriptives                           \n ────────────────────────────────────── \n                         weight_clean   \n ────────────────────────────────────── \n   N                              999   \n   Missing                          1   \n   Mean                      69.76406   \n   Median                    69.80000   \n   Standard deviation        5.055188   \n   Minimum                   53.80000   \n   Maximum                   85.80000   \n ────────────────────────────────────── \n\nboxplot(sample$weight_clean, xlab=\"Weight (kg)\", main=\"Boxplot of 999 weights\",\n        sub=\"(Excluding 1 observation of 700.2kg)\")"
  },
  {
    "objectID": "05.2-comparing-two-means-R.html#checking-data-for-the-independent-samples-t-test",
    "href": "05.2-comparing-two-means-R.html#checking-data-for-the-independent-samples-t-test",
    "title": "5 R notes",
    "section": "Checking data for the independent samples t-test",
    "text": "Checking data for the independent samples t-test\n\nProducing histograms and boxplots by a second variable\nWe have seen how to create histograms and boxplots separated by a second variable in Module 2 (REF). We will demonstrate using the birthweight data in Example_5.1.rds.\n\nlibrary(jmv)\n\nbwt <- readRDS(\"data/examples/Example_5.1.rds\")\n\nsummary(bwt)\n\n    gender    birthweight   \n Female:56   Min.   :2.750  \n Male  :44   1st Qu.:3.257  \n             Median :3.450  \n             Mean   :3.514  \n             3rd Qu.:3.772  \n             Max.   :4.250  \n\nsummary(bwt$gender)\n\nFemale   Male \n    56     44 \n\n\nWe can create subsets of the birthweight data, subsetted for males and females separately. Note here that gender is a factor, so we need to select based on the factor labels, not the underlying numeric code.\n\nbwt_m <- subset(bwt, bwt$gender==\"Male\")\nbwt_f <- subset(bwt, bwt$gender==\"Female\")\n\nWe can now create histograms and boxplots for males and females separately, in the usual way, using the par function to set the graphics parameters to display graphs in a 2-by-2 grid:\n\npar(mfrow=c(2,2))\nhist(bwt_m$birthweight, xlim=c(2.5, 4.5), xlab=\"Birthweight (kg)\", main=\"Males\")\nhist(bwt_f$birthweight, xlim=c(2.5, 4.5), xlab=\"Birthweight (kg)\", main=\"Females\")\n\nboxplot(bwt_m$birthweight, ylim=c(2.5, 4.5), ylab=\"Birthweight (kg)\", main=\"Males\")\nboxplot(bwt_f$birthweight, ylim=c(2.5, 4.5), ylab=\"Birthweight (kg)\", main=\"Females\")\n\n\n\npar(mfrow=c(1,1))\n\nWhen we are done plotting multiple graphs, we can reset the plot window by submitting par(mfrow=c(1,1)).\n\n\nProducing split summary statistics\nThe descriptives function within the jmv function allows summary statistics to be calculated within subgroups using the splitBy argument:\n\ndescriptives(data=bwt, vars=birthweight, splitBy=gender)\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         gender    birthweight   \n ─────────────────────────────────────────────── \n   N                     Female             56   \n                         Male               44   \n   Missing               Female              0   \n                         Male                0   \n   Mean                  Female       3.587411   \n                         Male         3.421364   \n   Median                Female       3.530000   \n                         Male         3.430000   \n   Standard deviation    Female      0.3629788   \n                         Male        0.3536165   \n   Minimum               Female       2.950000   \n                         Male         2.750000   \n   Maximum               Female       4.250000   \n                         Male         4.100000   \n ───────────────────────────────────────────────"
  },
  {
    "objectID": "05.2-comparing-two-means-R.html#independent-samples-t-test",
    "href": "05.2-comparing-two-means-R.html#independent-samples-t-test",
    "title": "5 R notes",
    "section": "Independent samples t-test",
    "text": "Independent samples t-test\nWe can use the ttestIS() (t-test, independent samples) function from the jmv package to perform the independent samples t-test. We include the meanDiff=TRUE and ci=TRUE options to obtain the difference in means, with its 95% confidence interval. We can request a Welch’s test (which does not assume equal variances) by the welchs=TRUE option:\n\nttestIS(data=bwt, vars=birthweight, group=gender, meanDiff=TRUE, ci=TRUE)\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                                                          \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                 Statistic    df          p            Mean difference    SE difference    Lower         Upper       \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   birthweight    Student's t     2.296556    98.00000    0.0237731          0.1660471       0.07230265    0.02256481    0.3095293   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\nttestIS(data=bwt, vars=birthweight, group=gender, meanDiff=TRUE, ci=TRUE, welchs=TRUE)\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                                                          \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                 Statistic    df          p            Mean difference    SE difference    Lower         Upper       \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   birthweight    Student's t     2.296556    98.00000    0.0237731          0.1660471       0.07230265    0.02256481    0.3095293   \n                  Welch's t       2.303840    93.54377    0.0234458          0.1660471       0.07207403    0.02293328    0.3091609   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\nThere is no built-in function to calculate an independent t-test from summarised data, nor is there a function within jmv. We can use the tsum.test() function within the BSDA package, with the following syntax:\ntsum.test(mean.x=, s.x=, n.x=,\n          mean.y=, s.y=, n.y=,\n          mu=0, alternative=\"two.sided\", var.equal = TRUE)\nHere we specify the mean, standard deviation and sample size for the first group (on the first line) and the second group (on the second line). We can relax the assumption of equal variances using var.equal=FALSE."
  },
  {
    "objectID": "05.2-comparing-two-means-R.html#checking-the-assumptions-for-a-paired-t-test",
    "href": "05.2-comparing-two-means-R.html#checking-the-assumptions-for-a-paired-t-test",
    "title": "5 R notes",
    "section": "Checking the assumptions for a Paired t-test",
    "text": "Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset Example_5.2.rds to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences and examine its histogram.\n\nsbp <- readRDS(\"data/examples/Example_5.2.rds\")\nsbp$diff = sbp$sbp_dp - sbp$sbp_tp\nhist(sbp$diff, xlab=\"Blood pressure (mmHg)\", main=\"Difference in systolic blood pressure\")\n\n\n\n\nWe might want to plot a Normal curve over this distribution, as we did in Module 2:\n\nhist(sbp$diff,\n     xlab=\"Systolic blood pressure (mmHg)\",\n     main=\"Difference in systolic blood pressure\",\n     probability = TRUE)\n\ncurve(dnorm(x,\n            mean=mean(sbp$diff),\n            sd=sd(sbp$diff)),\n      col=\"darkblue\",\n      add=TRUE)\n\n\n\n\nWhile there is a large difference in blood pressure (around 60 mmHg) that warrents further checking, the curve is roughly symmetric with an approximately Normal distribution."
  },
  {
    "objectID": "05.2-comparing-two-means-R.html#paired-t-test",
    "href": "05.2-comparing-two-means-R.html#paired-t-test",
    "title": "5 R notes",
    "section": "Paired t-Test",
    "text": "Paired t-Test\nTo perform a paired t-test we will use the dataset Example_5.2.rds. We can perform a paired t-test using the ttestPS() function within the jmv package, where we defined the paired observations as: `pairs=list(list(i1 = ‘variable1’, i2 = ‘variable2’))\n\nttestPS(data=sbp, pairs=list(list(i1 = 'sbp_dp', i2 = 'sbp_tp')), meanDiff=TRUE, ci=TRUE)\n\n\n PAIRED SAMPLES T-TEST\n\n Paired Samples T-Test                                                                                                                   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                      statistic     df          p            Mean difference    SE difference    Lower        Upper      \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   sbp_dp    sbp_tp    Student's t    -0.9621117    106.0000    0.3381832          -1.261682         1.311368    -3.861596    1.338232   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\nThe syntax of the ttestPS function is a little cumbersome. The t.test function can be used as an alternative:\n\nt.test(sbp$sbp_dp, sbp$sbp_tp, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  sbp$sbp_dp and sbp$sbp_tp\nt = -0.96211, df = 106, p-value = 0.3382\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.861596  1.338232\nsample estimates:\nmean difference \n      -1.261682"
  },
  {
    "objectID": "05.3-comparing-two-means-activities.html",
    "href": "05.3-comparing-two-means-activities.html",
    "title": "5 Learning Activities",
    "section": "",
    "text": "Activity 5.1\nIndicate what type of t-test could be used to analyse the data from the following studies and provide reasons:\n\nA total of 60 university students are randomly assigned to undergo either behaviour therapy or Gestalt therapy. After twenty therapeutic sessions, each student earns a score on a mental health questionnaire.\nA researcher wishes to determine whether attendance at a day care centre increases the scores of three year old twins on a motor skills test. Random assignment is used to decide which member from each of 30 pairs of twins attends the day care centre and which member stays at home.\nA child psychologist assigns aggression scores to each of 10 children during two 60 minute observation periods separated by an intervening exposure to a series of violent TV cartoons.\nA marketing researcher measures 100 doctors’ reports of the number of their patients asking them about a particular drug during the month before and the month after a major advertising campaign.\n\n\n\nActivity 5.2\nA study was conducted to compare haemoglobin levels in the blood of children with and without cystic fibrosis. It is known that haemoglobin levels are normally distributed in children. The study results are given below:\n\n\n\n\nSummary of haemoglobin (g/dL)\nStatisticChildren without CFChildren with CF\n\nn1215\n\nMean19.913.9\n\nSD (SE)5.9 (1.70)6.2 (1.60)\n\n\n\n\n\nState the appropriate null hypothesis and alternate hypothesis\nUse Stata to conduct an appropriate statistical test to evaluate the null hypothesis. Are the assumptions for the test met for this analysis to be valid?\n\n\n\nActivity 5.3\nA randomised controlled trial (RCT) was carried out to investigate the effect of a new tablet supplement in increasing the hematocrit (%) value in anaemic participants. In the study, hematocrit was measured as the proportion of blood that is made up of red blood cells. Hematocrit levels are often lower in anaemic people who do not have sufficient healthy red blood cells. In the RCT, 33 people in the intervention group received the new supplement and 31 people in the control group received standard care (i.e. the usual supplement was given). After 4 weeks, hematocrit values were measured as shown in the Stata file ActivityS5.3.dta. In the community, hematocrit levels are normally distributed.\n\nState the research question and frame it as a null hypothesis.\nUse Stata to conduct an appropriate statistical test to answer the research question. Before using the test, check the data to see if the assumptions required for the test are met. Obtain a box plot to obtain an estimate of the centre and spread of the data for each group.\nRun your statistical test.\nConstruct a table to show how you would report your results and write a conclusion.\n\n\n\nActivity 5.4\nA total of 41 babies aged 6 months to 2 years with haemangioma (birth mark) were enrolled in a study to test the effect of a new topical medication in reducing the volume of their haemangioma. Parents were asked to apply the medication twice daily. The volume (in mm3) of the haemangioma was measured at enrolment and again after 12 weeks of using the medication.\n\nWhat is the research question in this study? State the null and alternative hypotheses.\nUse the data in the Stata file ActivityS5.4.dta to answer the research question. Which statistical test is appropriate to answer the research question and why? Conduct the test in Stata and write your conclusion.\nWhat are the limitations of this study?"
  },
  {
    "objectID": "08-correlation-and-regression.html#learning-objectives",
    "href": "08-correlation-and-regression.html#learning-objectives",
    "title": "5  Correlation and linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplore the association between two continuous variables using a scatter plot;\nEstimate and interpret correlation coefficients;\nEstimate and interpret parameters from a simple linear regression;\nDecide whether a regression model is valid;\nTest a hypothesis using regression coefficients;\nOutline the concept of multiple regression and its role in investigative epidemiology."
  },
  {
    "objectID": "08-correlation-and-regression.html#readings",
    "href": "08-correlation-and-regression.html#readings",
    "title": "5  Correlation and linear regression",
    "section": "Readings",
    "text": "Readings\nKirkwood and Sterne (2001); Chapter 10. [UNSW Library Link]\nBland (2015); Chapter 11. [UNSW Library Link]\nAcock (2010); Chapter 8."
  },
  {
    "objectID": "08-correlation-and-regression.html#introduction",
    "href": "08-correlation-and-regression.html#introduction",
    "title": "5  Correlation and linear regression",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn Module 5, we saw how to test whether the means from two groups are equal - in other words, whether a continuous variable is related to a categorical variable. We often want to know how closely two continuous variables are related. For example, we may want to know how closely blood cholesterol levels are related to dietary fat intake in adult men. To measure the strength of association between two continuously distributed variables, a correlation coefficient is used.\nWe may also want to know how well one continuous measurement predicts the value of another continuous measurement. For example, we may want to know how well height predicts values of lung capacity in a community of adults. A regression model allows us to use one measurement to predict another measurement.\nAlthough both correlation coefficients and regression models can be used to describe the degree of association between two continuous variables, the two methods provide very different statistical information. It is important to note that both methods only measures the strengths of an association between variables and does not imply a causal relationship."
  },
  {
    "objectID": "08-correlation-and-regression.html#correlation",
    "href": "08-correlation-and-regression.html#correlation",
    "title": "5  Correlation and linear regression",
    "section": "5.2 Correlation",
    "text": "5.2 Correlation\nWe use correlation to measure the strength of a linear relationship between two variables. Before calculating a correlation coefficient, a scatter plot should first be obtained to give an understanding of the nature of the relationship between the two variables.\n\n5.2.1 Worked Example\nThe file Example_8.1.csv has information about height and lung function collected from a sample of 120 adults. A random sample of adults was approached to take part in the research study, but the response rate was low at 45%. Information was collected on height (cm) and lung function, which was measured as forced vital capacity (FVC). We can obtain a scatter-plot shown in Figure 5.1. This shows that as height increases, lung function also increases, which is as expected. One or two of the data points are separated from the rest of the data but are not so far away as to be considered outliers because they do not seem to stand out of other observations.\n\n\n\n\n\nFigure 5.1: Plot\n\n\n\n\n\n\n5.2.2 Correlation coefficients\nA correlation coefficient (r) describes how closely the variables are related, that is the strength of linear association between two continuous variables. The range of the coefficient is from +1 to −1 where +1 is a perfect positive association, 0 is no association and −1 is a perfect inverse association. In general, an absolute (disregarding the sign) r value below 0.3 indicates a weak association, 0.3 to < 0.6 is fair association, 0.6 to < 0.8 is a moderate association, and \\(\\ge\\) 0.8 indicates a strong association.\nThe coefficient is positive when large values of one variable tend to occur with large values of the other, and small values of one variable (y) tend to occur with small values of the other (x) (Figure 5.2 (a and b)). For example, height and weight in healthy children or age and blood pressure.\nThe coefficient is negative when large values of one variable tend to occur with small values of the other, and small values of one variable tend to occur with large values of the other (Figure 5.2 (c and d)). For example, percentage immunised against infectious diseases and under-five mortality rate.\n\n\n\n\n\nFigure 5.2: Scatter plots demonstrating strong and weak, positive and negative associations\n\n\n\n\nThe P value associated with an r value is an estimate of whether the correlation coefficient is significantly different from zero. However, a correlation coefficient that does not have a significant P value does not imply that there is no relationship because the correlation coefficient only tests for a linear association and there may be a non-linear relationship such as a curved or irregular relationship.\nThe assumptions for using a Pearson’s correlation coefficient are that:\n\nobservations are independent;\nboth variables are continuous variables;\nthe relationship between the two variables is linear.\n\nThere is a further assumption that the data follow a bivariate normal distribution. This assumes: y follows a normal distribution for given values of x; and x follows a normal distribution for given values of y. This is quite a technical assumption that we do not discuss further.\nThere are two types of correlation coefficients– the correct one to use is determined by the nature of the variables as shown in Table 5.1.\n\n\n\n\nTable 5.1:  Correlation coefficients and their application \n\nCorrelation coefficientApplication\n\nPearson’s correlation coefficient: rBoth variables are continuous and a bivariate normal distribution can be assumed\n\nSpearman’s rank correlation: rhoBivariate normality cannot be assumed. Also useful when at least one of the variables is ordinal\n\n\n\n\n\nSpearman’s \\(\\rho\\) is calculated using the ranks of the data, rather than the actual values of the data. We will see further examples of such methods in Module 9, when we consider non-parametric tests, which are often based on ranks.\nCorrelation coefficients are often presented in the form of a correlation matrix which can display the correlation between a number of variables in a single table (Table 5.2).\n\n\nTable 5.2: Correlation matrix for Height and FVC\n\n\n\n\n\n\n\n\nHeight\nFVC\n\n\nHeight\n1\n0.70\nP < 0.0001\n\n\nFVC\n0.70\nP < 0.0001\n1\n\n\n\n\nThis correlation matrix shows that the Pearson’s correlation coefficient between height and lung function is 0.70 with P<0.0001 indicating very strong evidence of a linear association between height and FVC. A correlation matrix sometimes includes correlations between the same variable, indicated as a correlation coefficient of 1. For example, \\(Height\\) is perfectly correlated with itself (i.e. has a correlation coefficient of 1). Similarly, \\(FVC\\) is perfectly correlated with itself.\nThis r value was calculated for the full data set of 120 adults who had heights ranging from 160 to 172cms. If the r value is calculated for the 60 adults with a height less than 165cms, it is much lower at 0.433 although significant at P=0.001. In general, r values are higher for a wider range of values on the x axis even though the relationship between the two variables remains the same.\nCorrelation coefficients are rarely used as important statistics in their own right because they do not fully explain the relationship between the two variables and the range of the data has an important influence on the size of the coefficient. In addition, the statistical significance of the correlation coefficient is often over interpreted because a small correlation which is of no clinical importance can become statistically significant even with a relatively small sample size. For example, a poor correlation of 0.3 will be statistically significant if the sample size is large enough."
  },
  {
    "objectID": "08-correlation-and-regression.html#linear-regression",
    "href": "08-correlation-and-regression.html#linear-regression",
    "title": "5  Correlation and linear regression",
    "section": "5.3 Linear regression",
    "text": "5.3 Linear regression\nThe nature of a relationship between two variables is more fully described using regression. There are two principal purposes for building a regression model. The most common is to build a predictive model, for example in situations in which age and gender are used to predict normal values of characteristics such as lung size or body mass index. Normal values are the range of values that occur naturally in the general population.\nThe second purpose for using a regression model is for testing the hypothesis that there is a linear relationship between one or more explanatory variables and an outcome variable. For example, a regression model can be used to test the extent to which age predicts BMI or to test the hypothesis that two groups with a different dietary regime have significantly different BMI values after adjusting for age differences.\nFrom Worked Example 8.1, we can be also plot a regression line through the scatter. Figure @ref(fig:scatter-plot-line) shows the data overlayed with the fitted regression line.\n\n\n\n\n\nAssociation between height and lung function in 120 adults\n\n\n\n\nThe line through the plot is called the line of ‘best fit’ because the size of the deviations between the data points and the line is minimised in the calculation. The distance between each data point and the regression line is called a ‘residual’.\n\n5.3.1 Regression equations\nThe mathematical equation for the line explains the relationship between the two variables. The equation of the regression line is as follows:\n\\[y = \\beta_{0} + \\beta_{1}x\\]\nThis line is shown in Figure 5.3 using the notation shown in Table 5.3.\n\n\n\n\n\nFigure 5.3: Coefficients of a linear regression equation\n\n\n\n\n\n\n\n\nTable 5.3:  Notation for linear regression equation \n\nSymbolInterpretation\n\n\\(y \\)Observed value of the outcome variable\n\n\\(x \\)Observed value of the explanatory variable\n\n\\(\\beta_0\\)Intercept of the regression line\n\n\\(\\beta_1\\)Slope of the regression line\n\n\n\n\n\nThe intercept is the point at which the regression line intersects with the y-axis when the value of ‘x’ is zero. In most cases, the intercept does not have a biologically meaningful interpretation as the explanatory variable cannot take a value of zero. In our working example, the intercept is not meaningful as it is not possible for an adult to have a height of 0cm.\nThe slope of the line is the predicted change in the outcome variable ‘y’ as the explanatory explanatory variable ‘x’ increases by 1 unit.\nAn important concept is that regression predicts an expected value of ‘y’ given an observed value of ‘x’: any error around the explanatory variable is not taken into account. For this reason, measurements that can be taken accurately, such as age and height, make good explanatory variables."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "href": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "title": "5  Correlation and linear regression",
    "section": "5.4 Regression coefficients: estimation",
    "text": "5.4 Regression coefficients: estimation\nSoftware is always used to estimate the regression equation for a set of data, using the method of least squares. This method estimates the intercept and the slope, and also their variability (i.e. standard errors)."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-inference",
    "href": "08-correlation-and-regression.html#regression-coefficients-inference",
    "title": "5  Correlation and linear regression",
    "section": "5.5 Regression coefficients: inference",
    "text": "5.5 Regression coefficients: inference\nWe can use the estimated regression coefficients and their variability to calculate 95% confidence intervals. Here, a t-value from a t-distribution with \\(n - 2\\) degrees of freedom is used:\n\n95% confidence interval for intercept: \\(b_0 \\pm t_{n-2} \\times SE(b_0)\\)\n95% confidence interval for slope: \\(b_1 \\pm t_{n-2} \\times SE(b_1)\\)\n\nNote that as the constant (\\(b_0\\)) is not often biologically plausible, the 95% confidence interval for the constant is often not reported.\nThe significance of the estimated slope (and less commonly, intercept) can be tested using a t-test. The null hypotheses and the alternative hypothesis for testing the slope of a simple linear regression model are:\n\nH0: \\(\\beta_1 = 0\\)\nH1: \\(\\beta_1 \\ne 0\\)\n\nTo test the null hypothesis for the regression coefficient beta_1, the following t-test is used:\n\\[t = b_1 /SE(b_1)\\]\nThis will give a t statistic which can be referred to a t distribution with n − 2 degrees of freedom to calculate the corresponding P-value.\nTable 5.4 shows the estimated regression coefficients for our working example.\n\n\n\n\nTable 5.4:  Estimated regression coefficients \n\nTermEstimateStandard errort valueP value95% Confidence interval\n\nIntercept-18.9 2.19 t=-8.60, 118df<0.001-23.22 to -14.53\n\nHeight0.140.013t=10.58, 118df<0.0010.11 to 0.17\n\n\n\n\n\nFrom this output, we see that the slope is estimated as 0.14 with an estimated intercept of -18.87. Therefore, the regression equation is estimated as:\nFVC (L) = − 18.87 + (0.14 \\(\\times\\) Height in cm)\nThere is very strong evidence of a linear association between FVC and height in cm (P < 0.001).\nThis equation can be used to predict FVC for a person of a given height. For example, the predicted FVC for a person 165 cm tall is estimated as:\nFVC = − 18.87347 + (0.1407567 \\(\\times\\) 165.0) = 4.40 L.\nNote that for the purpose of prediction we have kept all the decimal places in the coefficients to avoid rounding error in the intermediate calculation.\n\n5.5.1 Fit of a linear regression model\nAfter fitting a linear regression model, it is important to know how well the model fits the observed data. One way of assessing the model fit is to compute a statistic called coefficient of determination, denoted by \\(R^2\\). It is the square of the Pearson correlation coefficient \\(r: r^2 = R^2\\). Since the range of \\(r\\) is from −1 to 1, \\(R^2\\) must lie between 0 and 1.\n\\(R^2\\) can be interpreted as the proportion of variability in y that can be explained by variability in x. Hence, the following conditions may arise:\nIf \\(R^2 = 1\\), then all variation in y can be explained by variation of x and all data points fall on the regression line.\nIf \\(R^2 = 0\\), then none of the variation in y is related to x at all, and the variable x explains none of the variability in y.\nIf \\(0 < R^2 <1\\), then the variability of y can be partially explained by the variability in x. The larger the \\(R^2\\) value, the better is the fit of the regression model.\n\n\n5.5.2 Assumptions for linear regression\nRegression is robust to moderate degrees of non-normality in the variables, provided that the sample size is large enough and that there are no influential outliers. Also, the regression equation describes the relationship between the variables and this is not influenced as much by the spread of the data as the correlation coefficient is.\nThe assumptions that must be met when using linear regression are as follows:\n\nobservations are independent;\nthe relationship between the explanatory and the outcome variable is linear;\nthe residuals are normally distributed.\n\nA residual is defined as the difference between the observed and predicted outcome from the regression model. If the predicted value of the outcome variable is denoted by \\(\\hat y\\) then:\n\\[ \\text{Residual} = \\text{observed} - \\text{predicted} = y - \\hat y\\]\nIt is important for regression modelling that the data are collected in a period when the relationship remains constant. For example, in building a model to predict normal values for lung function the data must be collected when the participants have been resting and not exercising and people taking bronchodilator medications that influence lung capacity should be excluded. In regression, it is not so important that the variables themselves are normally distributed, but it is important that the residuals are. Scatter plots and specific diagnostic tests can be used to check the regression assumptions. Some of these will not be covered in this introductory course but will be discussed in detail in the Regression Methods in Biostatistics course.\nThe distribution of the residuals should always be checked. Large residuals can indicate unusual points or points that may exert undue influence on the estimated regression slope.\nThe histogram of residuals from the model is shown in Figure 5.4. The residuals are approximately normally distributed, with no outlying values.\n\n\n\n\n\nFigure 5.4: Histogram of regression residuals\n\n\n\n\n\n\n5.5.3 Critical appraisal\nWhen reading the literature, it is important to be critical about how correlation coefficients are interpreted. It is a good idea to check if a scatter plot is shown to help interpret the relationship and to indicate if there are any potential outliers. Also, question whether the correlation coefficient has been calculated from a random sample and if not, what selected samples the value can be generalised to.\nWhen regression is reported it is essential that the axes are correctly presented so that the equation is predictive. Thus, the explanatory variable must be presented on the x axis and the outcome on the y axis. It is also a good idea to check that all the assumptions are met. Outliers which result in a non-normal distribution of the residuals can severely bias the regression coefficients."
  },
  {
    "objectID": "08-correlation-and-regression.html#multiple-linear-regression",
    "href": "08-correlation-and-regression.html#multiple-linear-regression",
    "title": "5  Correlation and linear regression",
    "section": "5.6 Multiple linear regression",
    "text": "5.6 Multiple linear regression\nIn the above example, we have only used a simple linear regression model of two continuous variables. Other more complex models can be built from this e.g. if we wanted to look at the effect of gender (male vs. female) as binary indicator in the model while adjusting for the effect of height. In that case we would include both the variables in the model as explanatory variables. In the same way we can include any number of explanatory variables (both continuous and categorical) in the model: this is called a multivariable model. Multivariable models are often used for building predictive equations, for example by using age, height, gender and smoking history to predict lung function, or to adjust for confounding and detect effect modification to investigate the association between an exposure and an outcome factor.\nMultiple regression has an important role in investigating causality in epidemiology. The exposure variable under investigation must stay in the model and the effects of other variables which can be confounders or effect-modifiers are tested. The biological, psychological or social meaning of the variables in the model and their interactions are of great importance for interpreting theories of causality.\nOther multivariable models include binary logistic regression for use with a binary outcome variable, or Cox regression for survival analyses. These models, together with multiple regression, will be taught in PHCM9517: Regression Methods in Biostatistics.\n\n\n\n\nAcock, Alan C. 2010. A Gentle Introduction to Stata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical Statistics. 4th ed. Oxford, New York: Oxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of Medical Statistics. 2nd ed. Malden, Mass: Wiley-Blackwell."
  },
  {
    "objectID": "08.1-correlation-and-regression-stata.html#creating-a-scatter-plot",
    "href": "08.1-correlation-and-regression-stata.html#creating-a-scatter-plot",
    "title": "8 Stata notes",
    "section": "Creating a scatter plot",
    "text": "Creating a scatter plot\nWe will demonstrate using Stata for correlation and simple linear regression using the dataset Example_8.1.dta.\nTo create a scatter plot to explore the association between height and FVC click: Graphics > Twoway graph (scatter, line, etc.). In the twoway dialog box, click Create…\n\n\n\n\n\nA new dialog box will open. Select the Basic plots radio button and highlight Scatter under Basic plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button in the Plot 1 dialog box to return to the twoway dialog box, then click the OK or Submit button to produce the scatter plot shown in Figure 8.1.\n[Command: twoway (scatter FVC Height)]\nTo add a fitted line, go back to the twoway dialog box. If you clicked the OK button, you can go to Graphics > Twoway graph (scatter, line, etc.) to bring it back again.\n\n\n\n\n\nClick Create…, then select the Fit plots radio button and Linear prediction under Fit plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button, then the OK or Submit button to produce the scatterplot below.\n[Command: twoway (scatter FVC Height) (lfit FVC Height)]\n\n\n\n\n\nNotice that a legend now appears, and the y-axis title is missing. To add a y-axis title, go to the Y axis tab in the twoway dialog box to enter your title as shown below.\n\n\n\n\n\nYou can click the Submit button to check how the scatter plot looks like. Next go the Legend tab and select the Hide legend radio button.\n\n\n\n\n\nClick the OK or Submit button when you are finished to produce Figure 8.3.\n[Command: twoway (scatter FVC Height) (lfit FVC Height), ytitle(Forced vital capacity (L)) legend(off)]\nTo save your graph, go to File > Save in the Graph window, and be sure to save your file as a PNG file:"
  },
  {
    "objectID": "08.1-correlation-and-regression-stata.html#calculating-a-correlation-coefficient",
    "href": "08.1-correlation-and-regression-stata.html#calculating-a-correlation-coefficient",
    "title": "8 Stata notes",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nTo calculate the Pearson’s correlation using the dataset Example_8.1.dta go to: Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Pairwise correlations\nSelect the two variables, FVC and Height in the Variables box. You can click the Submit button to check the output. Next, tick the box for Print significance level for each entry to obtain the P-value and the box for Print number of observations for each entry to obtain the number of observations used as shown below.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Output 8.1,\n[Command: pwcorr Height FVC, obs sig]"
  },
  {
    "objectID": "08.1-correlation-and-regression-stata.html#fitting-a-simple-linear-regression-model",
    "href": "08.1-correlation-and-regression-stata.html#fitting-a-simple-linear-regression-model",
    "title": "8 Stata notes",
    "section": "Fitting a simple linear regression model",
    "text": "Fitting a simple linear regression model\nWe will fit a simple linear regression model with Example_8.1.dta to quantify the relationship between FVC and height.\nChoose Statistics > Linear models and related > Linear regression\nIn the regress dialog box, select FVC as the Dependent variable, and Height as the Independent variable.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Outputs 8.2 and 8.3.\n[Command: reg FVC Height]"
  },
  {
    "objectID": "08.1-correlation-and-regression-stata.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08.1-correlation-and-regression-stata.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "8 Stata notes",
    "section": "Plotting residuals from a simple linear regression",
    "text": "Plotting residuals from a simple linear regression\nTo obtain the residuals, go to Statistics > Post estimation after running the regress command.\nIn the Postestimation Selector dialog box, select Predictions and their SEs, leverage statistics, distance statistics, etc. in the list under Predictions as shown below.\n\n\n\n\n\nIn the predict dialog box, choose the Residuals button and enter a New variable name (e.g. FVC_resid) for the residuals from the regression model.\n\n\n\n\n\nClick OK button when you are done.\n[Command: predict FVC_resid, residuals]\nYou can now check the assumption that the residuals are normally distributed by creating a histogram with the normal curve using Graphics > Histogram as shown in Stata Notes section for Module 2. Below is the histogram dialog box used to produce the graph in Figure 8.5.\n\n\n\n\n\n[Command: histogram FVC_resid, bin(12) frequency normal]"
  },
  {
    "objectID": "08.2-correlation-and-regression-R.html#creating-a-scatter-plot",
    "href": "08.2-correlation-and-regression-R.html#creating-a-scatter-plot",
    "title": "8 R notes",
    "section": "Creating a scatter plot",
    "text": "Creating a scatter plot\nWe can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands:\n\nplot(x=lung$Height, y=lung$FVC, \n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\")\n\n\n\n\nTo add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with the lm() function, which fits a linear model.\nThe basic syntax of the lm() function is: lm(y ~ x) where y represents the outcome variable, and x represents the explanatory variable. Putting this all together:\n\nplot(x=lung$Height, y=lung$FVC,\n     xlab=\"Height (cm)\",\n     ylab=\"Forced vital capacity (L)\")\n\nabline(lm(lung$FVC ~ lung$Height))\n\n\n\n\nOr using the ggformula package, we form the basic plot using the following:\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") |>\n  gf_theme(theme = theme_minimal())\n\n\n\n\nWe can add an estimated linear regression line by piping the command gf_lm():\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") |>\n  gf_lm() |>\n  gf_theme(theme = theme_minimal())\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "08.2-correlation-and-regression-R.html#calculating-a-correlation-coefficient",
    "href": "08.2-correlation-and-regression-R.html#calculating-a-correlation-coefficient",
    "title": "8 R notes",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nWe can use the corrMatrix function in the Jamovi package to calculate a Pearson’s correlation coefficient:\n\ncorrMatrix(data=lung, vars=c(Height, FVC))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                   \n ──────────────────────────────────────────────────── \n                            Height        FVC         \n ──────────────────────────────────────────────────── \n   Height    Pearson's r             —                \n             p-value                 —                \n                                                      \n   FVC       Pearson's r     0.6976279            —   \n             p-value        < .0000001            —   \n ────────────────────────────────────────────────────"
  },
  {
    "objectID": "08.2-correlation-and-regression-R.html#fitting-a-simple-linear-regression-model",
    "href": "08.2-correlation-and-regression-R.html#fitting-a-simple-linear-regression-model",
    "title": "8 R notes",
    "section": "Fitting a simple linear regression model",
    "text": "Fitting a simple linear regression model\nWe can use the lm function to fit a simple linear regression model, specifying the model as y ~ x where y represents the outcome variable, and x represents the explanatory variable. Using Example_8.1.rds, we can quantify the relationship between FVC and height:\n\nlm(FVC ~ Height, data=lung)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nCoefficients:\n(Intercept)       Height  \n   -18.8735       0.1408  \n\n\nThe default output from the lm function is rather sparse. We can obtain much more useful information by defining the linear regression model as an object, then using the summary() function:\n\nmodel <- lm(FVC ~ Height, data=lung)\nsummary(model)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01139 -0.23643 -0.02082  0.24918  1.31786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -18.87347    2.19365  -8.604 3.89e-14 ***\nHeight        0.14076    0.01331  10.577  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3965 on 118 degrees of freedom\nMultiple R-squared:  0.4867,    Adjusted R-squared:  0.4823 \nF-statistic: 111.9 on 1 and 118 DF,  p-value: < 2.2e-16\n\n\nFinally, we can obtain 95% confidence intervals for the regression coefficients using the confint function:\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -23.2174967 -14.5294441\nHeight        0.1144042   0.1671092"
  },
  {
    "objectID": "08.2-correlation-and-regression-R.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08.2-correlation-and-regression-R.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "8 R notes",
    "section": "Plotting residuals from a simple linear regression",
    "text": "Plotting residuals from a simple linear regression\nWe can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a histogram in the usual way:\n\nresiduals <- resid(model)\nhist(residuals)\n\n\n\n\nA Normal curve can be overlaid if we plot the residuals using a probability scale.\n\nhist(residuals, probability = TRUE,\n     ylim = c(0, 1))\n\ncurve(dnorm(x, mean=mean(residuals), sd=sd(residuals)), \n      col=\"darkblue\", lwd=2, add=TRUE)\n\n\n\n\nUsing ggformula, we can plot the residuals as a histogram:\n\ngf_dhistogram(~ residuals, data=model) |>\n  gf_dist(\"norm\", \n          params=list(mean=mean(model$residuals), \n                      sd=sd(model$residuals)))\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\ngf_dhistogram(~ residuals, data=model) |>\n  gf_dens()"
  },
  {
    "objectID": "08.3-correlation-and-regression-activities.html",
    "href": "08.3-correlation-and-regression-activities.html",
    "title": "8 Learning Activities",
    "section": "",
    "text": "Activity 8.1\nTo investigate how body weight (kg) effects blood plasma volume (mL), data were collected from 30 participants and a simple linear regression analysis was conducted. The slope of the regression was 68 (95% confidence interval 52 to 84) and the intercept was −1570 (95% confidence interval −2655 to −492).\n[You do not need software for this Activity]\n\nWhat is the outcome variable and explanatory (exposure) variable?\nInterpret the regression slope and its 95% CI\nWrite the regression equation\nIf we randomly sampled a person from the population and found that their weight is 80kg, what would be the predicted value of plasma volume for this person?\n\n\n\nActivity 8.2\nTo examine whether age predicts IQ, data were collected on 104 people. Use the data in the file Activity_8.2.csv to answer the following questions.\n\nWhat are the outcome variable and the explanatory variable?\nCreate a scatter plot with the two variables. What can you infer from the scatter plot?\nObtain the correlation coefficient between age and IQ and interpret it.\nConduct a simple linear regression and report the relationship between the two variables including the interpretation of the R2 value. Are the assumptions for linear regression met in this model?\nWhat could you infer about the association between age and IQ in the population, based on the results of the regression analysis in this sample?\n\n\n\nActivity 8.3\nWhich of the following correlation coefficients indicates the weakest linear relationship and why?\n\nr = 0.72\nr = 0.41\nr = 0.13\nr = −0.33\nr = −0.84\n\n\n\nActivity 8.4\nAre the following statements true or false?\n\nIf a correlation coefficient is closer to 1.00 than to 0.00, this indicates that the outcome is caused by the exposure.\nIf a researcher has data on two variables, there will be a higher correlation if the two means are close together and a lower correlation if the two means are far apart."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acock, Alan C. 2010. A Gentle Introduction to\nStata. 3rd ed. College Station, Tex:\nStata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical\nStatistics. 4th ed. Oxford, New York:\nOxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of\nMedical Statistics. 2nd ed. Malden, Mass:\nWiley-Blackwell."
  }
]